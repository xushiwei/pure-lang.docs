This is pure.info, produced by makeinfo version 4.12 from pure.texi.

Generated by Sphinx

INFO-DIR-SECTION Pure Language and Library Documentation
START-INFO-DIR-ENTRY
* pure: (pure.info).    The Pure Manual
END-INFO-DIR-ENTRY


File: pure.info,  Node: Top,  Next: Introduction,  Up: (dir)

The Pure Manual
***************

  Version 0.47, January 18, 2011

  Albert Gräf <<Dr.Graef@t-online.de>>

  Copyright (c) 2009-2011 by Albert Gräf. This document is available
under the GNU Free Documentation License
(http://www.gnu.org/copyleft/fdl.html). Also see the *note Copying: 3.
section for licensing information of the software.

  This manual describes the Pure programming language and how to invoke
the Pure interpreter program. To read the manual inside the
interpreter, just type `help' at the command prompt. See the *note
Online Help: 4. section for details.

  There is a companion to this manual, the `purelib' which contains the
description of the standard library operations. More information about
Pure and the latest sources can be found under the following URLs:

   * Pure website: <http://pure-lang.googlecode.com>

   * Pure mailing list: <http://groups.google.com/group/pure-lang>

* Menu:

* Introduction::
* Invoking Pure::
* Pure Overview::
* Rule Syntax::
* Examples::
* Declarations::
* Macros::
* Exception Handling::
* C Interface::
* Standard Library::
* Interactive Usage::
* Batch Compilation::
* Caveats and Notes::
* Author::
* Copying::
* References and Links::
* Index::

 --- The Detailed Node Listing ---

Introduction

* Further Reading::
* Typographical Conventions::

Invoking Pure

* Options::
* Overview of Operation::
* Compiling Scripts::
* Tagging Scripts::
* Running Interactively::
* Verbosity and Debugging Options::
* Code Generation Options::
* Startup Files::
* Environment::

Pure Overview

* Lexical Matters::
* Definitions and Symbolic Evaluation::
* Variables in Equations::
* Expression Syntax::
* Special Forms::
* Toplevel::
* Scoping Rules::

Rule Syntax

* Patterns::
* Type Tags::
* General Rules::
* Simple Rules::

Examples

* List Comprehensions::
* Lazy Evaluation and Streams::
* Matrix Computations::
* Symbolic Matrices::
* Record Data::
* The Quote::

Declarations

* Symbol Declarations::
* Modules and Imports::
* Namespaces::

Macros

* Optimization Rules::
* Recursive Macros::
* User-Defined Special Forms::
* Macro Hygiene::

C Interface

* Extern Declarations::
* C Types::
* Importing Dynamic Libraries::
* Importing LLVM Bitcode::
* Inline Code::

Interactive Usage

* Online Help::
* Interactive Commands::
* Last Result::
* Specifying Symbol Selections::
* The show Command::
* Definition Levels::
* Debugging::
* Interactive Startup::

Batch Compilation

* Example::
* Code Size and Unstripped Executables::
* Other Output Code Formats::
* Calling Pure Functions From C::

Caveats and Notes

* Etymology::
* Backward Compatibility::
* Error Recovery::
* The __show__ Function::
* Non-Linear Patterns::
* "As" Patterns::
* Head = Function::
* With and when::
* Numeric Calculations::
* Constant Definitions::
* External C Functions::
* Calling Special Forms::
* Laziness::
* Reflection::
* Hygienic Macros::
* Stack Size and Tail Recursion::
* Handling of Asynchronous Signals::


File: pure.info,  Node: Introduction,  Next: Invoking Pure,  Prev: Top,  Up: Top

1 Introduction
**************

Pure is a functional programming language based on term rewriting. This
means that all your programs are essentially just collections of
symbolic equations which the interpreter uses to reduce expressions to
their simplest ("normal") form. This makes for a rather powerful and
flexible programming model featuring dynamic typing and general
polymorphism. In addition, Pure programs are compiled to efficient
native code on the fly, using the *note LLVM: 7. compiler framework, so
programs are executed reasonably fast and interfacing to C is very
easy. If you have the necessary 3rd party compilers installed then you
can even inline functions written in C and a number of other languages
and call them just like any other Pure function. The ease with which
you can interface to 3rd party software makes Pure useful for a wide
range of applications from symbolic algebra and scientific programming
to database, web and multimedia applications.

  The Pure language is implemented by the *Pure interpreter* program.
Just like other programming language interpreters, the Pure interpreter
provides an interactive environment in which you can type definitions
and expressions, which are executed as you type them at the
interpreter's command prompt.  However, despite its name the Pure
interpreter never really "interprets" any Pure code. Rather, it acts as
a frontend to the *Pure compiler*, which takes care of incrementally
compiling Pure code to native (machine) code. This has the benefit that
the compiled code runs much faster than the usual kinds of "bytecode"
that you find in traditional programming language interpreters.

  You can use the interpreter as a sophisticated kind of "desktop
calculator" program. Simply run the program from the shell as follows:

    $ pure
    Pure 0.47 (x86_64-unknown-linux-gnu) Copyright (c) 2008-2011 by Albert Graef
    (Type 'help' for help, 'help copying' for license information.)
    Loaded prelude from /usr/local/lib/pure/prelude.pure.

    >

The interpreter prints its sign-on message and leaves you at its '> '
command prompt, where you can start typing definitions and expressions
to be evaluated:

    > 17/12+23;
    24.4166666666667
    > fact n = if n>0 then n*fact (n-1) else 1;
    > map fact (1..10);
    [1,2,6,24,120,720,5040,40320,362880,3628800]

Typing the `quit' command or the end-of-file character (`Ctrl-d' on
Unix systems) at the beginning of the command line exits the
interpreter and takes you back to the shell.

  Instead of typing definitions and evaluating expressions in an
interactive fashion as shown above, you can also put the same code in
an (ASCII or UTF-8) text file called a *Pure program* or *script* which
can then be executed by the interpreter in "batch mode", or compiled to
a standalone executable which can be run directly from the command
line. As an aid for writing script files, a bunch of syntax
highlighting files and programming modes for various popular text
editors are included in the Pure sources.

  More information about invoking the Pure interpreter can be found in
the *note Invoking Pure: 8. section below. This is followed by a
description of the Pure language in *note Pure Overview: 9. and
subsequent sections. The interactive facilities of the Pure interpreter
are discussed in the *note Interactive Usage: a.  section, while the
*note Batch Compilation: b. section explains how to translate Pure
programs to native executables and a number of other object file
formats. The *note Caveats and Notes: c. section discusses useful tips
and tricks, as well as various pitfalls and how to avoid them. The
manual concludes with some authorship and licensing information and
pointers to related software.

* Menu:

* Further Reading::
* Typographical Conventions::


File: pure.info,  Node: Further Reading,  Next: Typographical Conventions,  Up: Introduction

1.1 Further Reading
===================

This manual is not intended as a general introduction to functional
programming, so at least some familiarity with this programming style is
assumed. If Pure is your first functional language then you might want
to look at the Functional Programming
(http://en.wikipedia.org/wiki/Functional_programming) wikipedia article
to see what it is all about and find pointers to current literature on
the subject. In any case we hope that you'll find Pure helpful in
exploring functional programming, as it is fairly easy to learn but a
very powerful language.

  As already mentioned, Pure uses term rewriting as its underlying
computational model, which goes well beyond functional programming in
some ways. Term rewriting has long been used in computer algebra
systems, and *note Michael O'Donnell: f. pioneered its use as a
programming language already in the 1980s. But until recently
implementations have not really been efficient enough to be useful as
general-purpose programming languages; Pure strives to change that. A
good introduction to the theory of the term rewriting calculus and its
applications is the book by *note Baader and Nipkow: 10.


File: pure.info,  Node: Typographical Conventions,  Prev: Further Reading,  Up: Introduction

1.2 Typographical Conventions
=============================

Program examples are always set in typewriter font. Here’s how a
typical code sample may look like:

    fact n = if n>0 then n*fact(n-1) else 1;

These can either be saved to a file and then loaded into the
interpreter, or you can also just type them directly in the
interpreter. If some lines start with the interpreter prompt '> ', this
indicates an example interaction with the interpreter. Everything
following the prompt (excluding the '> ' itself) is meant to be typed
exactly as written. Lines lacking the '> ' prefix show results printed
by the interpreter. Example:

    > fact n = if n>0 then n*fact(n-1) else 1;
    > map fact (1..10);
    [1,2,6,24,120,720,5040,40320,362880,3628800]

Similarly, lines starting with the ‘$ ’ prompt indicate shell
interactions.  For instance,

    $ pure

indicates that you should type the command `pure' on your system’s
command line.

  The grammar notation in this manual uses an extended form of BNF
(Backus-Naur form), which looks as follows:

    expression ::= "{" expr_list (";" expr_list)* [";"] "}"
    expr_list  ::= expression (',' expression)*

Parentheses are used to group syntactical elements, while brackets
denote optional elements. We also use the regular expression operators
`*' and `+' to denote repetitions (as usual, `*' denotes zero or more,
`+' one or more repetitions of the preceding element). Terminals
(literal elements such as keywords and delimiters) are enclosed in
double or single quotes.

  These EBNF rules are used for both lexical and syntactical elements,
but note that the former are concerned with entities formed from single
characters and thus tokens are meant to be typed exactly as written,
whereas the latter deal with larger syntactical structures where
whitespace between tokens is generally insignificant.


File: pure.info,  Node: Invoking Pure,  Next: Pure Overview,  Prev: Introduction,  Up: Top

2 Invoking Pure
***************

The Pure interpreter is invoked as follows:

    pure [options ...] [script ...] [-- args ...]
    pure [options ...] -x script [args ...]

Use `pure -h' to get help about the command line options. As already
mentioned, just the `pure' command without any command line parameters
invokes the interpreter in interactive mode, see *note Running
Interactively: 12.  below for details. Some other important ways to
invoke the interpreter are summarized below.

`pure -g'
     Runs the interpreter interactively, with debugging support.

`pure script ...'
     Runs the given scripts in batch mode.

`pure -i script ...'
     Runs the given scripts in batch mode as above, but then enters the
     interactive command loop. (Add *note -g: 13. to also get debugging
     support, and *note -q: 14. to suppress the sign-on message.)

`pure -x script [arg ...]'
     Runs the given script with the given parameters. The script name
     and command line arguments are available in the global `argv'
     variable.

`pure -c script [-o prog]'
     Batch compilation: Runs the given script, compiling it to a native
     executable `prog' (a.out by default).

  Depending on your local setup, there may be additional ways to run
the Pure interpreter. In particular, if you have Emacs Pure mode
installed, then you can just open a script in Emacs and run it with the
`C-c C-k' keyboard command. For Emacs aficionados, this is probably the
most convenient way to execute a Pure script interactively in the
interpreter. Pure mode actually turns Emacs into an advanced IDE
(integrated development environment) for Pure, which offers a lot of
convenient features such as syntax highlighting, automatic indentation,
online help and different ways to interact with the Pure interpreter.

* Menu:

* Options::
* Overview of Operation::
* Compiling Scripts::
* Tagging Scripts::
* Running Interactively::
* Verbosity and Debugging Options::
* Code Generation Options::
* Startup Files::
* Environment::


File: pure.info,  Node: Options,  Next: Overview of Operation,  Up: Invoking Pure

2.1 Options
===========

The interpreter accepts various options which are described in more
detail below.

 -- option: -c
     Batch compilation.

 -- option: --ctags
 -- option: --etags
     Create a tags file in ctags (vi) or etags (emacs) format.

 -- option: --eager-jit
     Enable eager JIT compilation. This requires LLVM 2.7 or later,
     otherwise this flag will be ignored.

 -- option: -fPIC
 -- option: -fpic
     Create position-independent code (batch compilation).

 -- option: -g
     Enable symbolic debugging.

 -- option: -h
 -- option: --help
     Print help message and exit.

 -- option: -i
     Force interactive mode (read commands from stdin).

 -- option: -I directory
     Add a directory to be searched for included source scripts.

 -- option: -L directory
     Add a directory to be searched for dynamic libraries.

 -- option: -l libname
     Library to be linked in batch compilation.

 -- option: --noediting
     Disable command-line editing.

 -- option: -n
 -- option: --noprelude
     Do not load the prelude.

 -- option: --norc
     Do not run the interactive startup files.

 -- option: -o filename
     Output filename for batch compilation.

 -- option: -q
     Quiet startup (suppresses sign-on message in interactive mode).

 -- option: -T filename
     Tags file to be written by *note --ctags: 17. or *note --etags: 18.

 -- option: -u
     Do not strip unused functions in batch compilation.

 -- option: -v[level]
     Set verbosity level. See below for details.

 -- option: --version
     Print version information and exit.

 -- option: -w
     Enable compiler warnings.

 -- option: -x
     Execute script with given command line arguments.

 -- option: --
     Stop option processing and pass the remaining command line
     arguments in the `argv' variable.

  (Besides these, the interpreter also understands a number of other
command line switches for setting various code generation options;
please see *note Code Generation Options: 2b. below for details.)


File: pure.info,  Node: Overview of Operation,  Next: Compiling Scripts,  Prev: Options,  Up: Invoking Pure

2.2 Overview of Operation
=========================

If any source scripts are specified on the command line, they are
loaded and executed, after which the interpreter exits. Otherwise the
interpreter enters the interactive read-eval-print loop, see *note
Running Interactively: 12. below. You can also use the *note -i: 1d.
option to enter the interactive loop (continue reading from stdin) even
after processing some source scripts.

  Options and source files are processed in the order in which they are
given on the command line. Processing of options and source files ends
when either the *note --: 2a. or the *note -x: 29. option is
encountered. The *note -x: 29.  option must be followed by the name of
a script to be executed, which becomes the "main script" of the
application. In either case, any remaining parameters are passed to the
executing script by means of the global `argc' and `argv' variables,
denoting the number of arguments and the list of the actual parameter
strings, respectively. In the case of *note -x: 29. this also includes
the script name as `argv!0'. The *note -x: 29. option is useful, in
particular, to turn Pure scripts into executable programs by including a
"shebang" like the following as the first line in your main script.
(This trick only works with Unix shells, though.)

    #!/usr/local/bin/pure -x

On startup, the interpreter also defines the `version' variable, which
is set to the version string of the Pure interpreter, and the `sysinfo'
variable, which provides a string identifying the host system. These are
useful if parts of your script depend on the particular version of the
interpreter and the system it runs on. (Moreover, Pure 0.21 and later
also define the variable `compiling' which indicates whether the
program is executed in a batch compilation, see *note Compiling
Scripts: 2d. below.)

  If available, the prelude script prelude.pure is loaded by the
interpreter prior to any other definitions, unless the *note -n: 20. or
*note --noprelude: 21. option is specified. The prelude is searched for
in the directory specified with the *note PURELIB: 2f. environment
variable. If the *note PURELIB: 2f. variable is not set, a
system-specific default is used. Relative pathnames of other source
scripts specified on the command line are interpreted relative to the
current working directory. In addition, the executed program may load
other scripts and libraries via a *note using: 31.  declaration in the
source, which are searched for in a number of locations, including the
directories named with the *note -I: 1d. and *note -L: 1e.  options;
see the *note Declarations: 32. and *note C Interface: 33. sections for
details.


File: pure.info,  Node: Compiling Scripts,  Next: Tagging Scripts,  Prev: Overview of Operation,  Up: Invoking Pure

2.3 Compiling Scripts
=====================

The interpreter compiles scripts, as well as definitions that you enter
interactively, automatically. This is done in an incremental fashion,
as the code is needed, and is therefore known as JIT (*just in time*)
compilation.  Thus the interpreter never really "interprets" the source
program or some intermediate representation, it just acts as a frontend
to the compiler, taking care of compiling source code to native machine
code before it gets executed.

  Pure's LLVM backend does "lazy JIT compilation" by default, meaning
that each function (global or local) is compiled no sooner than it is
run for the first time. With the *note --eager-jit: 19. option,
however, it will also compile all other (global or local) functions
that may be called by the compiled function. (The *note PURE_EAGER_JIT:
35. environment variable, when set to any value, has the same effect,
so that you do not have to specify the *note --eager-jit: 19. option
each time you run the interpreter.) Eager JIT compilation may be more
efficient in some cases (since bigger chunks of compilation work can be
done in one go) and less efficient in others (e.g., eager JITing may
compile large chunks of code which aren't actually called later, except
in rare circumstances).

  Note that the eager JIT mode is only available with LLVM 2.7 or later;
otherwise this option will be ignored.

  It is also possible to compile your scripts to native code
beforehand, using the *note -c: 16. batch compilation option. This
options forces the interpreter to non-interactive mode (unless *note
-i: 1d. is specified as well, which overrides *note -c: 16.). Any
scripts specified on the command line are then executed as usual, but
after execution the interpreter takes a snapshot of the program and
compiles it to one of several supported output formats, LLVM assembler
(.ll) or bitcode (.bc), native assembler (.s) or object (.o), or a
native executable, depending on the output filename specified with
*note -o: 23. If the output filename ends in the .ll extension, an LLVM
assembler file is created which can then be processed with the LLVM
toolchain. If the output filename is just '-', the assembler file is
written to standard output, which is useful if you want to pass the
generated code to the LLVM tools in a pipeline. If the output filename
ends in the .bc extension, an LLVM bitcode file is created instead.

  The .ll and .bc formats are supported natively by the Pure
interpreter, no external tools are required to generate these. If the
target is an .s, .o or executable file, the Pure interpreter creates a
temporary bitcode file on which it invokes the LLVM tools *opt* and
*llc* to create a native assembler file, and then uses *gcc* to
assemble and link the resulting program (if requested). You can also
specify additional libraries to be linked into the executable with the
*note -l: 1e. option. If the output filename is omitted, it defaults to
a.out (a.exe on Windows).

  The *note -c: 16. option provides a convenient way to quickly turn a
Pure script into a standalone executable which can be invoked directly
from the shell. One advantage of compiling your script is that this
eliminates the JIT compilation time and thus considerably reduces the
startup time of the program. Another reason to prefer a standalone
executable is that it lets you deploy the program on systems without a
full Pure installation (usually only the runtime library is required on
the target system). On the other hand, compiled scripts also have some
limitations, mostly concerning the use of the built-in `eval' function.
Please see the *note Batch Compilation: b. section for details.

  The `-v64' (or `-v0100') verbosity option can be used to have the
interpreter print the commands it executes during compilation, see
*note Verbosity and Debugging Options: 36. below. When creating an
object file, this also prints the suggested linker command (including
all the dynamic modules loaded by the script, which also have to be
linked in to create a working executable), to which you only have to
add the options describing the desired output file.


File: pure.info,  Node: Tagging Scripts,  Next: Running Interactively,  Prev: Compiling Scripts,  Up: Invoking Pure

2.4 Tagging Scripts
===================

Pure programs often have declarations and definitions of global symbols
scattered out over many different source files. The *note --ctags: 17.
and *note --etags: 18. options let you create a tags
(http://en.wikipedia.org/wiki/Ctags) file which allows you to quickly
locate these items in text editors such as *vi* and *emacs* which
support this feature.

  If *note --ctags: 17. or *note --etags: 18. is specified, the
interpreter enters a special mode in which it only parses source files
without executing them and collects information about the locations of
global symbol declarations and definitions. The collected information
is then written to a tags file in the ctags or etags format used by
*vi* and *emacs*, respectively. The desired name of the tags file can
be specified with the *note -T: 24. option; it defaults to tags for
*note --ctags: 17. and TAGS for *note --etags: 18. (which matches the
default tags file names used by *vi* and *emacs*, respectively).

  The tags file contains information about the global constant,
variable, macro, function and operator symbols of all scripts specified
on the command line, as well as the prelude and other scripts included
via a *note using: 31. clause.  Tagged scripts which are located in the
same directory as the tags file (or, recursively, in one of its
subdirectories) are specified using relative pathnames, while scripts
outside this hierarchy (such as included scripts from the standard
library) are denoted with absolute pathnames. This scheme makes it
possible to move an entire directory together with its tags file and
have the tags information still work in the new location.


File: pure.info,  Node: Running Interactively,  Next: Verbosity and Debugging Options,  Prev: Tagging Scripts,  Up: Invoking Pure

2.5 Running Interactively
=========================

If the interpreter runs in interactive mode, it repeatedly prompts you
for input (which may be any legal Pure code or some special interpreter
commands provided for interactive usage), and prints computed results.
This is also known as the *read-eval-print* loop and is described in
much more detail in the *note Interactive Usage: a. section. To exit
the interpreter, just type the `quit' command or the end-of-file
character (`Ctrl-d' on Unix) at the beginning of the command line.

  The interpreter may also source a few additional interactive startup
files immediately before entering the interactive loop, unless the
*note --norc: 22.  option is specified. First .purerc in the user's
home directory is read, then .purerc in the current working directory.
These are ordinary Pure scripts which can be used to provide additional
definitions for interactive usage. Finally, a .pure file in the current
directory (containing a dump from a previous interactive session) is
loaded if it is present.

  When the interpreter is in interactive mode and reads from a tty,
unless the *note --noediting: 1f. option is specified, commands are
usually read using *readline* or some compatible replacement, providing
completion for all commands listed under *note Interactive Usage: a, as
well as for symbols defined in the running program. When exiting the
interpreter, the command history is stored in ~/.pure_history, from
where it is restored the next time you run the interpreter.

  The interpreter also provides a simple source level debugger when run
in interactive mode, see *note Debugging: 39. for details. To enable
the debugger, you need to specify the *note -g: 13. option when
invoking the interpreter. This option causes your script to run _much_
slower, so you should only use this option if you want to run the
debugger.


File: pure.info,  Node: Verbosity and Debugging Options,  Next: Code Generation Options,  Prev: Running Interactively,  Up: Invoking Pure

2.6 Verbosity and Debugging Options
===================================

The *note -v: 26. option is useful for debugging the interpreter, or if
you are interested in the code your program gets compiled to. The level
argument is optional; it defaults to 1. Seven different levels are
implemented at this time (one more bit is reserved for future
extensions). Only the first two levels will be useful for the average
Pure programmer; the remaining levels are mostly intended for
maintenance purposes.

1 (0x1, 001)
     denotes echoing of parsed definitions and expressions.

2 (0x2, 002)
     adds special annotations concerning local bindings (de Bruijn
     indices, subterm paths; this can be helpful to debug tricky
     variable binding issues).

4 (0x4, 004)
     adds descriptions of the matching automata for the left-hand sides
     of equations (you probably want to see this only when working on
     the guts of the interpreter).

8 (0x8, 010)
     dumps the "real" output code (LLVM assembler, which is as close to
     the native machine code for your program as it gets; you
     definitely don't want to see this unless you have to inspect the
     generated code for bugs or performance issues).

16 (0x10, 020)
     adds debugging messages from the bison(1) parser; useful for
     debugging the parser.

32 (0x20, 040)
     adds debugging messages from the flex(1) lexer; useful for
     debugging the lexer.

64 (0x40, 0100)
     turns on verbose batch compilation; this is useful if you want to
     see exactly which commands get executed during batch compilation
     (*note -c: 16.).

  These values can be or'ed together, and, for convenience, can be
specified in either decimal, hexadecimal or octal. Thus 0xff or 0777
always gives you full debugging output (which isn't likely to be used
by anyone but the Pure developers). Some useful flag combinations for
experts are (in octal) 007 (echo definitions along with de Bruijn
indices and matching automata), 011 (definitions and assembler code)
and 021 (parser debugging output along with parsed definitions).

  Note that the *note -v: 26. option is only applied after the prelude
has been loaded. If you want to debug the prelude, use the *note -n:
20. option and specify the prelude.pure file explicitly on the command
line. Verbose output is also suppressed for modules imported through a
*note using: 31. clause. As a remedy, you can use the interactive
`show' command (see the *note Interactive Usage: a. section) to list
definitions along with additional debugging information.

  The *note -w: 28. option enables some additional warnings which are
useful to check your scripts for possible errors. Right now it will
report implicit declarations of function symbols which might indicate
missing or mistyped symbols that need to be fixed, see *note Symbol
Lookup and Creation: 3a. for details.


File: pure.info,  Node: Code Generation Options,  Next: Startup Files,  Prev: Verbosity and Debugging Options,  Up: Invoking Pure

2.7 Code Generation Options
===========================

Besides the options listed above, the interpreter also understands some
additional command line switches and corresponding environment
variables to control various code generation options. The options take
the form `--opt' and `--noopt', respectively, where `opt' denotes the
option name (see below for a list of supported options). By default,
these options are all enabled; `--noopt' disables the option, `--opt'
reenables it. In addition, for each option `opt' there is also a
corresponding environment variable `PURE_NOOPT' (with the option name
in uppercase) which, when set, disables the option by default. (Setting
this variable to any value will do, the interpreter only checks whether
the variable exists in the environment.)

  For instance, the `checks' option controls stack and signal checks.
Thus `--nochecks' on the command line disables the option, and setting
the `PURE_NOCHECKS' environment variable makes this the default, in
which case you can use `--checks' on the command line to reenable the
option.

  Each code generation option can also be used as a *pragma* (compiler
directive) in source code so that you can control it on a per-rule
basis. The pragma must be on a line by itself, starting in column 1,
and takes the following form (using `--nochecks' as an example):

    #! --nochecks // line-oriented comment may go here

Currently, the following code generation options are recognized:

 -- option: --checks
 -- option: --nochecks
     Enable or disable various extra stack and signal checks. By
     default, the interpreter checks for stack overflows (if the *note
     PURE_STACK: 3e.  environment variable is set) and pending signals
     on entry to every function, see *note Stack Size and Tail
     Recursion: 3f. and *note Handling of Asynchronous Signals: 40. for
     details. This is needed to catch these conditions in a reliable
     way, so we recommend to leave this enabled.  However, these checks
     also make programs run a little slower (typically some 5%, YMMV).
     If performance is critical then you can disable the checks with
     the *note --nochecks: 3c. option. (Even then, a minimal amount of
     checking will be done, usually on entry to every global function.)

 -- option: --const
 -- option: --noconst
     Enable or disable the precomputing of constant values in batch
     compilation (cf. *note Compiling Scripts: 2d.). If enabled (which
     is the default), the values of constants in *note const: 43.
     definitions are precomputed at compile time (if possible) and then
     stored in the generated executable. This usually yields faster
     startup times but bigger executables. You can disable this option
     with *note --noconst: 42. to get smaller executables at the
     expense of slower startup times. Please see the *note Batch
     Compilation: b.  section for an example.

 -- option: --fold
 -- option: --nofold
     Enable or disable constant folding in the compiler frontend. This
     means that constant expressions involving int and double values
     and the usual arithmetic and logical operations on these are
     precomputed at compile time. (This is mostly for cosmetic
     purposes; the LLVM backend will perform this optimization anyway
     when generating machine code.) For instance:

         > foo x = 2*3*x;
         > show foo
         foo x = 6*x;

     Disabling constant folding in the frontend causes constant
     expressions to be shown as you entered them:

         > #! --nofold
         > bar x = 2*3*x;
         > show bar
         bar x = 2*3*x;



 -- option: --tc
 -- option: --notc
     Enable or disable tail call optimization (TCO). TCO is needed to
     make tail-recursive functions execute in constant stack space, so
     we recommend to leave this enabled. However, at the time of this
     writing LLVM's TCO support is still bug-ridden on some platforms,
     so the *note --notc: 47.  option allows you to disable it. (Note
     that TCO can also be disabled when compiling the Pure interpreter,
     in which case these options have no effect; see the installation
     documentation for details.)

  Besides these, there are the following special pragmas affecting the
code generation of some given function, which is specified in the
pragma. These pragmas can only be used in source code, there are no
command line options for them.

 -- option: --eager fun
     Instruct the interpreter to JIT-compile the given function
     eagerly. This means that native code will be created for the
     function, as well as all other (global or local) functions that
     may be called by the compiled function, as soon as the function
     gets recompiled. This avoids the hiccups you get when a function
     is compiled on the fly if it is run for the first time, which is
     particularly useful for functions which are to be run in realtime
     (typically in multimedia applications). Please note that, in
     difference to the *note --eager-jit: 19. option, this feature is
     available for all LLVM versions (it doesn't require LLVM 2.7 or
     later).

 -- option: --required fun
     Inform the batch compiler (cf. *note Compiling Scripts: 2d.) that
     the given function symbol `fun' should never be stripped from the
     program. This is useful, e.g., if a function is never called
     explicitly but only through `eval'. Adding a *note --required: 49.
     pragma for the function then makes sure that the function is
     always linked into the program. Please see the *note Batch
     Compilation: b. section for an example.


File: pure.info,  Node: Startup Files,  Next: Environment,  Prev: Code Generation Options,  Up: Invoking Pure

2.8 Startup Files
=================

The interpreter may source various files during its startup. These are:

 -- Description: ~/.pure_history
     Interactive command history.

 -- Description: ~/.purerc, .purerc, .pure
     Interactive startup files. The latter is usually a dump from a
     previous interactive session.

 -- Description: prelude.pure
     Standard prelude. If available, this script is loaded before any
     other definitions, unless *note -n: 20. was specified.


File: pure.info,  Node: Environment,  Prev: Startup Files,  Up: Invoking Pure

2.9 Environment
===============

Various aspects of the interpreter can be configured through the
following shell environment variables:

 -- envvar: BROWSER
     If the *note PURE_HELP: 4e. variable is not set (see below), this
     specifies a colon-separated list of browsers to try for reading
     the online documentation. See <http://catb.org/~esr/BROWSER/>.

 -- envvar: PURELIB
     Directory to search for library scripts, including the prelude. If *note
     PURELIB: 2f. is not set, it defaults to some location specified at
     installation time.

 -- envvar: PURE_EAGER_JIT
     Enable eager JIT compilation (same as *note --eager-jit: 19.), see
     *note Compiling Scripts: 2d. for details.

 -- envvar: PURE_HELP
     Command used to browse the Pure manual. This must be a browser
     capable of displaying html files. Default is *w3m*.

 -- envvar: PURE_INCLUDE
     Additional directories (in colon-separated format) to be searched
     for included scripts.

 -- envvar: PURE_LIBRARY
     Additional directories (in colon-separated format) to be searched
     for dynamic libraries.

 -- envvar: PURE_MORE
     Shell command to be used for paging through output of the `show'
     command, when the interpreter runs in interactive mode.  `PURE_LESS'
     does the same for evaluation results printed by the interpreter.

 -- envvar: PURE_PS
     Command prompt used in the interactive command loop ("> " by
     default).

 -- envvar: PURE_STACK
     Maximum stack size in kilobytes (default: 0 = unlimited).

  Besides these, the interpreter also understands a number of other
environment variables for setting various code generation options (see
*note Code Generation Options: 2b. above) and commands to invoke
different LLVM compilers on inline code (see *note Inline Code: 55.).


File: pure.info,  Node: Pure Overview,  Next: Rule Syntax,  Prev: Invoking Pure,  Up: Top

3 Pure Overview
***************

Pure is a fairly simple yet powerful language. Programs are basically
collections of term rewriting rules, which are used to reduce
expressions to *normal form* in a symbolic fashion. For convenience,
Pure also offers some extensions to the basic term rewriting calculus,
like global variables and constants, nested scopes of local function
and variable definitions, anonymous functions (lambdas), exception
handling and a built-in macro facility. These are all described below
and in the following sections.

  Most basic operations are defined in the standard _prelude_.  This
includes the usual arithmetic and logical operations, as well as the
basic string, list and matrix functions. The prelude is always loaded
by the interpreter, so that you can start using the interpreter as a
sophisticated kind of desktop calculator right away.  Other useful
operations are provided through separate library modules. Some of
these, like the system interface and the container data structures, are
distributed with the interpreter, others are available as separate
add-on packages from the Pure website. A (very) brief overview of some
of the modules distributed with the Pure interpreter can be found in
the *note Standard Library: 56. section.

  Here's a first example which demonstrates how to define a simple
recursive function in Pure, entered interactively in the interpreter
(note that the '> ' symbol at the beginning of each input line is the
interpreter's default command prompt):

    > // my first Pure example
    > fact 0 = 1;
    > fact n::int = n*fact (n-1) if n>0;
    > let x = fact 10; x;
    3628800


* Menu:

* Lexical Matters::
* Definitions and Symbolic Evaluation::
* Variables in Equations::
* Expression Syntax::
* Special Forms::
* Toplevel::
* Scoping Rules::


File: pure.info,  Node: Lexical Matters,  Next: Definitions and Symbolic Evaluation,  Up: Pure Overview

3.1 Lexical Matters
===================

Pure is a *free-format* language, i.e., whitespace is insignificant
(unless it is used to delimit other symbols). Thus, in contrast to
"layout-based" languages like Haskell, you _must_ use the proper
delimiters (`;') and keywords (*note end: 58.) to terminate definitions
and block structures. In particular, as shown in the example above,
definitions and expressions at the toplevel have to be terminated with
a semicolon, even in interactive mode.

  Comments use the same syntax as in C++: `//' for line-oriented, and
`/* ... */' for multiline comments. The latter must not be nested. Lines
beginning with `#!' are treated as comments, too; as already discussed
above, on Unix-like systems this allows you to add a "shebang" to your
main script in order to turn it into an executable program.

  A few ASCII symbols are reserved for special uses, namely the
semicolon, the "at" symbol `@', the equals sign `=', the backslash `\',
the Unix pipe symbol `|', parentheses `()', brackets `[]' and curly
braces `{}'.  (Among these, only the semicolon is a "hard delimiter"
which is always a lexeme by itself; the other symbols can be used
inside operator symbols.)  Moreover, there are some keywords which
cannot be used as identifiers:

    case       const   def      else    end        extern  if
    infix      infixl  infixr   let     namespace  nonfix  of
    otherwise  outfix  postfix  prefix  private    public  then
    using      when    with

Pure fully supports the *Unicode* character set or, more precisely,
UTF-8.  This is an ASCII extension capable of representing all Unicode
characters, which provides you with thousands of characters from most
of the languages of the world, as well as an abundance of special
symbols for almost any purpose.  If your text editor supports the UTF-8
encoding (most editors do nowadays), you can use all Unicode characters
in your Pure programs, not only inside strings, but also for denoting
identifiers and special operator and constant symbols.

  The customary notations for identifiers, numbers and strings are all
provided. In addition, Pure also allows you to define your own operator
symbols. Identifiers and other symbols are described by the following
grammar rules in EBNF format:

    symbol     ::= identifier | special
    identifier ::= letter (letter | digit)*
    special    ::= punct+
    letter     ::= "A"|...|"Z"|"a"|...|"z"|"_"|...
    digit      ::= "0"|...|"9"
    punct      ::= "!"|"#"|"$"|"%"|"&"|...

Pure uses the following rules to distinguish "punctuation" (which may
only occur in declared operator and constant symbols) and "letters"
(identifier constituents). In addition to the punctuation symbols in
the 7 bit ASCII range, the following code points in the Unicode
repertoire are considered as punctuation: U+00A1 through U+00BF,
U+00D7, U+00F7, and U+20D0 through U+2BFF. This comprises the special
symbols in the Latin-1 repertoire, as well as the Combining Diacritical
Marks for Symbols, Letterlike Symbols, Number Forms, Arrows,
Mathematical Symbols, Miscellaneous Technical Symbols, Control
Pictures, OCR, Enclosed Alphanumerics, Box Drawing, Blocks, Geometric
Shapes, Miscellaneous Symbols, Dingbats, Miscellaneous Mathematical
Symbols A, Supplemental Arrows A, Supplemental Arrows B, Miscellaneous
Mathematical Symbols B, Supplemental Mathematical Operators, and
Miscellaneous Symbols and Arrows. This should cover almost everything
you'd ever want to use in an operator symbol. All other extended
Unicode characters are effectively treated as "letters" which can be
used as identifier constituents. (Charts of all Unicode symbols can be
found at the Code Charts (http://www.unicode.org/charts/) page of the
Unicode Consortium (http://www.unicode.org/).)

  The following are examples of valid identifiers: `foo', `foo_bar',
`FooBar', `BAR', `bar99'. Case is significant in identifiers, so `Bar'
and `bar' are distinct identifiers, but otherwise the case of letters
carries no meaning. Special symbols consist entirely of punctuation,
such as `::='. These may be used as operator symbols, but have to be
declared before they can be used (see *note Symbol Declarations: 5b.).

  Pure also has a notation for qualified symbols which carry a
namespace prefix.  These take the following format (note that no
whitespace is permitted between the namespace prefix and the symbol):

    qualified_symbol     ::= [qualifier] symbol
    qualified_identifier ::= [qualifier] identifier
    qualifier            ::= [identifier] "::" (identifier "::")*

Example: `foo::bar'.

  Number literals come in three flavours: integers, bigints (denoted
with an `L' suffix) and floating point numbers (indicated by the
presence of the decimal point and/or a base 10 scaling factor).
Integers and bigints may be written in different bases (decimal,
binary, octal and hexadecimal), while floating point numbers are always
denoted in decimal.

    number    ::= integer | integer "L" | float
    integer   ::= digit+
                | "0" ("X"|"x") hex_digit+
                | "0" ("B"|"b") bin_digit+
                | "0" oct_digit+
    oct_digit ::= "0"|...|"7"
    hex_digit ::= "0"|...|"9"|"A"|...|"F"|"a"|...|"f"
    bin_digit ::= "0"|"1"
    float     ::= digit+ ["." digit+] exponent
              | digit* "." digit+ [exponent]
    exponent  ::= ("E"|"e") ["+"|"-"] digit+

Examples: `4711', `4711L', `1.2e-3'. Numbers in different bases: `1000'
(decimal), `0x3e8' (hexadecimal), `01750' (octal), `0b1111101000'
(binary).

  String literals are arbitrary sequences of characters enclosed in
double quotes, such as `"Hello, world!"'.

    string ::= '"' char* '"'

Special escape sequences may be used to denote double quotes and
backslashes (`\"', `\\'), control characters (`\b', `\f', `\n', `\r',
`\t', these have the same meaning as in C), and arbitrary Unicode
characters given by their number or XML entity name
(http://www.w3.org/TR/xml-entity-names/) (e.g., `\169', `\0xa9' and
`\&copy;' all denote the Unicode copyright character, code point
U+00A9). As indicated, numeric escapes can be specified in any of the
supported bases for integer literals. For disambiguating purposes,
these can also be enclosed in parentheses. E.g., `"\(123)4"' is a
string consisting of the character `\123' followed by the digit `4'.


File: pure.info,  Node: Definitions and Symbolic Evaluation,  Next: Variables in Equations,  Prev: Lexical Matters,  Up: Pure Overview

3.2 Definitions and Symbolic Evaluation
=======================================

On the surface, Pure is quite similar to other modern functional
languages like *note Haskell: 5e. and *note ML: 5f. But under the hood
it is a much more dynamic language, more akin to Lisp. In particular,
Pure is dynamically typed, so functions can be fully polymorphic and
you can add to the definition of an existing function at any time. For
instance, we can extend our first example above to make the `fact'
function work with floating point numbers, too:

    > fact 0.0 = 1.0;
    > fact n::double = n*fact (n-1) if n>0;
    > fact 10.0;
    3628800.0
    > fact 10;
    3628800

Note the `n::double' construct on the left-hand side of the second
equation, which means that the equation is only to be applied for
(double precision) floating point values `n'. This construct is also
called a "type tag" in Pure parlance, which is actually a simple form
of pattern matching (see below). Similarly, our previous definition at
the beginning of this section employed the `int' tag to indicate that
the `n' parameter is an integer value. The `int' and `double' types are
built into the Pure language, but it is also possible to introduce your
own type tags for user-defined data structures. This will be explained
in more detail under *note Type Tags: 60. in the *note Rule Syntax: 61.
section below.

  Expressions are generally evaluated from left to right, innermost
expressions first, i.e., using call by value semantics. Pure also has a
few built-in special forms (most notably, conditional expressions, the
short-circuit logical connectives *note &&: 62. and *note ||: 63, the
sequencing operator *note $$: 64, the lazy evaluation operator *note &:
65, and the *note quote: 66.) which take some or all of their arguments
unevaluated, using call by name.

  Like in Haskell and ML, functions are often defined by pattern
matching, i.e., the left-hand side of a definition is compared to the
target expression, binding the variables in the pattern to their actual
values accordingly:

    > foo (bar x) = x-1;
    > foo (bar 99);
    98

Due to its term rewriting semantics, Pure goes beyond most other
functional languages in that it can do symbolic evaluations just as
well as "normal" computations:

    > square x = x*x;
    > square 4;
    16
    > square (a+b);
    (a+b)*(a+b)

In fact, leaving aside the built-in support for some common data
structures such as numbers and strings, all the Pure interpreter really
does is evaluate expressions in a symbolic fashion, rewriting
expressions using the equations supplied by the programmer, until no
more equations are applicable. The result of this process is called a
*normal form* which represents the "value" of the original expression.
Keeping with the tradition of term rewriting, there's no distinction
between "defined" and "constructor" function symbols in Pure.
Consequently, any function symbol or operator can be used _anywhere_ on
the left-hand side of an equation, and may act as a constructor symbol
if it happens to occur in a normal form term. This enables you to work
with algebraic rules like associativity and distributivity in a direct
fashion:

    > (x+y)*z = x*z+y*z; x*(y+z) = x*y+x*z;
    > x*(y*z) = (x*y)*z; x+(y+z) = (x+y)+z;
    > square (a+b);
    a*a+a*b+b*a+b*b

Here's another basic symbolic algebra example, which lets you compute
the disjunctive normal form of logical expressions:

    // eliminate double negations:
    ~~a           = a;

    // de Morgan's laws:
    ~(a || b)     = ~a && ~b;
    ~(a && b)     = ~a || ~b;

    // distributivity:
    a && (b || c) = a && b || a && c;
    (a || b) && c = a && c || b && c;

    // associativity:
    (a && b) && c = a && (b && c);
    (a || b) || c = a || (b || c);

Example:

    > a || ~(b || (c && ~d));
    a||~b&&~c||~b&&d

Note that the above isn't possible in languages like Haskell and ML
which always enforce the so-called "constructor discipline", which
stipulates that only pure constructor symbols (without any defining
equations) may occur as a subterm on the left-hand side of a
definition. Thus equational definitions like the above are forbidden in
these languages. It's possible to work around this, but only at the
cost of an extra layer of interpretation, which treats the expressions
to be evaluated as data manipulated by an evaluation function.

  In Pure this extra layer is not necessary, you can just add equations
like the above to your Pure program. In addition, you can also reduce
an expression in a local context of algebraic equations specified in a
*note with: 67. clause.  This can be done with the `reduce' macro
defined in the prelude:

    expand = reduce with
      (a+b)*c = a*c+b*c;
      a*(b+c) = a*b+a*c;
    end;

    factor = reduce with
      a*c+b*c = (a+b)*c;
      a*b+a*c = a*(b+c);
    end;

Example:

    > expand ((a+b)*2);
    a*2+b*2
    > factor (a*2+b*2);
    (a+b)*2



File: pure.info,  Node: Variables in Equations,  Next: Expression Syntax,  Prev: Definitions and Symbolic Evaluation,  Up: Pure Overview

3.3 Variables in Equations
==========================

Taking a look at the above examples, you might have been wondering how
the Pure interpreter figures out what the parameters (a.k.a.
"variables") in an equation are. This is quite obvious in rules
involving just variables and special operator symbols, such as `(x+y)*z
= x*z+y*z'. However, what about an equation like `foo (foo bar) = bar'?
Since most of the time we don't declare any symbols in Pure, how does
the interpreter know that `foo' is a literal function symbol here,
while `bar' is a variable?

  The answer is that the interpreter considers the different positions
in the left-hand side expression of an equation. Basically, a Pure
expression is just a tree formed by applying expressions to other
expressions, with the atomic subexpressions like numbers and symbols at
the leaves of the tree. (This is true even for infix expressions like
`x+y', since in Pure these are always equivalent to a function
application of the form `(+) x y' which has the atomic subterms `(+)',
`x' and `y' at its leaves.)

  Now the interpreter divides the leaves of the expression tree into
"head" (or "function") and "parameter" (or "variable") positions based
on which leaves are leftmost in a function application or not. Thus, in
an expression like `f x y z', `f' is in the head or function position,
while `x', `y' and `z' are in parameter or variable positions. (Note
that in an infix expression like `x+y', `(+)' is the head symbol, not
`x', as the expression is really parsed as `(+) x y', see above.)

  Identifiers in head positions are taken as literal function symbols
by the interpreter, while identifiers in variable positions denote,
well, variables. We also refer to this convention as the *head =
function rule*. It is quite intuitive and lets us get away without
declaring the variables in equations. (There are some corner cases not
covered here, however. In particular, Pure allows you to declare
special constant symbols, if you need a symbol to be recognized as a
literal even if it occurs in a variable position. This is done by means
of a *note nonfix: 69. declaration, see *note Symbol Declarations: 5b.
for details.)


File: pure.info,  Node: Expression Syntax,  Next: Special Forms,  Prev: Variables in Equations,  Up: Pure Overview

3.4 Expression Syntax
=====================

Like in other functional languages, expressions are the central
ingredient of all Pure programs. All computation performed by a Pure
program consists in the evaluation of expressions, and expressions also
form the building blocks of the equational rules which are used to
define the constants, variables, functions and macros of a Pure program.

  Pure's expression syntax can be summarized in the following grammar
rules:

    expr         ::= "\" prim_expr+ "->" expr
             | "case" expr "of" rules "end"
             | expr "when" simple_rules "end"
             | expr "with" rules "end"
             | "if" expr "then" expr "else" expr
             | simple_expr
    simple_expr  ::= simple_expr op simple_expr
                    | op simple_expr
                    | simple_expr op
                    | application
    application  ::= application prim_expr
                    | prim_expr
    rules        ::= rule (";" rule)* [";"]
    simple_rules ::= simple_rule (";" simple_rule)* [";"]

(Note that the *note rule: 6b. and *note simple_rule: 6c. elements are
part of the definition syntax, which is explained in the *note Rule
Syntax: 61. section.)

    prim_expr ::= qualified_symbol
                  | number
                  | string
                  | "(" op ")"
                  | "(" left_op right_op ")"
                  | "(" simple_expr op ")"
                  | "(" op simple_expr ")"
                  | "(" expr ")"
                  | left_op expr right_op
                  | "[" exprs "]"
                  | "{" exprs (";" exprs)* [";"] "}"
                  | "[" expr "|" simple_rules "]"
                  | "{" expr "|" simple_rules "}"
    exprs     ::= expr ("," expr)*
    op        ::= qualified_symbol
    left_op   ::= qualified_symbol
    right_op  ::= qualified_symbol

Typical examples of the different expression types are summarized in the
following table. Note that lambdas bind most weakly, followed by the
special *note case: 6d, *note when: 6e. and *note with: 67. constructs,
followed by conditional expressions (*note if: 6f.-*note then:
70.-*note else: 71.), followed by the simple expressions. Operators are
a part of the simple expression syntax, and are parsed according to
their declared precedences and associativities (cf. *note Symbol
Declarations: 5b.). Function application binds stronger than all
operators. Parentheses can be used to group expressions and override
default precedences as usual.

Type            Example                              Description
-------------------------------------------------------------------------------------- 
Lambda          `\x->x+1'                            anonymous function
Block           `case x of y = z; ... end'           pattern-matching conditional
                `x when y = z; ... end'              local variable definition
                `x with f y = z; ... end'            local function definition
Conditional     `if x then y else z'                 conditional expression
Simple          `x+y', `-x', `x mod y'               operator application
                `sin x', `max a b'                   function application
Primary         `4711', `1.2e-3'                     number
                `"Hello, world!\n"'                  string
                `foo', `x', `(+)'                    function or variable symbol
                `[1,2,3]', `{1,2;3,4}'               list and matrix
                `[x,-y | x=1..n; y=1..m; x<y]'       list comprehension
                `{i==j | i=1..n; j=1..m}'            matrix comprehension

* Menu:

* Primary Expressions::
* Simple Expressions::
* Special Expressions::


File: pure.info,  Node: Primary Expressions,  Next: Simple Expressions,  Up: Expression Syntax

3.4.1 Primary Expressions
-------------------------

The Pure language provides built-in support for machine integers (32
bit), bigints (implemented using *note GMP: 73.), floating point values
(double precision IEEE 754) and character strings (UTF-8 encoded).
These can all be denoted using the corresponding literals described in
*note Lexical Matters: 57. Truth values are encoded as machine
integers; as you might expect, zero denotes _false_ and any non-zero
value _true_, and the prelude also provides symbolic constants `false'
and `true' to denote these. Pure also supports generic C pointers, but
these don't have a syntactic representation in Pure, except that the
predefined constant `NULL' may be used to denote a generic null
pointer; other pointer values need to be created with external C
functions.  Finally, Pure also provides some built-in support for
compound primaries in the form of lists and matrices, although most of
the corresponding operations are actually defined in the prelude.

  Together, these "atomic" types of expressions make up Pure's *primary
expression* syntax. Here is a brief rundown of the primary expression
types.

Numbers: `4711', `4711L', `1.2e-3'
     The usual C notations for integers (decimal: `1000', hexadecimal:
     `0x3e8', octal: `01750') and floating point values are all
     provided.  Integers can also be denoted in base 2 by using the
     `0b' or `0B' prefix: `0b1111101000'. Integer constants that are
     too large to fit into machine integers are promoted to bigints
     automatically.  Moreover, integer literals immediately followed by
     the uppercase letter `L' are always interpreted as bigint
     constants, even if they fit into machine integers.  This notation
     is also used when printing bigint constants, to distinguish them
     from machine integers.

Strings: `"Hello, world!\n"'
     String constants are double-quoted and terminated with a null
     character, like in C. In contrast to C, strings are always encoded
     in UTF-8, and character escapes in Pure strings have a more
     flexible syntax (borrowed from the author's Q language) which
     provides notations to specify any Unicode character. Please refer
     to *note Lexical Matters: 57. for details.

Function and variable symbols: `foo', `foo_bar', `BAR', `foo::bar'
     These consist of the usual sequence of letters (including the
     underscore) and digits, starting with a letter. Case is
     significant, thus `foo', `Foo' and `FOO' are distinct identifiers.
     The '`_'' symbol, when occurring on the left-hand side of an
     equation, is special; it denotes the *anonymous variable* which
     matches any value without actually binding a variable. Identifiers
     can also be prefixed with a namespace identifier, like in
     `foo::bar'. (This requires that the given namespace has already
     been created, as explained under *note Namespaces: 74. in the
     *note Declarations: 32. section.)

Operator and constant symbols: `+', `==', `not'
     For convenience, Pure also provides you with a limited means to
     extend the syntax of the language with special operator and
     constant symbols by means of a corresponding *fixity* declaration,
     as discussed in section *note Symbol Declarations: 5b. Besides the
     usual infix, prefix and postfix operators, Pure also provides
     outfix (bracket) and nonfix (constant) symbols. (Nonfix symbols
     actually work more or less like ordinary identifiers, but the
     *note nonfix: 69. attribute tells the compiler that when such a
     symbol occurs on the left-hand side of an equation, it is always
     to be interpreted as a literal constant, cf. *note Variables in
     Equations: 68.)

     Operator and constant symbols may take the form of an identifier
     or a sequence of punctuation characters. They must always be
     declared before use. Once declared, they are always special, and
     can't be used as ordinary identifiers any more. However, like in
     Haskell, by enclosing an operator in parentheses, such as `(+)' or
     `(not)', you can turn it into an ordinary function symbol. Also,
     operators and constant symbols can be qualified with a namespace
     just like normal identifiers.

          Note: The common operator symbols like `+', `-', `*', `/'
          etc. are all declared at the beginning of the prelude, see the
          `purelib' for a list of these. Arithmetic and relational
          operators mostly follow C conventions. However, out of
          necessity (`!', `&' and `|' are used for other purposes in
          Pure) the logical and bitwise operations, as well as the
          negated equality predicates are named a bit differently: `~',
          `&&' and `||' denote logical negation, conjunction and
          disjunction, while the corresponding bitwise operations are
          named `not', `and' and `or'.  Moreover, following these
          conventions, inequality is denoted `~='. Also note that `&&'
          and `||' are special forms which are evaluated in
          short-circuit mode (see *note Special Forms: 75. below),
          whereas the bitwise connectives receive their arguments using
          call-by-value, just like the other arithmetic operations.

Lists: `[x,y,z]', `x:xs', `x..y', `x:y..z'
     Pure's basic list syntax is the same as in Haskell, thus `[]' is
     the empty list and `x:xs' denotes a list with head element `x' and
     tail list `xs'. The infix constructor symbol '`:'' is declared in
     the prelude. The usual syntactic sugar for list values in brackets
     is provided, thus `[x,y,z]' is exactly the same as `x:y:z:[]'.

     There's also a way to denote arithmetic sequences such as `1..5',
     which denotes the list `[1,2,3,4,5]'. Haskell users should note
     the missing brackets. In contrast to Haskell, Pure doesn't use any
     special syntax for arithmetic sequences, the '`..'' symbol is just
     an ordinary infix operator declared and defined in the prelude.
     Sequences with arbitrary stepsizes can be written by denoting the
     first two sequence elements using the '`:'' operator, as in
     `1.0:1.2..3.0'. To prevent unwanted artifacts due to rounding
     errors, the upper bound in a floating point sequence is always
     rounded to the nearest grid point. Thus, e.g., `0.0:0.1..0.29'
     actually yields `[0.0,0.1,0.2,0.3]', as does `0.0:0.1..0.31'.

Tuples: `x,y,z'
     Pure's tuples are a bit unusual: They are constructed by just
     "pairing" things using the '`,'' operator, for which the empty
     tuple `()' acts as a neutral element (i.e., `(),x' is just `x', as
     is `x,()'). Pairs always associate to the right, meaning that
     `x,y,z == x,(y,z) == (x,y),z', where `x,(y,z)' is the normalized
     representation.  This implies that tuples are always flat, i.e.,
     there are no nested tuples (tuples of tuples); if you need such
     constructs then you should use lists instead.

          Note: Syntactically, tuples aren't really primary
          expressions, but we still include them here because they are
          closely related to lists which are also defined in the
          prelude. Also, tuples are often used as a simpler replacement
          for lists, in particular in function arguments and return
          values, when no elaborate hierarchical structure is needed.

          Also note that parentheses are generally only used to group
          expressions and are _not_ part of the tuple syntax in Pure,
          although they will be needed to include a tuple in a list or
          matrix. E.g., `[(1,2),3,(4,5)]' is a three element list
          consisting of the tuple `1,2', the integer `3', and another
          tuple `4,5'. Likewise, `[(1,2,3)]' is a list with a single
          element, the tuple `1,2,3'.

Matrices: `{1.0,2.0,3.0}', `{1,2;3,4}', `{1L,y+1;foo,bar}'
     Pure also offers matrices, a kind of two-dimensional arrays, as a
     built-in data structure which provides efficient storage and
     element access. These work more or less like their Octave/MATLAB
     equivalents, but using curly braces instead of brackets. As
     indicated, commas are used to separate the columns of a matrix,
     semicolons for its rows. In fact, the `{...}' construct is rather
     general and allows you to construct new matrices from any
     collection of individual elements ("scalars") and submatrices,
     provided that all dimensions match up. Here, any expression which
     doesn't yield a matrix denotes a scalar, which is considered to be
     a 1x1 matrix for the purpose of matrix construction. The comma
     arranges submatrices in columns, while the semicolon arranges them
     in rows. So, if both `x' and `y' are `n'x`m' matrices, then
     `{x,y}' becomes an `n' x `2*m' matrix consisting of all the
     columns of `x' followed by all the columns of `y'. Likewise,
     `{x;y}' becomes a `2*n' x `m' matrix (all the rows of `x' above of
     all rows of `y'). In addition, `{...}' constructs can be nested to
     an arbitrary depth. Thus `{{1;3},{2;4}}' is another way to write
     the 2x2 matrix `{1,2;3,4}' in a kind of "column-major" format
     (however, internally all matrices are stored in C's row-major
     format).

     Pure supports both numeric and symbolic matrices. The former are
     homogeneous arrays of double, complex double or (machine) int
     matrices, while the latter can contain any mixture of Pure
     expressions. Pure will pick the appropriate type for the data at
     hand. If a matrix contains values of different types, or Pure
     values which cannot be stored in a numeric matrix, then a symbolic
     matrix is created instead (this also includes the case of bigints,
     which are considered as symbolic values as far as matrix
     construction is concerned). Numeric matrices use an internal data
     layout that is fully compatible with the *note GNU Scientific
     Library: 76.  (GSL), and can readily be passed to GSL routines via
     the C interface. (The Pure interpreter does not require GSL,
     however, so numeric matrices will work even if GSL is not
     installed.)

     More information about matrices and corresponding examples can be
     found in the *note Examples: 77. section below.

          Note: While the `[...]' and `{...}' constructs look
          superficially similar, they work in very different ways. The
          former is just syntactic sugar for a corresponding
          constructor term and can thus be used as a pattern on the
          left-hand side of an equation, cf. *note Patterns: 78. In
          contrast, the latter is special syntax for a built-in
          operation which creates objects of a special matrix type.
          Thus matrix expressions can _not_ be used as patterns
          (instead, matrix values can be matched as a whole using the
          special `matrix' type tag).

Comprehensions: `[x,y | x=1..n; y=1..m; x<y]', `{f x | x=1..n}'
     Pure provides both list and matrix comprehensions as a convenient
     means to construct list and matrix values from a "template"
     expression and one or more "generator" and "filter" clauses. The
     former bind a pattern to values drawn from a list or matrix, the
     latter are just predicates determining which generated elements
     should actually be added to the result. Both list and matrix
     comprehensions are in fact syntactic sugar for a combination of
     nested lambdas, conditional expressions and "catmaps" (a
     collection of operations which combine list or matrix construction
     and mapping a function over a list or matrix, defined in the
     prelude), but they are often much easier to write.

     Matrix comprehensions work pretty much like list comprehensions,
     but produce matrices instead of lists. List generators in matrix
     comprehensions alternate between row and column generation so that
     most common mathematical abbreviations carry over quite easily.
     Examples of both kinds of comprehensions can be found in the *note
     Examples: 77. section below.


File: pure.info,  Node: Simple Expressions,  Next: Special Expressions,  Prev: Primary Expressions,  Up: Expression Syntax

3.4.2 Simple Expressions
------------------------

The rest of Pure's expression syntax mostly revolves around the notion
of function applications. For convenience, Pure also allows you to
declare pre-, post-, out- and infix operator symbols, but these are in
fact just syntactic sugar for function applications; see *note Symbol
Declarations: 5b. for details.  Function and operator applications are
used to combine primary expressions to compound terms, also referred to
as *simple expressions*; these are the data elements which are
manipulated by Pure programs.

  As in other modern FPLs, function applications are written simply as
juxtaposition (i.e., in "curried" form) and associate to the left. This
means that in fact all functions only take a single argument.
Multi-argument functions are represented as chains of single-argument
functions. For instance, in `f x y = (f x) y' first the function `f' is
applied to the first argument `x', yielding the function `f x' which in
turn gets applied to the second argument `y'. This makes it possible to
derive new functions from existing ones using *partial applications*
which only specify some but not all arguments of a function. For
instance, taking the `max' function from the prelude as an example,
`max 0' is the function which, for a given `x', returns `x' itself if
it is nonnegative and zero otherwise. This works because `(max 0) x =
max 0 x' is the maximum of `0' and `x'.

     Note: The major advantage of having curried function applications
     is that, without any further ado, functions become first-class
     objects. That is, they can be passed around freely both as
     parameters and as function return values. Functions which take
     other functions as arguments and/or yield them as results are also
     known as *higher-order functions* (HOFs). Much of the power of
     functional programming languages stems from this feature, so the
     treatment of functions as first-class values is generally
     considered as one of the defining characteristics of functional
     languages.

  Operator applications are written using prefix, postfix, outfix or
infix notation, as the declaration of the operator demands, but are
just ordinary function applications in disguise. As already mentioned,
enclosing an operator in parentheses turns it into an ordinary function
symbol, thus `x+y' is exactly the same as `(+) x y'. For convenience,
partial applications of infix operators can also be written using
so-called *operator sections*. A _left section_ takes the form `(x+)'
which is equivalent to the partial application `(+) x'. A _right
section_ takes the form `(+x)' and is equivalent to the term `flip (+)
x'. (This uses the `flip' combinator from the prelude which is defined
as `flip f x y = f y x'.) Thus `(x+) y' is equivalent to `x+y', while
`(+x) y' reduces to `y+x'. For instance, `(1/)' denotes the reciprocal
and `(+1)' the successor function. (Note that, in contrast, `(-x)'
always denotes an application of unary minus; the section `(+-x)' can
be used to indicate a function which subtracts `x' from its argument.)


File: pure.info,  Node: Special Expressions,  Prev: Simple Expressions,  Up: Expression Syntax

3.4.3 Special Expressions
-------------------------

Some special notations are provided for conditional expressions as well
as anonymous functions (lambdas) and blocks of local function and
variable definitions.

Conditional expressions: `if x then y else z'
     Evaluates to `y' or `z' depending on whether `x' is "true" (i.e., a
     nonzero integer). An exception is raised if the condition is not an
     integer.

Lambdas: `\x -> y'
     These denote anonymous functions and work pretty much like in
     Haskell.  Pure supports multiple-argument lambdas (e.g, `\x y ->
     x*y'), as well as pattern-matching lambda abstractions which match
     one or more patterns against the lambda arguments, such as `\(x,y)
     -> x*y'. An exception is raised if the actual lambda arguments do
     not match the given patterns.

Case expressions: `case x of rule; ... end'
     Matches an expression, discriminating over a number of different
     cases, similar to the Haskell case construct. The expression `x'
     is matched in turn against each left-hand side pattern in the rule
     list, and the first pattern which matches `x' gives the value of
     the entire expression, by evaluating the corresponding right-hand
     side with the variables in the pattern bound to their
     corresponding values. An exception is raised if the target
     expression doesn't match any of the patterns.

When expressions: `x when rule; ... end'
     An alternative way to bind local variables by matching a
     collection of subject terms against corresponding patterns,
     similar to *note Aardappel: 7b.'s `when' construct. A single
     binding such as `x when u = v end' is equivalent to `case v of u =
     x end', but the former is often more convenient to write. A `when'
     clause may contain multiple definitions, which are processed from
     left to right, so that later definitions may refer to the
     variables in earlier ones. This is exactly the same as several
     nested single definitions, with the first binding being the
     "outermost" one.

With expressions: `x with rule; ... end'
     Defines local functions. Like Haskell's `where' construct, but it
     can be used anywhere inside an expression (just like Aardappel's
     `where', but Pure uses the keyword `with' which better lines up
     with `case' and `when'). Several functions can be defined in a
     single `with' clause, and the definitions can be mutually
     recursive and consist of as many equations as you want.


File: pure.info,  Node: Special Forms,  Next: Toplevel,  Prev: Expression Syntax,  Up: Pure Overview

3.5 Special Forms
=================

As already mentioned, some operations are actually implemented as
special forms which process some or all of their arguments using
call-by-name.

 -- Description: if x then y else z
     The conditional expression is a special form with call-by-name
     arguments `y' and `z'; only one of the branches is actually
     evaluated, depending on the value of `x'.

 -- Function: x && y
 -- Function: x || y
     The logical connectives evaluate their operands in *short-circuit
     mode*.  Thus the second operand is passed by name and will only be
     evaluated if the first operand fails to determine the value of the
     expression. For instance, `x&&y' immediately becomes false if `x'
     evaluates to false; otherwise `y' is evaluated to give the value
     of the expression. The built-in definitions of these operations
     work as if they were defined by the following equations (but note
     that the second operand is indeed passed by name):

         x::int && y = if x then y else x;
         x::int || y = if x then x else y;

     Note that this isn't quite the same as in C, as the results of
     these operations are _not_ normalized, i.e., they may return
     nonzero values other than 1 to denote "true". (This has the
     advantage that these operations can be implemented
     tail-recursively, see *note Stack Size and Tail Recursion: 3f.)
     Thus, if you need a normalized truth value then you'll have to
     make sure that either both operands are already normalized, or
     you'll have to normalize the result yourself. (A quick way to turn
     a machine int `x' into a normalized truth value is to compute
     `~~x' or `x~=0'.)

     Moreover, if the built-in definition fails because the first
     operand is not a machine int, then the second operand will be
     evaluated anyway and the resulting application becomes a normal
     form, which gives you the opportunity to extend these operations
     with your own definitions just like the other built-in operations.
     Note, however, that in this case the operands are effectively
     passed by value.

 -- Function: x $$ y
     The sequencing operator *note $$: 64. evaluates its left operand,
     immediately throws the result away and then goes on to evaluate
     the right operand which gives the result of the entire expression.
     This operator is useful to write imperative-style code such as the
     following prompt-input interaction:

         > using system;
         > puts "Enter a number:" $$ scanf "%g";
         Enter a number:
         21
         21.0

     We mention in passing here that the same effect can be achieved
     with a *note when: 6e. clause, which also allows you to execute a
     function solely for its side-effects and just ignore the return
     value:

         > scanf "%g" when puts "Enter a number:" end;
         Enter a number:
         21
         21.0



 -- Function: x &
     The *note &: 65. operator does lazy evaluation. This is the only
     postfix operator defined in the standard prelude. It turns its
     operand into a kind of parameterless anonymous closure, deferring
     its evaluation. These kinds of objects are also commonly known as
     *thunks* or *futures*. When the value of a future is actually
     needed (during pattern-matching, or when the value becomes an
     argument of a C call), it is evaluated automatically and gets
     memoized, i.e., the computed result replaces the thunk so that it
     only has to be computed once.

     Futures are useful to implement all kinds of lazy data structures
     in Pure, in particular: lazy lists a.k.a. streams. A *stream* is
     simply a list with a thunked tail, which allows it to be infinite.
     The Pure prelude defines many functions for creating and
     manipulating these kinds of objects; further details and examples
     can be found in the *note Examples: 77. section below.

 -- Function: quote x
 -- Function: ' x
     This special form quotes an expression, i.e., `quote x' (or,
     equivalently, `'x') returns just `x' itself without evaluating it.
     The prelude also provides a function `eval' which can be used to
     evaluate a quoted expression at a later time. For instance:

         > let x = '(2*42+2^12); x;
         2*42+2^12
         > eval x;
         4180.0

     This enables some powerful metaprogramming techniques, which
     should be well familiar to Lisp programmers. However, there are
     some notable differences to Lisp's quote, please see *note The
     Quote: 7e. in the *note Examples: 77. section for details and more
     examples.


File: pure.info,  Node: Toplevel,  Next: Scoping Rules,  Prev: Special Forms,  Up: Pure Overview

3.6 Toplevel
============

At the toplevel, a Pure program basically consists of rewriting rules
(which are used to define functions and macros), constant and variable
definitions, and expressions to be evaluated:

    script ::= item*
    item   ::= "let" simple_rule ";"
             | "const" simple_rule ";"
             | "def" simple_rule ";"
             | rule ";"
             | expr ";"

(The syntax of the *note rule: 6b. and *note simple_rule: 6c. elements
is discussed in the *note Rule Syntax: 61. section below. Also, a few
additional toplevel elements are provided in the declaration syntax,
see *note Declarations: 32.)

 -- Description: lhs = rhs;
     Rewriting rules always combine a left-hand side pattern (which
     must be a simple expression) and a right-hand side (which can be
     any kind of Pure expression described above). The same format is
     also used in *note with: 67, *note when: 6e. and *note case: 6d.
     expressions. In toplevel rules, *note with: 67. and *note case:
     6d. expressions, this basic form can also be augmented with a
     condition `if guard' tacked on to the end of the rule, where
     `guard' is an integer expression which determines whether the rule
     is applicable.  Moreover, the keyword `otherwise' may be used to
     denote an empty guard which is always true (this is syntactic
     sugar to point out the "default" case of a definition; the
     interpreter just treats this as a comment). Pure also provides some
     abbreviations for factoring out common left-hand or right-hand
     sides in collections of rules; see the *note Rule Syntax: 61.
     section for details.

 -- Description: def lhs = rhs;
     A rule starting with the keyword *note def: 7f. defines a macro
     function. No guards or multiple left-hand and right-hand sides are
     permitted here. Macro rules are used to preprocess expressions on
     the right-hand side of other definitions at compile time, and are
     typically employed to implement user-defined special forms and
     simple kinds of optimization rules. See the *note Macros: 82.
     section below for details and examples.

 -- Description: let lhs = rhs;
     Binds every variable in the left-hand side pattern to the
     corresponding subterm of the right-hand side (after evaluating
     it). This works like a *note when: 6e. clause, but serves to bind
     global variables occurring free on the right-hand side of other
     function and variable definitions.

 -- Description: const lhs = rhs;
     An alternative form of *note let: 80. which defines constants
     rather than variables. (These are not to be confused with nonfix
     symbols which simply stand for themselves!) Like *note let: 80,
     this construct binds the variable symbols on the left-hand side to
     the corresponding values on the right-hand side (after
     evaluation). The difference is that *note const: 43.  symbols can
     only be defined once, and thus their values do not change during
     program execution. This also allows the compiler to apply some
     special optimizations such as constant folding.

 -- Description: expr;
     A singleton expression at the toplevel, terminated with a
     semicolon, simply causes the given value to be evaluated (and the
     result to be printed, when running in interactive mode).


File: pure.info,  Node: Scoping Rules,  Prev: Toplevel,  Up: Pure Overview

3.7 Scoping Rules
=================

A few remarks about the scope of identifiers and other symbols are in
order here. Like most modern functional languages, Pure uses *lexical*
or *static* binding for local functions and variables. What this means
is that the binding of a local name is completely determined at compile
time by the surrounding program text, and does not change as the
program is being executed. In particular, if a function returns another
(anonymous or local) function, the returned function captures the
environment it was created in, i.e., it becomes a (lexical) *closure*.
For instance, the following function, when invoked with a single
argument `x', returns another function which adds `x' to its argument:

    > foo x = bar with bar y = x+y end;
    > let f = foo 99; f;
    bar
    > f 10, f 20;
    109,119

This works the same no matter what other bindings of `x' may be in
effect when the closure is invoked:

    > let x = 77; f 10, (f 20 when x = 88 end);
    109,119

Global bindings of variable and function symbols work a bit differently,
though. Like many languages which are to be used interactively, Pure
binds global symbols *dynamically*, so that the bindings can be changed
easily at any time during an interactive session. This is mainly a
convenience for interactive usage, but works the same no matter whether
the source code is entered interactively or being read from a script,
in order to ensure consistent behaviour between interactive and batch
mode operation.

  So, for instance, you can easily bind a global variable to a new
value by just entering a corresponding *note let: 80. command:

    > foo x = c*x;
    > foo 99;
    c*99
    > let c = 2; foo 99;
    198
    > let c = 3; foo 99;
    297

This works pretty much like global variables in imperative languages,
but note that in Pure the value of a global variable can only be
changed with a *note let: 80. command at the toplevel. Thus referential
transparency is unimpaired; while the value of a global variable may
change between different toplevel expressions, it will always take the
same value in a single evaluation.

  Similarly, you can also add new equations to an existing function at
any time:

    > fact 0 = 1;
    > fact n::int = n*fact (n-1) if n>0;
    > fact 10;
    3628800
    > fact 10.0;
    fact 10.0
    > fact 1.0 = 1.0;
    > fact n::double = n*fact (n-1) if n>1;
    > fact 10.0;
    3628800.0
    > fact 10;
    3628800

(In interactive mode, it is even possible to completely erase a
definition, see section *note Interactive Usage: a. for details.)

  So, while the meaning of a local symbol never changes once its
definition has been processed, toplevel definitions may well evolve
while the program is being processed, and the interpreter will always
use the latest definitions at a given point in the source when an
expression is evaluated. This means that, even in a script file, you
have to define all symbols needed in an evaluation before entering the
expression to be evaluated.


File: pure.info,  Node: Rule Syntax,  Next: Examples,  Prev: Pure Overview,  Up: Top

4 Rule Syntax
*************

Basically, the same rule syntax is used in all kinds of global and local
definitions. However, some constructs (specifically, *note when: 6e,
*note let: 80, *note const: 43. and *note def: 7f.) use a restricted
rule syntax where no guards or multiple left-hand and right-hand sides
are permitted. The syntax of these elements is captured by the
following grammar rules:

    rule        ::= pattern ("|" pattern)* "=" expr [guard]
             (";" "=" expr [guard])*
    simple_rule ::= pattern = expr | expr
    pattern     ::= simple_expr
    guard       ::= "if" simple_expr
              | "otherwise"
              | guard "when" simple_rules "end"
              | guard "with" rules "end"

When matching against a function or macro call, or the subject term in a
*note case: 6d. expression, the rules are always considered in the
order in which they are written, and the first matching rule (whose
guard evaluates to a nonzero value, if applicable) is picked. (Again,
the *note when: 6e.  construct is treated differently, because each
rule is actually a separate definition.)

* Menu:

* Patterns::
* Type Tags::
* General Rules::
* Simple Rules::


File: pure.info,  Node: Patterns,  Next: Type Tags,  Up: Rule Syntax

4.1 Patterns
============

The left-hand side of a rule is a special kind of simple expression,
called a *pattern*. Patterns consist of function and operator
applications as well as any of the "atomic" expression types (symbols,
numbers, strings and list values). _Not_ permitted are any of the
special expression types (lambda, *note case: 6d, *note when: 6e, *note
with: 67, conditional expressions, as well as list and matrix
comprehensions). For technical reasons, the current implementation also
forbids matrix values in patterns, but it is possible to match a matrix
value as a whole using the `matrix' type tag, see below.

  As already mentioned, the '`_'' symbol is special in patterns; it
denotes the *anonymous variable* which matches an arbitrary value
(independently for all occurrences) without actually binding a
variable. For instance:

    foo _ _ = 0;

This will match the application of `foo' to any combination of two
arguments (and just ignore the values of these arguments).

  Constants in patterns must be matched literally. For instance:

    foo 0 = 1;

This will only match an application of `foo' to the machine integer `0',
not `0.0' or `0L' (even though these compare equal to `0' using the
'`=='' operator).

  In contrast to Haskell, patterns may contain repeated variables
(other than the anonymous variable), i.e., they may be *non-linear*.
Thus rules like the following are legal in Pure, and will only be
matched if all occurrences of the same variable in the left-hand side
pattern are matched to the same value:

    > foo x x = x;
    > foo 1 1;
    1
    > foo 1 2;
    foo 1 2

Non-linear patterns are particularly useful for computer algebra where
you will frequently encounter rules such as the following:

    > x*y+x*z = x*(y+z);
    > a*(3*4)+a*5;
    a*17

The notion of "sameness" employed here is that of syntactical identity,
which means that the matched subterms must be identical in structure
and content.  The prelude provides syntactic equality as a function
`same' and a comparison predicate '`===''. Thus the above definition of
`foo' is roughly equivalent to the following:

    foo x y = x if same x y;

It is important to note the differences between syntactic equality
embodied by `same' and '`==='', and the "semantic" equality operator
'`==''. The former are always defined on all terms, whereas '`=='' is
only available on data where it has been defined explicitly, either in
the prelude or by the programmer. Also note that '`=='' may assert that
two terms are equal even if they are syntactically different. Consider,
e.g.:

    > 0==0.0;
    1
    > 0===0.0;
    0

This distinction is actually quite useful. It gives the programmer the
flexibility to define '`=='' in any way that he sees fit, which is
consistent with the way the other comparison operators like '`<'' and
'`>'' are handled in Pure.

  Patterns may also contain the following special elements which are not
permitted in right-hand side expressions:

   * A Haskell-style *"as" pattern* of the form _variable_ `@' _pattern_
     binds the given variable to the expression matched by the
     subpattern _pattern_ (in addition to the variables bound by
     _pattern_ itself). This is convenient if the value matched by the
     subpattern is to be used on the right-hand side of an equation.

   * A left-hand side variable (including the anonymous variable) may
     be followed by a *type tag* of the form `::' _name_, where _name_
     is either one of the built-in type symbols `int', `bigint',
     `double', `string', `matrix', `pointer', or an existing identifier
     denoting a custom constructor symbol for a user-defined data type.
     The variable can then match only values of the designated type.
     Thus, for instance, '`x::int'' only matches machine integers. See
     the *note Type Tags: 60. section below for details.

  To these ends, the expression syntax is augmented with the following
grammar rule (but note that this form of expression is in fact only
allowed on the left-hand side of a rule):

    prim_expr ::= qualified_identifier
                  ("::" qualified_identifier | "@" prim_expr)

As shown, both "as" patterns and type tags are primary expressions, and
the subpattern of an "as" pattern is a primary expression, too. Thus,
if a compound expression is to be used as the subpattern, it _must_ be
parenthesized. For instance, the following function duplicates the head
element of a list:

    foo xs@(x:_) = x:xs;

Note that if you accidentally forget the parentheses around the
subpattern `x:_', you still get a syntactically correct definition:

    foo xs@x:_ = x:xs;

But this gets parsed as `(foo xs@x):_ = x:xs', which is most certainly
_not_ what you want. It is thus a good idea to just always enclose the
subpattern with parentheses in order to prevent such glitches.

  Another potential pitfall is that the notation `foo::bar' is also
used to denote "qualified symbols" in Pure, cf. *note Namespaces: 74.
Usually this will be resolved correctly, but if `foo' happens to also
be a valid namespace then most likely you'll get an error message about
an undeclared symbol. You can always work around this by adding spaces
around the '`::'' symbol, as in `foo :: bar'. Spaces are never
permitted in qualified symbols, so this makes it clear that the
construct denotes a type tag.


File: pure.info,  Node: Type Tags,  Next: General Rules,  Prev: Patterns,  Up: Rule Syntax

4.2 Type Tags
=============

Type tags are really nothing but a special form of "as" patterns which
restrict variables to given data "domains". Like Lisp, Pure is
essentially a typeless language and doesn't really have a notion of
"data types"; all data belongs to the same universe of terms. Thus the
only way to restrict the type of values which can be matched by a
variable is to provide an "as" pattern which matches objects of the
desired domain and nothing else. However, in the special case of the
built-in types (machine and big integers, double values, strings,
matrices and pointers) there is no way to spell out all "constructors",
as there are infinitely many (or none, as in the case of `matrix' and
`pointer' which are constructed and inspected using special primitives,
but are otherwise "opaque" at the Pure level). As a remedy, an
appropriate type tag makes it possible to match these values.

  In order to generalize this to user-defined domains of data, Pure
adopts the convention that any other tagged variable `x::bar' is just a
shorthand for the "as" pattern `x@(bar _)'. Thus a custom data type can
be represented by designating a special constructor symbol (`bar' in
the above example) which takes the actual data as its single argument.
This works the same no matter what the internal structure of the data
is (which in many cases you wouldn't want to depend on anyway, for the
sake of data abstraction).

  Note that this is merely a convention, but it works reasonably well
and makes up for the fact that Pure doesn't support data types at the
language level.  For instance, we might represent points in the plane
using a constructor symbol `Point' which gets applied to pairs of
coordinates. We equip this data type with an operation `point' to
construct a point from its coordinates, and two operations `xcoord' and
`ycoord' to retrieve the coordinates:

    point x y = Point (x,y);
    xcoord (Point (x,y)) = x;
    ycoord (Point (x,y)) = y;

Now we might define a function `translate' which shifts the coordinates
of a point by a given amount in the x and y directions as follows:

    translate (x,y) p::Point = point (xcoord p+x) (ycoord p+y);

Note the use of `Point' as a type tag on the `p' variable. By these
means, we can ensure that the argument is actually an instance of the
point data type, without knowing anything about the internal
representation. We can use these definitions as follows:

    > let p::Point = point 3 3;
    > p; translate (1,2) p;
    Point (3,3)
    Point (4,5)

Some data types in Pure's standard library (specifically, the container
data types) are actually represented in this fashion, see the `purelib'
for details.


File: pure.info,  Node: General Rules,  Next: Simple Rules,  Prev: Type Tags,  Up: Rule Syntax

4.3 General Rules
=================

The most general type of rule, used in function definitions and *note
case: 6d. expressions, consists of a left-hand side pattern, a
right-hand side expression and an optional guard. The left-hand side of
a rule can be omitted if it is the same as for the previous rule. This
provides a convenient means to write out a collection of equations for
the same left-hand side which discriminates over different conditions:

    lhs       = rhs if guard;
              = rhs if guard;
              ...
              = rhs otherwise;

For instance:

    fact n  = n*fact (n-1) if n>0;
            = 1 otherwise;

Pure also allows a collection of rules with different left-hand sides
but the same right-hand side(s) to be abbreviated as follows:

    lhs       |
              ...
    lhs       = rhs;

This is useful if you need different specializations of the same rule
which use different type tags on the left-hand side variables. For
instance:

    fact n::int    |
    fact n::double |
    fact n         = n*fact(n-1) if n>0;
                   = 1 otherwise;

In fact, the left-hand sides don't have to be related at all, so that
you can also write something like:

    foo x | bar y = x*y;

However, this construct is most useful when using an "as" pattern to
bind a common variable to a parameter value after checking that it
matches one of several possible argument patterns (which is slightly
more efficient than using an equivalent type-checking guard). E.g., the
following definition binds the `xs' variable to the parameter of `foo',
if it is either the empty list or a list starting with an integer:

    foo xs@[] | foo xs@(_::int:_) = ... xs ...;

The same construct also works in *note case: 6d. expressions, which is
convenient if different cases should be mapped to the same value, e.g.:

    case ans of "y" | "Y" = 1; _ = 0; end;

Sometimes it is useful if local definitions (*note when: 6e. and *note
with: 67.) can be shared by the right-hand side and the guard of a
rule. This can be done by placing the local definitions behind the
guard, as follows (we only show the case of a single *note when: 6e.
clause here, but of course there may be any number of *note when: 6e.
and *note with: 67. clauses behind the guard):

    lhs = rhs if guard when defns end;

Note that this is different from the following, which indicates that the
definitions only apply to the guard but not the right-hand side of the
rule:

    lhs = rhs if (guard when defns end);

Conversely, definitions placed _before_ the guard only apply to the
right-hand side but not the guard (no parentheses are required in this
case):

    lhs = rhs when defns end if guard;

An example showing the use of a local variable binding spanning both the
right-hand side and the guard of a rule is the following quadratic
equation solver, which returns the (real) solutions of the equation
`x^2+p*x+q = 0' if the discriminant `d = p^2/4-q' is nonnegative:

    > using math;
    > solve p q = -p/2+sqrt d,-p/2-sqrt d if d>=0 when d = p^2/4-q end;
    > solve 4 2; solve 2 4;
    -0.585786437626905,-3.41421356237309
    solve 2 4

Note that the above definition leaves the case of a negative
discriminant undefined.


File: pure.info,  Node: Simple Rules,  Prev: General Rules,  Up: Rule Syntax

4.4 Simple Rules
================

As already mentioned, *note when: 6e, *note let: 80. and *note const:
43. use a simplified kind of rule syntax which just consists of a
left-hand and a right-hand side separated by the equals sign. In this
case the meaning of the rule is to bind the variables in the left-hand
side of the rule to the corresponding subterms of the value of the
right-hand side. This is also called a *pattern binding*.

  Guards or multiple left-hand or right-hand sides are not permitted in
these rules. However, it is possible to omit the left-hand side if it
is just the anonymous variable '`_'' by itself, indicating that you
don't care about the result. The right-hand side is still evaluated, if
only for its side-effects, which is handy, e.g., for adding debugging
statements to your code. For instance, here is a variation of the
quadratic equation solver which also prints the discriminant after it
has been computed:

    > using math, system;
    > solve p q = -p/2+sqrt d,-p/2-sqrt d if d>=0
    > when d = p^2/4-q; printf "The discriminant is: %g\n" d; end;
    > solve 4 2;
    The discriminant is: 2
    -0.585786437626905,-3.41421356237309
    > solve 2 4;
    The discriminant is: -3
    solve 2 4

Note that simple rules of the same form `lhs = rhs' are also used in
macro definitions (*note def: 7f.), to be discussed in the *note
Macros: 82. section. In this case, however, the rule denotes a real
rewriting rule, not a pattern binding, hence the left-hand side is
mandatory in these rules.


File: pure.info,  Node: Examples,  Next: Declarations,  Prev: Rule Syntax,  Up: Top

5 Examples
**********

Here are a few examples of simple Pure programs.

  The factorial:

    fact n = n*fact (n-1) if n>0;
           = 1 otherwise;
    let facts = map fact (1..10); facts;

The Fibonacci numbers:

    fib n = a when a,b = fibs n end
              with fibs n = 0,1 if n<=0;
                          = case fibs (n-1) of
                              a,b = b,a+b;
                            end;
              end;
    let fibs = map fib (1..30); fibs;

It is worth noting here that Pure performs tail call optimization so
that tail-recursive definitions like the following will be executed in
constant stack space (see *note Stack Size and Tail Recursion: 3f. in
the *note Caveats and Notes: c.  section for more details on this):

    // tail-recursive factorial using an "accumulating parameter"
    fact n = loop 1 n with
      loop p n = if n>0 then loop (p*n) (n-1) else p;
    end;

Here is an example showing how constants are defined and used. Constant
definitions take pretty much the same form as variable definitions with
*note let: 80. (see above), but work more like the definition of a
parameterless function whose value is precomputed at compile time:

    > extern double atan(double);
    > const pi = 4*atan 1.0;
    > pi;
    3.14159265358979
    > foo x = 2*pi*x;
    > show foo
    foo x = 6.28318530717959*x;

Note that the compiler normally computes constant subexpressions at
compile time, such as `2*pi' in the `foo' function. This works with all
simple scalars (machine ints and doubles), see *note Constant
Definitions: 87. for details.

* Menu:

* List Comprehensions::
* Lazy Evaluation and Streams::
* Matrix Computations::
* Symbolic Matrices::
* Record Data::
* The Quote::


File: pure.info,  Node: List Comprehensions,  Next: Lazy Evaluation and Streams,  Up: Examples

5.1 List Comprehensions
=======================

List comprehensions are Pure's main workhorse for generating and
processing all kinds of list values. Here's a well-known example, a
variation of Erathosthenes' classical prime sieve:

    primes n        = sieve (2..n) with
      sieve []      = [];
      sieve (p:qs)  = p : sieve [q | q = qs; q mod p];
    end;

(This definition is actually rather inefficient, there are much better
albeit more complicated implementations of this sieve.)

  For instance:

    > primes 100;
    [2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71,73,79,83,89,97]

If you dare, you can actually have a look at the
catmap-lambda-if-then-else expression the comprehension expanded to:

    > show primes
    primes n = sieve (2..n) with sieve [] = []; sieve (p:qs) = p:sieve
    (catmap (\q -> if q mod p then [q] else []) qs) end;

List comprehensions are also a useful device to organize backtracking
searches. For instance, here's an algorithm for the n queens problem,
which returns the list of all placements of n queens on an n x n board
(encoded as lists of n pairs (i,j) with i = 1..n), so that no two
queens hold each other in check:

    queens n       = search n 1 [] with
      search n i p = [reverse p] if i>n;
                   = cat [search n (i+1) ((i,j):p) | j = 1..n; safe (i,j) p];
      safe (i,j) p = ~any (check (i,j)) p;
      check (i1,j1) (i2,j2)
                   = i1==i2 || j1==j2 || i1+j1==i2+j2 || i1-j1==i2-j2;
    end;

(Again, this algorithm is rather inefficient, see the examples included
in the Pure distribution for a much better algorithm by Libor Spacek.)


File: pure.info,  Node: Lazy Evaluation and Streams,  Next: Matrix Computations,  Prev: List Comprehensions,  Up: Examples

5.2 Lazy Evaluation and Streams
===============================

As already mentioned, lists can also be evaluated in a "lazy" fashion,
by just turning the tail of a list into a future. This special kind of
list is also called a *stream*. Streams enable you to work with
infinite lists (or finite lists which are so huge that you would never
want to keep them in memory in their entirety). E.g., here's one way to
define the infinite stream of all Fibonacci numbers:

    > let fibs = fibs 0L 1L with fibs a b = a : fibs b (a+b) & end;
    > fibs;
    0L:#<thunk 0xb5d54320>

Note the *note &: 65. on the tail of the list in the definition of the
local `fibs' function. This turns the result of `fibs' into a stream,
which is required to prevent the function from recursing into samadhi.
Also note that we work with bigints in this example because the
Fibonacci numbers grow quite rapidly, so with machine integers the
values would soon start wrapping around to negative integers.

  Streams like these can be worked with in pretty much the same way as
with lists. Of course, care must be taken not to invoke "eager"
operations such as `#' (which computes the size of a list) on infinite
streams, to prevent infinite recursion. However, many list operations
work with infinite streams just fine, and return the appropriate stream
results. E.g., the `take' function (which retrieves a given number of
elements from the front of a list) works with streams just as well as
with "eager" lists:

    > take 10 fibs;
    0L:#<thunk 0xb5d54350>

Hmm, not much progress there, but that's just how streams work (or
rather they don't, they're lazy bums indeed!). Nevertheless, the stream
computed with `take' is in fact finite and we can readily convert it to
an ordinary list, forcing its evaluation:

    > list (take 10 fibs);
    [0L,1L,1L,2L,3L,5L,8L,13L,21L,34L]

An easier way to achieve this is to cut a "slice" from the stream:

    > fibs!!(0..10);
    [0L,1L,1L,2L,3L,5L,8L,13L,21L,34L,55L]

Also note that since we bound the stream to a variable, the already
computed prefix of the stream has been memoized, so that this portion
of the stream is now readily available in case we need to have another
look at it later. By these means, possibly costly reevaluations are
avoided, trading memory for execution speed:

    > fibs;
    0L:1L:1L:2L:3L:5L:8L:13L:21L:34L:55L:#<thunk 0xb5d54590>

Let's take a look at some of the other convenience operations for
generating stream values. The prelude defines infinite arithmetic
sequences, using `inf' or `-inf' to denote an upper (or lower) infinite
bound for the sequence, e.g.:

    > let u = 1..inf; let v = -1.0:-1.2..-inf;
    > u!!(0..10); v!!(0..10);
    [1,2,3,4,5,6,7,8,9,10,11]
    [-1.0,-1.2,-1.4,-1.6,-1.8,-2.0,-2.2,-2.4,-2.6,-2.8,-3.0]

Other useful stream generator functions are `iterate', which keeps
applying the same function over and over again, `repeat', which just
repeats its argument forever, and `cycle', which cycles through the
elements of the given list:

    > iterate (*2) 1!!(0..10);
    [1,2,4,8,16,32,64,128,256,512,1024]
    > repeat 1!!(0..10);
    [1,1,1,1,1,1,1,1,1,1,1]
    > cycle [0,1]!!(0..10);
    [0,1,0,1,0,1,0,1,0,1,0]

Moreover, list comprehensions can draw values from streams and return
the appropriate stream result:

    > let rats = [m,n-m | n=2..inf; m=1..n-1; gcd m (n-m) == 1]; rats;
    (1,1):#<thunk 0xb5d54950>
    > rats!!(0..10);
    [(1,1),(1,2),(2,1),(1,3),(3,1),(1,4),(2,3),(3,2),(4,1),(1,5),(5,1)]

Finally, let's rewrite our prime sieve so that it generates the infinite
stream of _all_ prime numbers:

    all_primes      = sieve (2..inf) with
      sieve (p:qs)  = p : sieve [q | q = qs; q mod p] &;
    end;

Note that we can omit the empty list case of `sieve' here, since the
sieve now never becomes empty. Example:

    > let P = all_primes;
    > P!!(0..20);
    [2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71,73]
    > P!299;
    1987

You can also just print the entire stream. This will run forever, so hit
`Ctrl-c' when you get bored:

    > using system;
    > do (printf "%d\n") all_primes;
    2
    3
    5
      ...

(Make sure that you really use the `all_primes' function instead of the
`P' variable to print the stream. Otherwise, because of memoization the
stream stored in `P' will grow with the number of elements printed until
memory is exhausted. Calling `do' on a fresh instance of the stream of
primes allows `do' to get rid of each "cons" cell after having printed
the corresponding stream element.)


File: pure.info,  Node: Matrix Computations,  Next: Symbolic Matrices,  Prev: Lazy Evaluation and Streams,  Up: Examples

5.3 Matrix Computations
=======================

Pure offers a number of basic matrix operations, such as matrix
construction, indexing, slicing, as well as getting the size and
dimensions of a matrix (these are briefly described in the *note
Standard Library: 56. section). However, it does _not_ supply built-in
support for matrix arithmetic and other linear algebra algorithms. The
idea is that these can and should be provided through separate
libraries (please check the Pure website for the pure-gsl module which
is an ongoing project to provide a full GSL interface for the Pure
language).

  But Pure's facilities for matrix and list processing also make it
easy to roll your own, if desired. First, the prelude provides matrix
versions of the common list operations like `map', `foldl', `zip' etc.,
which provide a way to implement common matrix operations. E.g.,
multiplying a matrix `x' with a scalar `a' amounts to mapping the
function `(a*)' to x, which can be done as follows:

    > a * x::matrix = map (a*) x if ~matrixp a;
    > 2*{1,2,3;4,5,6};
    {2,4,6;8,10,12}

Likewise, matrix addition and other element-wise operations can be
realized using `zipwith', which combines corresponding elements of two
matrices using a given binary function:

    > x::matrix + y::matrix = zipwith (+) x y;
    > {1,2,3;4,5,6}+{1,2,1;3,2,3};
    {2,4,4;7,7,9}

Second, matrix comprehensions make it easy to express a variety of
algorithms which would typically be implemented using `for' loops in
conventional programming languages. To illustrate the use of matrix
comprehensions, here is how we can define an operation to create a
square identity matrix of a given dimension:

    > eye n = {i==j | i = 1..n; j = 1..n};
    > eye 3;
    {1,0,0;0,1,0;0,0,1}

Note that the `i==j' term is just a Pure idiom for the Kronecker
symbol. Another point worth mentioning here is that the generator
clauses of matrix comprehensions alternate between row and column
generation automatically, if values are drawn from lists as in the
example above. (More precisely, the last generator, which varies most
quickly, yields a row, the next-to-last one a column of these row
vectors, and so on.) This makes matrix comprehensions resemble
customary mathematical notation very closely.

  Of course, matrix comprehensions can also draw values from other
matrices instead of lists. In this case the block layout of the
component matrices is preserved. For instance:

    > {x,y|x={1,2};y={a,b;c,d}};
    {(1,a),(1,b),(2,a),(2,b);(1,c),(1,d),(2,c),(2,d)}

Note that a matrix comprehension involving filters may fail because the
filtered result isn't a rectangular matrix any more. E.g.,
`{2*x|x={1,2,3,-4};x>0}' works, as does `{2*x|x={-1,2;3,-4};x>0}', but
`{2*x|x={1,2;3,-4};x>0}' doesn't because the rows of the result matrix
have different lengths.

  As a slightly more comprehensive example (no pun intended!), here is a
definition of matrix multiplication in Pure. The building block here is
the "dot" product of two vectors which can be defined as follows:

    > sum = foldl (+) 0;
    > dot x::matrix y::matrix = sum $ zipwith (*) (rowvector x) (rowvector y);
    > dot {1,2,3} {1,0,1};
    4

The general matrix product now boils down to a simple matrix
comprehension which just computes the dot product of all rows of `x'
with all columns of `y' (the `rows' and `cols' functions are prelude
operations found in matrices.pure):

    > x::matrix * y::matrix = {dot u v | u = rows x; v = cols y};
    > {0,1;1,0;1,1}*{1,2,3;4,5,6};
    {4,5,6;1,2,3;5,7,9}

(For the sake of simplicity, this doesn't do much error checking. In
production code you'd check at least the conformance of matrix
dimensions, of course.)

  Well, that was easy. So let's take a look at a more challenging
example, Gaussian elimination, which can be used to solve systems of
linear equations. The algorithm brings a matrix into "row echelon"
form, a generalization of triangular matrices. The resulting system can
then be solved quite easily using back substitution.

  Here is a Pure implementation of the algorithm. Note that the real
meat is in the pivoting and elimination step (`step' function) which is
iterated over all columns of the input matrix. In each step, `x' is the
current matrix, `i' the current row index, `j' the current column
index, and `p' keeps track of the current permutation of the row
indices performed during pivoting. The algorithm returns the updated
matrix `x', row index `i' and row permutation `p'.

    gauss_elimination x::matrix = p,x
    when n,m = dim x; p,_,x = foldl step (0..n-1,0,x) (0..m-1) end;

    // One pivoting and elimination step in column j of the matrix:
    step (p,i,x) j
    = if max_x==0 then p,i,x
      else
        // updated row permutation and index:
        transp i max_i p, i+1,
        {// the top rows of the matrix remain unchanged:
         x!!(0..i-1,0..m-1);
         // the pivot row, divided by the pivot element:
         {x!(i,l)/x!(i,j)                 | l=0..m-1};
         // subtract suitable multiples of the pivot row:
         {x!(k,l)-x!(k,j)*x!(i,l)/x!(i,j) | k=i+1..n-1; l=0..m-1}}
    when
      n,m = dim x; max_i, max_x = pivot i (col x j);
      x = if max_x>0 then swap x i max_i else x;
    end with
      pivot i x       = foldl max (0,0) [j,abs (x!j)|j=i..#x-1];
      max (i,x) (j,y) = if x<y then j,y else i,x;
    end;

Please refer to any good textbook on numerical mathematics for a closer
description of the algorithm. But here is a brief rundown of what
happens in each elimination step: First we find the pivot element in
column `j' of the matrix. (We're doing partial pivoting here, i.e., we
only look for the element with the largest absolute value in column
`j', starting at row `i'. That's usually good enough to achieve
numerical stability.) If the pivot is zero then we're done (the rest of
the pivot column is already zeroed out). Otherwise, we bring it into
the pivot position (swapping row `i' and the pivot row), divide the
pivot row by the pivot, and subtract suitable multiples of the pivot
row to eliminate the elements of the pivot column in all subsequent
rows. Finally we update `i' and `p' accordingly and return the result.

  In order to complete the implementation, we still need the following
little helper functions to swap two rows of a matrix (this is used in
the pivoting step) and to apply a transposition to a permutation
(represented as a list):

    swap x i j = x!!(transp i j (0..n-1),0..m-1) when n,m = dim x end;
    transp i j p = [p!tr k | k=0..#p-1]
    with tr k = if k==i then j else if k==j then i else k end;

Finally, let us define a convenient print representation of double
matrices a la *note Octave: 8d. (the meaning of the *note __show__: 8e.
function is explained in *note The __show__ Function: 8f.):

    using system;
    __show__ x::matrix
    = strcat [printd j (x!(i,j))|i=0..n-1; j=0..m-1] + "\n"
    with printd 0 = sprintf "\n%10.5f"; printd _ = sprintf "%10.5f" end
    when n,m = dim x end if dmatrixp x;

Example:

    > let x = dmatrix {2,1,-1,8; -3,-1,2,-11; -2,1,2,-3};
    > x; gauss_elimination x;
       2.00000   1.00000  -1.00000   8.00000
      -3.00000  -1.00000   2.00000 -11.00000
      -2.00000   1.00000   2.00000  -3.00000
    [1,2,0],
       1.00000   0.33333  -0.66667   3.66667
       0.00000   1.00000   0.40000   2.60000
       0.00000   0.00000   1.00000  -1.00000



File: pure.info,  Node: Symbolic Matrices,  Next: Record Data,  Prev: Matrix Computations,  Up: Examples

5.4 Symbolic Matrices
=====================

As already mentioned, matrices may contain not just numbers but any
kind of Pure value, in which case they become _symbolic_ matrices.
Symbolic matrices are a convenient data structure for storing arbitrary
collections of values which provides fast random access to its members.
In particular, symbolic matrices can also be nested, and thus arrays of
arbitrary dimension can be realized as nested symbolic vectors.
However, you have to be careful when constructing such values, as the
`{...}' construct normally combines submatrices to larger matrices. For
instance:

    > {{1,2},{3,4}};
    {1,2,3,4}

One way to inhibit this "splicing" of the submatrices in a larger
matrix is to use the "quote" operator (cf. *note The Quote: 7e.):

    > '{{1,2},{3,4}};
    {{1,2},{3,4}}

(Note that this result is really different from `{1,2;3,4}'. The latter
is a 2x2 integer matrix, while the former is a symbolic vector a.k.a.
1x2 matrix whose elements happen to be two integer vectors.)

  Unfortunately, the quote operator in fact inhibits evaluation of _all_
embedded subterms which may be undesirable if the matrix expression
contains arithmetic (as in `'{{1+1,2*3}}'), so this method works best
for constant matrices. A more general way to create a symbolic vector
of matrices is provided by the `vector' function from the prelude,
which is applied to a list of the vector elements as follows:

    > vector [{1,2},{3,4}];
    {{1,2},{3,4}}

Calls to the `vector' function can be nested to an arbitrary depth to
obtain higher-dimensional "arrays":

    > vector [vector [{1,2}],vector [{3,4}]];
    {{{1,2}},{{3,4}}}

This obviously becomes a bit unwieldy for higher dimensions, but in
Pure you can easily define yourself some more convenient notation if
you like. For instance, the following macro may be used to define a
pair of "non-splicing" vector brackets:

    > outfix {: :};
    > def {: xs@(_,_) :} = vector (list xs);
    > def {: x :} = vector [x];
    > {:{:{1,2}:},{:{3,4}:}:};
    {{{1,2}},{{3,4}}}

(Both macros and *note outfix: 91. symbol declarations are described
later in the appropriate sections, see *note Macros: 82. and *note
Symbol Declarations: 5b.)


File: pure.info,  Node: Record Data,  Next: The Quote,  Prev: Symbolic Matrices,  Up: Examples

5.5 Record Data
===============

Symbolic matrices also provide a means to represent simple record-like
data, by encoding records as symbolic vectors consisting of "hash
pairs" of the form `key => value'. This kind of data structure is very
convenient to represent aggregates with lots of different components.
Since the components of records can be accessed by indexing with key
values, you don't have to remember which components are stored in which
order, just knowing the keys of the required members is enough. In
contrast, tuples, lists and other kinds of constructor terms quickly
become unwieldy for such purposes.

  The keys used for indexing the record data must be either symbols or
strings, while the corresponding values may be arbitrary Pure values.
The prelude provides some operations on these special kinds of
matrices, which let you retrieve vector elements by indexing and
perform non-destructive updates, see the _Record Functions_ section in
the `purelib' for details. Here are a few examples which illustrate how
to create records and work with them:

    > let r = {x=>5, y=>12};
    > recordp r, member r x;
    1,1
    > r!y; r!![y,x];
    12
    {12,5}
    > insert r (x=>99);
    {x=>99,y=>12}
    > insert ans (z=>77);
    {x=>99,y=>12,z=>77}
    > delete ans z;
    {x=>99,y=>12}

Note the use of the "hash rocket" `=>' which denotes the `key=>value'
associations in a record. The hash rocket is a constructor declared as
an infix operator in the prelude, see the _Prelude_ section in the
`purelib'. There's one caveat here, however. Since neither '`=>'' nor
'`!'' treat their key operand in a special way, you'll have to take care
that the key symbols do not evaluate to something else, as might be the
case if they are bound to a global or local variable or parameterless
function:

    > let u = 99;
    > {u=>u};
    {99=>99}

In the case of global variables and function symbols, you might also
protect the symbol with a quote (see *note The Quote: 7e.):

    > {'u=>u};
    {u=>99}

However, even the quote doesn't save you from local variable
substitution, so you'll just have to rename the local variable in such
cases:

    > {'u=>u} when u = 99 end;
    {99=>99}
    > {'u=>v} when v = 99 end;
    {u=>99}

It's also possible to use strings as keys instead, which may actually
be more convenient in some cases:

    > let r = {"x"=>5, "y"=>12};
    > keys r; vals r;
    {"x","y"}
    {5,12}
    > update r "y" (r!"y"+1);
    {"x"=>5,"y"=>13}

You can also mix strings and symbols as keys in the same record (but
note that strings and symbols are always distinct, so `y' and `"y"' are
really two different keys here):

    > insert r (y=>99);
    {"x"=>5,"y"=>12,y=>99}

As records are in fact just special kinds of matrices, the standard
matrix operations can be used on record values as well. For instance,
the matrix constructor provides an alternative way to quickly augment a
record with a collection of new `key=>value' associations:

    > let r = {x=>5, y=>12};
    > let r = {r, x=>7, z=>3}; r;
    {x=>5,y=>12,x=>7,z=>3}
    > r!x, r!z;
    7,3
    > delete r x;
    {x=>5,y=>12,z=>3}
    > ans!x;
    5

As the example shows, this may produce duplicate keys, but these are
handled gracefully; indexing and updates will always work with the
_last_ association for a given key in the record. If necessary, you can
remove duplicate entries from a record as follows; this will only keep
the last association for each key:

    > record r;
    {x=>7,y=>12,z=>3}

In fact, the `record' operation not only removes duplicates, but also
orders the record entries by keys. This produces a kind of normalized
representation which is useful if you want to compare or combine two
record values irrespective of the ordering of the fields. For instance:

    > record {x=>5, y=>12} === record {y=>12, x=>5};
    1

The `record' function can also be used to construct a normalized record
directly from a list or tuple of hash pairs:

    > record [x=>5, x=>7, y=>12];
    {x=>7,y=>12}

Other matrix operations such as `map', `foldl', etc., and matrix
comprehensions can be applied to records just as easily. This enables
you to perform bulk updates of record data in a straightforward way.
For instance, here's how you can define a function `maprec' which
applies a function to all values stored in a record:

    > maprec f = map (\(u=>v) -> u=>f v);
    > maprec (*2) {x=>5,y=>12};
    {x=>10,y=>24}

Another example: The following `ziprec' function collects pairs of
values stored under common keys in two records (we also normalize the
result here so that duplicate keys are always removed):

    > ziprec x y = record {u=>(x!u,y!u) | u = keys x; member y u};
    > ziprec {a=>3,x=>5,y=>12} {x=>10,y=>24,z=>7};
    {x=>(5,10),y=>(12,24)}

Thus the full power of generic matrix operations is available for
records, which turns them into a very versatile data structure, much
more powerful than records in conventional programming languages which
are usually limited to constructing records and accessing or modifying
their components. Note that since the values stored in records can be
arbitrary Pure values, you can also have mutable records by making use
of Pure's expression references (see _Expression References_ in the
library manual). And of course records can be nested, too:

    > let r = {a => {b=>1,c=>2}, b => 2};
    > r!a, r!b, r!a!b;
    {b=>1,c=>2},2,1



File: pure.info,  Node: The Quote,  Prev: Record Data,  Up: Examples

5.6 The Quote
=============

As already mentioned in *note Special Forms: 75, the *note quote: 66.
operation quotes an expression, so that it can be passed around and
manipulated freely until its value is needed, in which case you can
pass it to the `eval' function to obtain its value. For instance:

    > let x = '(2*42+2^12); x;
    2*42+2^12
    > eval x;
    4180.0

The quote also inhibits evaluation inside matrix values, including the
"splicing" of embedded submatrices:

    > '{1,2+3,2*3};
    {1,2+3,2*3}
    > '{1,{2,3},4};
    {1,{2,3},4}

Lisp programmers will be well familiar with this operation which
enables some powerful metaprogramming techniques. However, there are
some notable differences to Lisp's quote. First, *note quote: 66. only
inhibits the evaluation of global variables, _local_ variables are
substituted as usual:

    > (\x -> '(2*x+1)) 99;
    2*99+1
    > foo x = '(2*x+1);
    > foo 99; foo $ '(7/y);
    2*99+1
    2*(7/y)+1
    > '(x+1) when x = '(2*3) end;
    2*3+1
    > '(2*42+2^n) when n = 12 end;
    2*42+2^12

This may come as a surprise (or even annoyance) to real Lisp weenies,
but it does have its advantages. In particular, it makes it easy to
fill in the variable parts in a quoted "template" expression, without
any need for an arguably complex tool like Lisp's "quasiquote". (It is
quite easy to define quasiquote in Pure if you want it, however. See
the *note Recursive Macros: 94.  section for a simplified version; a
full implementation can be found in the Pure library sources.)

  Another useful feature of Lisp's quasiquote is the capability to
splice arguments into a function application. It is possible to achieve
pretty much the same in Pure with the following variation of the `$'
operator which "curries" its second (tuple) operand:

    infixr 0 $@ ;
    f $@ ()     = f;
    f $@ (x,xs) = f x $@ xs;
    f $@ x      = f x;

Now you can write, e.g.:

    > '(foo 1 2) $@ '(2/3,3/4);
    foo 1 2 (2/3) (3/4)

Second, it is in fact possible to perform arbitrary computations right
in the middle of a quoted expression. This is because *note quote: 66.
only ever quotes *note simple expressions: 79, embedded *note special
expressions: 7a. (conditionals, lambda and the *note case: 6d, *note
when: 6e. and *note with: 67. constructs) are evaluated as usual.
Example:

    > '(x+1 when x = '(2*3) end);
    2*3+1
    > '(2*42+(2^n when n = 2*6 end));
    2*42+4096.0

The downside of this is that there is no way to quote special
expressions.  Macro expansion _is_ inhibited in quoted expressions,
however, so it is possible to work around this limitation by defining a
custom special form (see *note Macros: 82.) to be used as a symbolic
representation for, say, a lambda expression, which reduces to a real
lambda when evaluated. To these ends, the `eval' function can be
invoked with a string argument as follows:

    def lambda x y = eval $ "\\ "+str ('x)+" -> "+str ('y);

Example:

    > let l = 'lambda x (x+1); l;
    lambda x (x+1)
    > let f = eval l; f; f 9;
    #<closure 0x7fdc3ca45be8>
    10

Other special constructs, such as *note case: 6d, *note when: 6e. and
*note with: 67. can be handled in a similar fashion.


File: pure.info,  Node: Declarations,  Next: Macros,  Prev: Examples,  Up: Top

6 Declarations
**************

Pure is a very terse language by design. Usually you don't declare much
stuff, you just define it and be done with it. However, there are a few
constructs which let you declare symbols with special attributes and
manage programs consisting of several source modules:

   * symbol declarations determine "scope" and "fixity" of a symbol;

   * *note extern: 95. declarations specify external C functions
     (described in the *note C Interface: 33. section);

   * *note using: 31. clauses let you include other scripts in a Pure
     script;

   * *note namespace: 96. declarations let you avoid name clashes and
     thereby make it easier to manage large programs consisting of many
     separate modules.

  These are toplevel elements (cf. *note Toplevel: 81.):

    item ::= symbol_decl | extern_decl | using_decl | namespace_decl

The syntax of each of these is described in the following subsections,
except *note extern_decl: 97. which can be found in the *note C
Interface: 33. section.

* Menu:

* Symbol Declarations::
* Modules and Imports::
* Namespaces::


File: pure.info,  Node: Symbol Declarations,  Next: Modules and Imports,  Up: Declarations

6.1 Symbol Declarations
=======================

    symbol_decl ::= scope symbol+ ";"
                    | [scope] fixity symbol+ ";"
    scope       ::= "public" | "private"
    fixity      ::= "nonfix" | "outfix"
               | ("infix"|"infixl"|"infixr"|"prefix"|"postfix") precedence
    precedence  ::= integer | "(" op ")"

Scope declarations take the following form:

 -- Description: public symbol ...;
 -- Description: private symbol ...;

  This declares the listed symbols as public or private, respectively.
Each symbol must either be an identifier or a sequence of punctuation
characters. The latter kind of symbols _must_ always be declared before
use, whereas ordinary identifiers can be used without a prior
declaration in which case they are declared implicitly and default to
public scope, meaning that they are visible everywhere in a program. An
explicit public declaration of ordinary identifiers is thus rarely
needed (unless you want to declare symbols as members of a specific
namespace, see *note Namespaces: 74. below). Symbols can also be
declared private, meaning that the symbol is visible only in the
namespace it belongs to. This is explained in more detail under *note
Private Symbols: 9f. in the *note Namespaces: 74. section below.

  Note that to declare several symbols in a single declaration, you can
list them all with whitespace in between. The same syntax applies to
the other types of symbol declarations discussed below. (Commas are
_not_ allowed as delimiters here, as they may occur as legal symbol
constituents in the list of symbols.) The *note public: 98. and *note
private: 99. keywords can also be used as a prefix in any of the
special symbol declarations discussed below, to specify the scope of
the declared symbols (if the scope prefix is omitted, it defaults to
*note public: 98.).

  The following "fixity" declarations are available for introducing
special operator and constant symbols. This changes the way that these
symbols are parsed and thus provides you with a limited means to extend
the Pure language at the lexical and syntactical level.

 -- Description: infix level symbol ...;
 -- Description: infixl level symbol ...;
 -- Description: infixr level symbol ...;
 -- Description: prefix level symbol ...;
 -- Description: postfix level symbol ...;

  Pure provides you with a theoretically unlimited number of different
precedence levels for user-defined infix, prefix and postfix operators.
Precedence levels are numbered starting at 0; larger numbers indicate
higher precedence. (For practical reasons, the current implementation
does require that precedence numbers can be encoded as 24 bit unsigned
machine integers, giving you a range from 0 to 16777215, but this
should be large enough to incur no real limitations on applications.
Also, the operator declarations in the prelude have been set up to
leave enough "space" between the "standard" levels so that you can
easily sneak in new operator symbols at low, high or intermediate
precedences.)

  On each precedence level, you can declare (in order of increasing
precedence) *note infix: 9a. (binary non-associative), *note infixl:
9b. (binary left-associative), *note infixr: 9c. (binary
right-associative), *note prefix: 9d. (unary prefix) and *note postfix:
9e. (unary postfix) operators. For instance, here is a typical excerpt
from the prelude (the full table can be found in the _Prelude_ section
of the `purelib'):

    infix  1800 < > <= >= == ~= ;
    infixl 2200 + - ;
    infixl 2300 * / div mod ;
    infixr 2500 ^ ;
    prefix 2600 # ;

Instead of denoting the precedence by an explicit integer value, you
can also specify an existing operator symbol enclosed in parentheses.
Thus the following declaration gives the `++' operator the same
precedence as `+':

    infixl (+) ++ ;

The given symbol may be of a different fixity than the declaration, but
it must have a proper precedence level (i.e., it must be an infix,
prefix or postfix symbol). E.g., the following declaration gives `^^'
the same precedence level as the infix `^' symbol, but turns it into a
postfix operator:

    postfix (^) ^^ ;

Pure also provides unary outfix operators, which work like in Wm Leler's
constraint programming language *note Bertrand: a0. These can be
declared as follows:

 -- Description: outfix left right ...;

  Outfix operators let you define your own bracket structures. The
operators must be given as pairs of matching left and right symbols
(which must be distinct). For instance:

    outfix |: :| BEGIN END;

After this declaration you can write bracketed expressions like `|:x:|'
or `BEGIN foo, bar END'. These are always at the highest precedence
level (i.e., syntactically they work like parenthesized expressions).
Just like other operators, you can turn outfix symbols into ordinary
functions by enclosing them in parentheses, but you have to specify the
symbols in matching pairs, such as `(BEGIN END)'.

  Pure also has a notation for "nullary" operators, i.e., "operators
without operands", which are used to denote special constants. These
are introduced using a *note nonfix: 69. declaration:

 -- Description: nonfix symbol ...;

  For instance:

    nonfix red green blue;

Syntactically, these work just like ordinary identifiers, so they may
stand whereever an identifier is allowed (no parentheses are required
to "escape" them). The difference to ordinary identifiers is that
nonfix symbols are always interpreted as literals, even if they occur
in a variable position on the left-hand side of a rule. So, with the
above declaration, you can write something like:

    > foo x = case x of red = green; green = blue; blue = red end;
    > map foo [red,green,blue];
    [green,blue,red]

Thus nonfix symbols are pretty much like nullary constructor symbols in
languages like Haskell. Non-fixity is just a syntactic attribute,
however. Pure doesn't enforce that such values are really "constant",
so you can still write a "constructor equation" like the following:

    > red = blue;
    > map foo [red,green,blue];
    [blue,blue,blue]

Examples for all types of symbol declarations can be found in the
_prelude_ which declares a bunch of standard (arithmetic, relational,
logical) operator symbols as well as the list and pair constructors
'`:'' and '`,'', and a few nonfix symbols (mostly for denoting
different kinds of exceptions).

  One final thing worth noting here is that unary minus plays a special
role in the syntax. Like in Haskell and following mathematical
tradition, unary minus is the only prefix operator symbol which is also
used as an infix operator, and is always on the same precedence level
as binary minus, whose precedence may be chosen freely in the prelude.
(The minus operator is the only symbol which gets that special
treatment; all other operators must have distinct lexical
representations.) Thus, with the standard prelude, `-x+y' will be
parsed as `(-x)+y', whereas `-x*y' is the same as `-(x*y)'. Also note
that the notation `(-)' always denotes the binary minus operator; the
unary minus operation can be denoted using the built-in `neg' function.


File: pure.info,  Node: Modules and Imports,  Next: Namespaces,  Prev: Symbol Declarations,  Up: Declarations

6.2 Modules and Imports
=======================

    using_decl ::= "using" name ("," name)* ";"
    name       ::= qualified_identifier | string

While Pure doesn't offer separate compilation, the *note using: 31.
declaration provides a simple but effective way to assemble a Pure
program from several source modules. It takes the following form (note
that in contrast to symbol declarations, the comma is used as a
delimiter symbol here):

 -- Description: using name, ...;

  This causes each given script to be included in the Pure program at
the given point (if it wasn't already included before), which makes
available all the definitions of the included script in your program.
Note that each included script is loaded only _once_, when the first
*note using: 31. clause for the script is encountered. Nested imports
are allowed, i.e., an imported module may itself import other modules,
etc. A Pure program then basically is the concatenation of all the
source modules given as command line arguments, with other modules
listed in *note using: 31. clauses inserted at the corresponding source
locations.

  (The *note using: 31. clause also has an alternative form which
allows dynamic libraries and LLVM bitcode modules to be loaded, this
will be discussed in the *note C Interface: 33. section.)

  For instance, the following declaration causes the math.pure script
from the standard library to be included in your program:

    using math;

You can also import multiple scripts in one go:

    using array, dict, set;

Moreover, Pure provides a notation for qualified module names which can
be used to denote scripts located in specific package directories, e.g.:

    using examples::libor::bits;

In fact this is equivalent to the following *note using: 31. clause
which spells out the real filename of the script between double quotes
(the `.pure' suffix can also be omitted in which case it is added
automatically):

    using "examples/libor/bits.pure";

Both notations can be used interchangeably; the former is usually more
convenient, but the latter allows you to denote scripts whose names
aren't valid Pure identifiers.

  Script identifiers are translated to the corresponding filenames by
replacing the '`::'' symbol with the pathname separator '`/'' and
tacking on the '`.pure'' suffix. The following table illustrates this
with a few examples.

Script identifier               Filename
-------------------------------------------------------------------- 
`math'                          `"math.pure"'
`examples::libor::bits'         `"examples/libor/bits.pure"'
`::pure::examples::hello'       `"/pure/examples/hello.pure"'

  Note the last example, which shows how an absolute pathname can be
denoted using a qualifier starting with '`::''.

  Unless an absolute pathname is given, the interpreter performs a
search to locate the script. The search algorithm considers the
following directories in the given order:

   * the directory of the current script, which is the directory of the
     script containing the *note using: 31. clause, or the current
     working directory if the clause was read from standard input (as
     is the case, e.g., in an interactive session);

   * the directories named in *note -I: 1d. options on the command line
     (in the given order);

   * the colon-separated list of directories in the *note PURE_INCLUDE:
     50.  environment variable (in the given order);

   * finally the directory named by the *note PURELIB: 2f. environment
     variable.

  Note that the current working directory is not searched by default
(unless the *note using: 31. clause is read from standard input), but
of course you can force this by adding the option *note -I: 1d. to the
command line, or by including '.' in the *note PURE_INCLUDE: 50.
variable.

  The directory of the current script (the first item above) can be
skipped by specifying the script to be loaded as a filename in double
quotes, prefixed with the special `sys:' tag. The search then starts
with the "system" directories (*note -I: 1d, *note PURE_INCLUDE: 50. and *note
PURELIB: 2f.)  instead. This is useful, e.g., if you want to provide
your own custom version of a standard library script which in turn
imports that library script. For instance, a custom version of
math.pure might employ the following *note using: 31. clause to load
the math.pure script from the Pure library:

    using "sys:math";
    // custom definitions go here
    log2 x = ln x/ln 2;

The interpreter compares script names (to determine whether two scripts
are actually the same) by using the _canonicalized_ full pathname of
the script, following symbolic links to the destination file (albeit
only one level). Thus different scripts with the same basename, such as
foo/utils.pure and bar/utils.pure can both be included in the same
program (unless they link to the same file).

  More precisely, canonicalizing a pathname involves the following
steps:

   * relative pathnames are expanded to absolute ones, using the search
     rules discussed above;

   * the directory part of the pathname is normalized to the form
     returned by the `getcwd' system call;

   * the ".pure" suffix is added if needed;

   * if the resulting script name is actually a symbolic link, the
     interpreter follows that link to its destination, albeit only one
     level. (This is only done on Unix-like systems.)

  The directory of the canonicalized pathname is also used when
searching other scripts included in a script. This makes it possible to
have an executable script with a shebang line in its own directory,
which is then executed via a symbolic link placed on the system `PATH'.
In this case the script search performed in *note using: 31. clauses
will use the real script directory and thus other required scripts can
be located there. This is the recommended practice for installing
standalone Pure applications in source form which are to be run
directly from the shell.


File: pure.info,  Node: Namespaces,  Prev: Modules and Imports,  Up: Declarations

6.3 Namespaces
==============

    namespace_decl ::= "namespace" [name] ";"
                       | "namespace" name "with" item+ "end" ";"
                       | "using" "namespace" [name_spec ("," name_spec)*] ";"
    name_spec      ::= name ["(" symbol+ ")"]

To facilitate modular development, Pure also provides namespaces as a
means to avoid name clashes between symbols, and to keep the global
namespace tidy and clean. Namespaces serve as containers holding groups
of related identifiers and other symbols. Inside each namespace,
symbols must be unique, but the same symbol may be used to denote
different objects (variables, functions, etc.) in different namespaces.
(Pure's namespace system was heavily inspired by C++ and works in a
very similar fashion. So if you know C++ you should feel right at home
and skimming this section to pick up Pure's syntax of the namespace
constructs should be enough to start using it.)

  The global namespace is always available. By default, new symbols are
created in this namespace, which is also called the *default
namespace*. Additional namespaces can be created with the *note
namespace: 96. declaration, which also switches to the given namespace
(makes it the _current_ namespace), so that new symbols are then
created in that namespace rather than the default one. The current
namespace also applies to all kinds of symbol declarations, including
operator and constant symbol declarations, as well as *note extern: 95.
declarations (the latter are described in the *note C Interface: 33.
section).

  The basic form of the *note namespace: 96. declaration has the
following syntax (there's also a "scoped" form of the *note namespace:
96. declaration which will be discussed in *note Scoped Namespaces: a8.
at the end of this section):

    namespace name;
    // declarations and definitions in namespace 'name'
    namespace;

The second form switches back to the default namespace. For instance,
in order to define two symbols with the same print name `foo' in two
different namespaces `foo' and `bar', you can write:

    namespace foo;
    foo x = x+1;
    namespace bar;
    foo x = x-1;
    namespace;

We can now refer to the symbols we just defined using *qualified
symbols* of the form `namespace::symbol':

    > foo::foo 99;
    100
    > bar::foo 99;
    98

This avoids any potential name clashes, since the qualified identifier
notation always makes it clear which namespace the given identifier
belongs to.

  A namespace can be "reopened" at any time to add new symbols and
definitions to it. This allows namespaces to be created that span
several source modules. You can also create several different
namespaces in the same module.

  Similar to the *note using: 31. declaration, a *note namespace: 96.
declaration accepts either identifiers or double-quoted strings as
namespace names. E.g., the following two declarations are equivalent:

    namespace foo;
    namespace "foo";

The latter form also allows more descriptive labels which aren't
identifiers, e.g.:

    namespace "Private stuff, keep out!";

Note that the namespace prefix in a qualified identifier must be a legal
identifier, so it isn't possible to access symbols in namespaces with
such descriptive labels in a direct fashion. The only way to get at the
symbols in this case is to use a *note namespace: 96. or *note using
namespace: a9.  declaration (for the latter see *note Using Namespaces:
aa. below).

* Menu:

* Using Namespaces::
* Symbol Lookup and Creation::
* Private Symbols::
* Hierarchical Namespaces::
* Scoped Namespaces::


File: pure.info,  Node: Using Namespaces,  Next: Symbol Lookup and Creation,  Up: Namespaces

6.3.1 Using Namespaces
----------------------

Since it is rather inconvenient if you always have to write identifiers
in their qualified form outside of their "home" namespace, Pure allows
you to specify a list of _search_ namespaces which are used to look up
symbols not in the default or the current namespace. This is done with
the *note using namespace: a9. declaration, which takes the following
form:

    using namespace name1, name2, ...;
    // ...
    using namespace;

(As with *note namespace: 96. declarations, the second form without any
namespace arguments gets you back to the default empty list of search
namespaces.)

  For instance, consider this example:

    namespace foo;
    foo x = x+1;
    namespace bar;
    foo x = x-1;
    bar x = x+1;
    namespace;

The symbols in these namespaces can be accessed unqualified as follows:

    > using namespace foo;
    > foo 99;
    100
    > using namespace bar;
    > foo 99;
    98
    > bar 99;
    100

This method is often to be preferred over opening a namespace with the
*note namespace: 96. declaration, since *note using namespace: a9. only
gives you "read access" to the imported symbols, so you can't
accidentally mess up the definitions of the namespace you're using.
Another advantage is that the *note using namespace: a9. declaration
also lets you search multiple namespaces at once:

    using namespace foo, bar;

Be warned, however, that this brings up the very same issue of name
clashes again:

    > using namespace foo, bar;
    > foo 99;
    <stdin>, line 15: symbol 'foo' is ambiguous here

In such a case you'll have to resort to using namespace qualifiers
again, in order to resolve the name clash:

    > foo::foo 99;
    100

To avoid this kind of mishap, you can also selectively import just a few
symbols from a namespace instead. This can be done with a declaration
of the following form:

    using namespace name1 ( sym1 sym2 ... ), name2 ... ;

As indicated, the symbols to be imported can optionally be placed as a
whitespace-delimited list inside parentheses, following the
corresponding namespace name. For instance:

    > using namespace foo, bar (bar);
    > foo 99;
    100
    > bar 99;
    100
    > bar::foo 99;
    98

Note that now we have no clash on the `foo' symbol any more, because we
restricted the import from the `bar' namespace to the `bar' symbol, so
that `bar::foo' has to be denoted with a qualified symbol now.


File: pure.info,  Node: Symbol Lookup and Creation,  Next: Private Symbols,  Prev: Using Namespaces,  Up: Namespaces

6.3.2 Symbol Lookup and Creation
--------------------------------

Pure's rules for looking up and creating symbols are fairly
straightforward and akin to those in other languages featuring
namespaces. However, there are some intricacies involved, because the
rewriting rule format of definitions allows "referential" use of
symbols not only in the "body" (right-hand side) of a definition, but
also in the left-hand side patterns. We discuss this in detail below.

  The compiler searches for symbols first in the current namespace (if
any), then in the currently active search namespaces (if any), and
finally in the default (i.e., the global) namespace, in that order.
This automatic lookup can be bypassed by using an _absolute_ namespace
qualifier of the form `::foo::bar'. In particular, `::bar' always
denotes the symbol `bar' in the default namespace, while `::foo::bar'
denotes the symbol `bar' in the `foo' namespace. (Normally, the latter
kind of notation is only needed if you have to deal with nested
namespaces, see *note Hierarchical Namespaces: ab.  below.)

  If no existing symbol is found, a new symbol is created
automatically, by implicitly declaring a public symbol with default
attributes. New _unqualified_ symbols are always created in the current
namespace, while new _qualified_ symbols are created in the namespace
given by the namespace prefix of the symbol. However, note that in the
latter case the compiler always checks that the given namespace prefix
matches the current namespace:

    > namespace foo;
    > namespace;
    > foo::bar x = 1/x;
    <stdin>, line 3: undeclared symbol 'foo::bar'

Thus it's only possible to introduce a new symbol in a given namespace
if that namespace is the current one. These error messages are somewhat
annoying, but they provide at least some protection against typos and
other silly mistakes and prevent you from accidentally clobbering the
contents of other namespaces. To make these errors go away it's enough
to just declare the symbols in their proper namespaces.

  New symbols are also created if a global unqualified (and yet
undeclared) symbol is being "defined" in a rewriting rule or *note let:
80./*note const: 43. definition, even if a symbol with the same print
name from another namespace is already visible in the current scope. To
distinguish "defining" from "referring" uses of a global symbol, Pure
uses the following (purely syntactic) notions:

   * A *defining occurrence* of a global _function_ or _macro symbol_
     is any occurrence of the symbol as the _head symbol_ on the
     left-hand side of a rewriting rule.

   * A *defining occurrence* of a global _variable_ or _constant
     symbol_ is any occurrence of the symbol in a _variable position_
     (as given by the "head = function" rule, cf. *note Variables in
     Equations: 68.) on the left-hand side of a *note let: 80. or *note
     const: 43. definition.

   * All other occurrences of global symbols on the left-hand side, as
     well as _all_ symbol occurrences on the right-hand side of a
     definition are *referring occurrences*.

  The following example illustrates these notions:

    namespace foo;
    bar (bar x) = bar x;
    let x,y = 1,2;
    namespace;

Here, the first occurrence of `bar' on the left-hand side `bar (bar x)'
of the first rule is a _defining_ occurrence, as are the occurrences of
`x' and `y' on the left-hand side of the *note let: 80. definition.
Hence these symbols are created as new symbols in the namespace `foo'.
On the other hand, the other occurrences of `bar' in the first rule, as
well as the '`,'' symbol on the left-hand side of the *note let: 80.
definition are _referring_ occurrences. In the former case, `bar'
refers to the `bar' symbol defined by the rule, while in the latter
case the '`,'' operator is actually declared in the prelude and thus
imported from the global namespace.

  As an additional safety measure against missing or mistyped symbols,
the interpreter provides the option *note -w: 28. (see *note Invoking
Pure: 8.) to check your scripts for non-defining uses of undeclared
unqualified function symbols. For instance:

    $ pure -w
    > puts "bla"; // missing import of system module
    <stdin>, line 1: warning: implicit declaration of 'puts'
    puts "bla"

For legitimate uses (such as forward uses of a symbol which is defined
later), you can make these warnings go away by declaring the symbol
before using it.

  Note that special operator (and nonfix) symbols _always_ require an
explicit declaration. This works as already discussed in the *note
Symbol Declarations: 5b.  section, except that you first switch to the
appropriate namespace before declaring the symbols. For instance, here
is how you can create a new `+' operation which multiplies its operands
rather than adding them:

    > namespace my;
    > infixl 2200 +;
    > x+y = x*y;
    > 5+7;
    35

Note that the new `+' operation really belongs to the namespace we
created. The `+' operation in the default namespace works as before,
and in fact you can use qualified symbols to pick the version that you
need:

    > namespace;
    > 5+7;
    12
    > 5 ::+ 7;
    12
    > 5 my::+ 7;
    35

Here's what you get if you happen to forget the declaration of the `+'
operator:

    > namespace my;
    > x+y = x*y;
    <stdin>, line 2: infixl symbol '+' was not declared in this namespace

Thus the compiler will never create a new instance of an operator
symbol on the fly, an explicit declaration is always needed in such
cases.

  Note that if you _really_ wanted to redefine the global `+' operator,
you can do this even while the `my' namespace is current. You just have
to use a qualified identifier in this case, as follows:

    > namespace my;
    > x ::+ y = x*y;
    > a+b;
    a*b

This should rarely be necessary (in the above example you might just as
well enter this rule while in the global namespace), but it can be
useful in some circumstances.  Specifically, you might want to
"overload" a global function or operator with a definition that makes
use of private symbols of a namespace (which are only visible inside
that namespace; see *note Private Symbols: 9f.  below). For instance:

    > namespace my;
    > private bar;
    > bar x y = x*y;
    > x ::+ y = bar x y;
    > a+b;
    a*b

(The above is a rather contrived example, since the very same
functionality can be accomplished much easier, but there are some
situations where this approach is necessary.)


File: pure.info,  Node: Private Symbols,  Next: Hierarchical Namespaces,  Prev: Symbol Lookup and Creation,  Up: Namespaces

6.3.3 Private Symbols
---------------------

Pure also allows you to have private symbols, as a means to hide away
internal operations which shouldn't be accessed directly outside the
namespace in which they are declared. The scope of a private symbol is
confined to its namespace, i.e., the symbol is only visible when its
"home" namespace is current. Symbols are declared private by using the
*note private: 99. keyword in the symbol declaration:

    > namespace secret;
    > private baz;
    > // 'baz' is a private symbol in namespace 'secret' here
    > baz x = 2*x;
    > // you can use 'baz' just like any other symbol here
    > baz 99;
    198
    > namespace;

Note that, at this point, `secret::baz' is now invisible, even if you
have `secret' in the search namespace list:

    > using namespace secret;
    > // this actually creates a 'baz' symbol in the default namespace:
    > baz 99;
    baz 99
    > secret::baz 99;
    <stdin>, line 27: symbol 'secret::baz' is private here

The only way to bring the symbol back into scope is to make the `secret'
namespace current again:

    > namespace secret;
    > baz 99;
    198
    > secret::baz 99;
    198



File: pure.info,  Node: Hierarchical Namespaces,  Next: Scoped Namespaces,  Prev: Private Symbols,  Up: Namespaces

6.3.4 Hierarchical Namespaces
-----------------------------

Namespace identifiers can themselves be qualified identifiers in Pure,
which enables you to introduce a hierarchy of namespaces. This is
useful, e.g., to group related namespaces together under a common
"umbrella" namespace:

    namespace my;
    namespace my::old;
    foo x = x+1;
    namespace my::new;
    foo x = x-1;

Note that the namespace `my', which serves as the parent namespace,
must be created before the `my::old' and `my::new' namespaces, even if
it does not contain any symbols of its own. After these declarations,
the `my::old' and `my::new' namespaces are part of the `my' namespace
and will be considered in name lookup accordingly, so that you can
write:

    > using namespace my;
    > old::foo 99;
    100
    > new::foo 99;
    98

This works pretty much like a hierarchy of directories and files, where
the namespaces play the role of the directories (with the default
namespace as the root directory), the symbols in each namespace
correspond to the files in a directory, and the *note using namespace:
a9. declaration functions similar to the shell's `PATH' variable.

  Sometimes it is necessary to tell the compiler to use a symbol in a
specific namespace, bypassing the usual symbol lookup mechanism. For
instance, suppose that we introduce another _global_ `old' namespace
and define yet another version of `foo' in that namespace:

    namespace old;
    foo x = 2*x;
    namespace;

Now, if we want to access that function, with `my' still active as the
search namespace, we cannot simply refer to the new function as
`old::foo', since this name will resolve to `my::old::foo' instead. As
a remedy, the compiler accepts an *absolute* qualified identifier of
the form `::old::foo'. This bypasses name lookup and thus always yields
exactly the symbol in the given namespace (if it exists; as mentioned
previously, the compiler will complain about an undeclared symbol
otherwise):

    > old::foo 99;
    100
    > ::old::foo 99;
    198

Also note that, as a special case of the absolute qualifier notation,
`::foo' always denotes the symbol `foo' in the default namespace.


File: pure.info,  Node: Scoped Namespaces,  Prev: Hierarchical Namespaces,  Up: Namespaces

6.3.5 Scoped Namespaces
-----------------------

Pure also provides an alternative scoped *note namespace: 96. construct
which makes nested namespace definitions more convenient. This
construct takes the following form:

    namespace name with ... end;

The part between *note with: 67. and *note end: 58. may contain
arbitrary declarations and definitions, using the same syntax as the
toplevel. These are processed in the context of the given namespace, as
if you had written:

    namespace name;
    ...
    namespace;

However, the scoped namespace construct always returns you to the
namespace which was active before, and thus these declarations may be
nested:

    namespace foo with
      // declarations and definitions in namespace foo
      namespace bar with
        // declarations and definitions in namespace bar
      end;
      // more declarations and definitions in namespace foo
    end;

Note that this kind of nesting does not necessarily imply a namespace
hierarchy as discussed in *note Hierarchical Namespaces: ab. However,
you can achieve this by using the appropriate qualified namespace names:

    namespace foo with
      // ...
      namespace foo::bar with
        // ...
      end;
      // ...
    end;

Another special feature of the scoped namespace construct is that *note
using namespace: a9. declarations are always local to the current
namespace scope (and other nested namespace scopes inside it). Thus the
previous setting is restored at the end of each scope:

    using namespace foo;
    namespace foo with
      // still using namespace foo here
      using namespace bar;
      // now using namespace bar
      namespace bar with
        // still using namespace bar here
        using namespace foo;
        // now using namespace foo
      end;
      // back to using namespace bar
    end;
    // back to using namespace foo at toplevel

Finally, here's a more concrete example which shows how scoped
namespaces might be used to declare two namespaces and populate them
with various functions and operators:

    namespace foo with
      infixr (::^) ^;
      foo x = x+1;
      bar x = x-1;
      x^y = 2*x+y;
    end;

    namespace bar with
      outfix <: :>;
      foo x = x+2;
      bar x = x-2;
    end;

    using namespace foo(^ foo), bar(bar <: :>);

    // namespace foo
    foo x;
    x^y;

    // namespace bar
    bar x;
    <: x,y :>;

Pure's namespaces can thus be used pretty much like "modules" or
"packages" in languages like Ada or Modula-2. They provide a structured
way to describe program components offering collections of related data
and operations, which can be brought into scope in a controlled way by
making judicious use of *note using namespace: a9. declarations. They
also provide an abstraction barrier, since internal operations and data
structures can be hidden away employing private symbols.

  Please note that these facilities are not Pure's main focus and thus
they are somewhat limited compared to programming languages
specifically designed for big projects and large teams of developers.
Nevertheless they should be useful if your programs grow beyond a small
collection of simple source modules, and enable you to manage most Pure
projects with ease.


File: pure.info,  Node: Macros,  Next: Exception Handling,  Prev: Declarations,  Up: Top

7 Macros
********

Macros are a special type of functions to be executed as a kind of
"preprocessing stage" at compile time. In Pure these are typically used
to define custom special forms and to perform inlining of function
calls and other simple kinds of source-level optimizations.

  Whereas the macro facilities of most programming languages simply
provide a kind of textual substitution mechanism, Pure macros operate
on symbolic expressions and are implemented by the same kind of
rewriting rules that are also used to define ordinary functions in
Pure. In contrast to these, macro rules start out with the keyword
*note def: 7f, and only simple kinds of rules without any guards or
multiple left-hand and right-hand sides are permitted.

  Syntactically, a macro definition looks just like a variable or
constant definition, using *note def: 7f. in lieu of *note let: 80. or
*note const: 43, but they are processed in a different way. Macros are
substituted into the right-hand sides of function, constant and variable
definitions. All macro substitution happens before constant
substitutions and the actual compilation step. Macros can be defined in
terms of other macros (also recursively), and are evaluated using call
by value (i.e., macro calls in macro arguments are expanded before the
macro gets applied to its parameters).

  Pure macros also have their limitations. Specifically, the left-hand
side of a macro rule must be a simple expression, just like in ordinary
function definitions. This restricts the kinds of expressions which can
be rewritten by a macro. But Pure macros are certainly powerful enough
for most common preprocessing purposes, while still being robust and
easy to use.

* Menu:

* Optimization Rules::
* Recursive Macros::
* User-Defined Special Forms::
* Macro Hygiene::


File: pure.info,  Node: Optimization Rules,  Next: Recursive Macros,  Up: Macros

7.1 Optimization Rules
======================

Here is a simple example, showing a rule which expands saturated calls
of the `succ' function (defined in the prelude) at compile time:

    > def succ x = x+1;
    > foo x::int = succ (succ x);
    > show foo
    foo x::int = x+1+1;

Rules like these can be useful to help the compiler generate better
code. Note that a macro may have the same name as an ordinary Pure
function, which is essential if you want to optimize calls to an
existing function, as in the previous example. (Just like ordinary
functions, the number of parameters in each rule for a given macro must
be the same, but a macro may have a different number of arguments than
the corresponding function.)

  A somewhat more practical example is the following rule from the
prelude, which eliminates saturated instances of the right-associative
function application operator:

    def f $ x = f x;

Like in Haskell, this low-priority operator is handy to write cascading
function calls. With the above macro rule, these will be "inlined" as
ordinary function applications automatically. Example:

    > foo x = bar $ bar $ 2*x;
    > show foo
    foo x = bar (bar (2*x));

Here are two slightly more tricky rules from the prelude, which
optimize the case of "throwaway" list comprehensions. This is useful if
a list comprehension is evaluated solely for its side effects:

    def void (catmap f x) = do f x;
    def void (listmap f x) = do f x;

Note that the `void' function simply throws away its argument and
returns `()' instead. The `do' function applies a function to every
member of a list (like `map'), but throws away all intermediate results
and just returns `()', which is much more efficient if you don't need
those results anyway. These are both defined in the prelude.

  Before we delve into this example, a few remarks are in order about
the way list comprehensions are implemented in Pure. As already
mentioned, list comprehensions are just syntactic sugar; the compiler
immediately transforms them to an equivalent expression involving only
lambdas and a few other list operations. Note that list comprehensions
are essentially equivalent to piles of nested lambdas, filters and
maps, but for various reasons they are actually implemented using two
special helper operations, `catmap' and `listmap'.

  The `catmap' operation combines `map' and `cat'; this is needed, in
particular, to accumulate the results of nested generators, such as
`[i,j | i = 1..n; j = 1..m]'. The same operation is also used to
implement filter clauses, you can see this below in the examples.
However, for efficiency simple generators like `[2*i | i = 1..n]' are
translated to a `listmap' instead (which is basically just `map', but
works with different aggregate types, so that list comprehensions can
draw values from aggregates other than lists, such as matrices).

  Now let's see how the rules above transform a list comprehension if we
"voidify" it:

    > using system;
    > f = [printf "%g\n" (2^x+1) | x=1..5; x mod 2];
    > g = void [printf "%g\n" (2^x+1) | x=1..5; x mod 2];
    > show f g
    f = catmap (\x -> if x mod 2 then [printf "%g\n" (2^x+1)] else []) (1..5);
    g = do (\x -> if x mod 2 then [printf "%g\n" (2^x+1)] else []) (1..5);

Ok, so the `catmap' got replaced with a `do' which is just what we need
to make this code go essentially as fast as a `for' loop in
conventional programming languages (up to constant factors, of course).
Here's how it looks like when we run the `g' function:

    > g;
    3
    9
    33
    ()

It's not all roses, however, since the above macro rules will only get
rid of the outermost `catmap' if the list comprehension binds multiple
variables:

    > u = void [puts $ str (x,y) | x=1..2; y=1..3];
    > show u
    u = do (\x -> listmap (\y -> puts (str (x,y))) (1..3)) (1..2);

If you're bothered by this, you'll have to apply `void' recursively,
creating a nested list comprehension which expands to a nested `do':

    > v = void [void [puts $ str (x,y) | y=1..3] | x=1..2];
    > show v
    v = do (\x -> do (\y -> puts (str (x,y))) (1..3)) (1..2);

(It would be nice to have this handled automatically, but the left-hand
side of a macro definition must be a simple expression, and thus it's
not possible to write a macro which descends recursively into the
lambda argument of `catmap'.)


File: pure.info,  Node: Recursive Macros,  Next: User-Defined Special Forms,  Prev: Optimization Rules,  Up: Macros

7.2 Recursive Macros
====================

Macros can also be recursive, in which case they usually consist of
multiple rules and make use of pattern-matching like ordinary function
definitions. As a simple example, let's implement a Pure version of
Lisp's quasiquote which allows you to create a quoted expression from a
"template" while substituting variable parts of the template. (For the
sake of brevity, our definition is somewhat simplified and does not
cover some corner cases. See the Pure distribution for a full version
of this example.)

    def quasiquote (unquote x)      = x;
    def quasiquote (f@_ (splice x)) = foldl ($) (quasiquote f) x;
    def quasiquote (f@_ x)          = quasiquote f (quasiquote x);
    def quasiquote x                = quote x;

(Note the `f@_', which is an anonymous "as" pattern forcing the
compiler to recognize `f' as a function variable, rather than a literal
function symbol. See *note Head = Function: af. in the *note Caveats
and Notes: c. section for an explanation of this trick.)

  The first rule above takes care of "unquoting" embedded subterms. The
second rule "splices" an argument list into an enclosing function
application. The third rule recurses into subterms of a function
application, and the fourth and last rule takes care of quoting the
"atomic" subterms. Note that `unquote' and `splice' themselves are just
passive constructor symbols, the real work is done by `quasiquote',
using `foldl' at runtime to actually perform the splicing. (Putting off
the splicing until runtime makes it possible to splice argument lists
computed at runtime.)

  If we want, we can also add some syntactic sugar for Lisp weenies.
(Note that we cannot have '`,'' for unquoting, so we use '`,$''
instead.)

    prefix 9 ` ,$ ,@ ;
    def `x = quasiquote x; def ,$x = unquote x; def ,@x = splice x;

Examples:

    > `(2*42+2^12);
    2*42+2^12
    > `(2*42+,$(2^12));
    2*42+4096.0
    > `foo 1 2 (,@'[2/3,3/4]) (5/6);
    foo 1 2 (2/3) (3/4) (5/6)
    > `foo 1 2 (,@'args) (5/6) when args = '[2/3,3/4] end;
    foo 1 2 (2/3) (3/4) (5/6)

We mention in passing here that, technically, Pure macros are just as
powerful as (unconditional) term rewriting systems and thus they are
Turing-complete. This implies that a badly written macro may well send
the Pure compiler into an infinite recursion, which results in a stack
overflow at compile time. See *note Stack Size and Tail Recursion: 3f.
in the *note Caveats and Notes: c.  section for information on how to
deal with these by setting the *note PURE_STACK: 3e. environment
variable.


File: pure.info,  Node: User-Defined Special Forms,  Next: Macro Hygiene,  Prev: Recursive Macros,  Up: Macros

7.3 User-Defined Special Forms
==============================

The `quasiquote' macro in the preceding subsection also provides an
example of how you can use macros to define your own special forms.
This works because the actual evaluation of macro arguments is put off
until runtime, and thus we can safely pass them to built-in special
forms and other constructs which defer their evaluation at _runtime_.
In fact, the right-hand side of a macro rule may be an arbitrary Pure
expression involving conditional expressions, lambdas, binding clauses,
etc. These are never evaluated during macro substitution, they just
become part of the macro expansion (after substituting the macro
parameters).

  Here is another useful example of a user-defined special form, the
macro `timex' which employs the system function `clock' to report the
cpu time in seconds needed to evaluate a given expression, along with
the computed result:

    > using system;
    > def timex x = (clock-t0)/CLOCKS_PER_SEC,y when t0 = clock; y = x end;
    > sum = foldl (+) 0L;
    > timex $ sum (1L..100000L);
    0.43,5000050000L

Note that the above definition of `timex' wouldn't work as an ordinary
function definition, since by virtue of Pure's basic eager evaluation
strategy the `x' parameter would have been evaluated already before it
is passed to `timex', making `timex' always return a zero time value.
Try it!

  Here's yet another example, which is handy if you need to trace
function calls. (Note that the interpreter also has its own built-in
debugging facility, see *note Debugging: 39. However, the following
macro allows you to trace functions using your own custom output
format, and may thus be useful in situations where the built-in
debugger is not appropriate.)

    using system;
    def trace f x y = printf "** exit %s: %s -> %s\n" (str f,str x,str y) $$ y
    when y = printf "** call %s: %s\n: " (str f,str x) $$ gets $$ y end;

This macro is invoked with the function to be traced, the arguments (or
whatever you want to be printed as additional debugging information)
and the actual function call as parameters. (This is a rather
simplistic version, which just prints a prompt on function entry and
the final reduction after the call. You can easily make this as
elaborate as you like. E.g., you might want to keep track of recursive
levels and profiling information, add various interactive commands to
selectively enable and disable tracing during the evaluation, etc.)

  We can still make this a bit more convenient by introducing the
following ordinary function definition:

    trace f x = trace f x (f x);

This lets us patch up a call to trace a given function, as shown below,
without having to change the definition of the function at all. This
trick only works with global functions; for local functions you'll have
to add an explicit call of the `trace' macro to the local definition
yourself. Also note that the definition above only works with functions
taking a single parameter; see the trace.pure example in the
distribution for the full version which can deal with any number of
arguments.

    // Uncomment this line to trace calls to the 'fact' function.
    def fact n = trace fact n;
    // Sample function to be traced.
    fact n = if n>0 then n*fact(n-1) else 1;

Here's a trace of the `fact' function obtained in this fashion (hit
carriage return after each '`:'' prompt to proceed with the
computation):

    > fact 2;
    ** call fact: 2
    :
    ** call fact: 1
    :
    ** call fact: 0
    :
    ** exit fact: 0 -> 1
    ** exit fact: 1 -> 1
    ** exit fact: 2 -> 2
    2

Note that by just removing the macro definition for `fact' above, you
can make the function run untraced as usual again. This scheme is quite
flexible, the only real drawback is that you have to explicitly add
some code for each function you want to trace.


File: pure.info,  Node: Macro Hygiene,  Prev: User-Defined Special Forms,  Up: Macros

7.4 Macro Hygiene
=================

Pure macros are lexically scoped, i.e., the binding of symbols in the
right-hand-side of a macro definition is determined statically by the
text of the definition, and macro parameter substitution also takes
into account binding constructs, such as *note with: 67. and *note
when: 6e. clauses, in the right-hand side of the definition. Macro
facilities with these pleasant properties are also known as *hygienic
macros*. They are not susceptible to so-called "name capture," which
makes macros in less sophisticated languages bug-ridden and hard to use.

  Macro hygiene is a somewhat esoteric topic for most programmers, so
let us take a brief look at what it's all about. The problem avoided by
hygienic macros is that of _name capture_. There are actually two kinds
of name capture which may occur in unhygienic macro systems:

   * A free symbol in the macro _body_ inadvertently becomes bound to
     the value of a local symbol in the context in which the macro is
     called.

   * A free symbol in the macro _call_ inadvertently becomes bound to
     the value of a local symbol in the macro body.

  Pure's hygienic macros avoid both pitfalls. Here is an example for
the first form of name capture:

    > def G x = x+y;
    > G 10 when y = 99 end;
    10+y

Note that the expansion of the `G' macro correctly uses the global
instance of `y', even though `y' is locally defined in the context of
the macro call. (In some languages this form of name capture is
sometimes used deliberately in order to make the macro use the binding
of the symbol which is active at the point of the macro call. This
never works in Pure, hence in such cases you will have to explicitly
pass such symbols to the macro.)

  In contrast, the second form of name capture is usually not intended,
and is therefore more dangerous. Consider the following example:

    > def F x = x+y when y = x+1 end;
    > F y;
    y+(y+1)

Pure again gives the correct result here. You'd have to be worried if
you got `(y+1)+(y+1)' instead, which would result from the literal
expansion `y+y when y = y+1 end', where the (free) variable `y' passed
to `F' gets captured by the local binding of `y'. In fact, that's
exactly what you get with C macros:

    #define F(x) { int y = x+1; return x+y; }

Here `F(y)' expands to `{ int y = y+1; return y+y; }' which is usually
_not_ what you want.


File: pure.info,  Node: Exception Handling,  Next: C Interface,  Prev: Macros,  Up: Top

8 Exception Handling
********************

Pure also offers a useful exception handling facility. To raise an
exception, you just invoke the built-in function `throw' with the value
to be thrown as the argument. Exceptions are caught with the built-in
special form *note catch: b5. which is invoked as follows:

 -- Function: catch handler x
     Catch an exception. The first argument denotes the exception
     handler (a function to be applied to the exception value). The
     second (call-by-name) argument is the expression to be evaluated.

  For instance:

    > catch error (throw hello_world);
    error hello_world

Exceptions are also generated by the runtime system if the program runs
out of stack space, when a guard does not evaluate to a truth value,
and when the subject term fails to match the pattern in a
pattern-matching lambda abstraction, or a *note let: 80, *note case:
6d. or *note when: 6e.  construct. These types of exceptions are
reported using the symbols `stack_fault', `failed_cond' and
`failed_match', respectively, which are declared as constant symbols in
the standard prelude. You can use *note catch: b5. to handle these
kinds of exceptions just like any other. For instance:

    > fact n = if n>0 then n*fact(n-1) else 1;
    > catch error (fact foo);
    error failed_cond
    > catch error (fact 100000);
    error stack_fault

(You'll only get the latter kind of exception if the interpreter does
stack checks, see the discussion of the *note PURE_STACK: 3e.
environment variable in *note Stack Size and Tail Recursion: 3f.)

  Note that unhandled exceptions are reported by the interpreter with a
corresponding error message:

    > fact foo;
    <stdin>, line 2: unhandled exception 'failed_cond' while evaluating 'fact foo'

Exceptions also provide a way to handle asynchronous signals. Pure's
system module provides symbolic constants for common POSIX signals and
also defines the operation `trap' which lets you rebind any signal to a
signal exception. For instance, the following lets you handle the
`SIGQUIT' signal:

    > using system;
    > trap SIG_TRAP SIGQUIT;

You can also use `trap' to just ignore a signal or revert to the
system's default handler (which might take different actions depending
on the type of signal, see signal(7) for details):

    > trap SIG_IGN SIGQUIT; // signal is ignored
    > trap SIG_DFL SIGQUIT; // reinstalls the default signal handler

Note that when the interpreter runs interactively, for convenience most
standard termination signals (`SIGINT', `SIGTERM', etc.) are already set
up to produce corresponding Pure exceptions of the form `signal SIG'
where `SIG' is the signal number. If a script is to be run
non-interactively then you'll have to do this yourself (otherwise most
signals will terminate the program).

  Last but not least, exceptions can also be used to implement
non-local value returns. For instance, here's a variation of our n
queens algorithm which only returns the first solution. Note the use of
`throw' in the recursive search routine to bail out with a solution as
soon as we found one. The value thrown there is caught in the main
routine. Also note the use of `void' in the second equation of
`search'. This effectively turns the list comprehension into a simple
loop which suppresses the normal list result and just returns `()'
instead. Thus, if no value gets thrown then the function regularly
returns with `()' to indicate that there is no solution.

    queens n       = catch reverse (search n 1 []) with
      search n i p = throw p if i>n;
                   = void [search n (i+1) ((i,j):p) | j = 1..n; safe (i,j) p];
      safe (i,j) p = ~any (check (i,j)) p;
      check (i1,j1) (i2,j2)
                   = i1==i2 || j1==j2 || i1+j1==i2+j2 || i1-j1==i2-j2;
    end;

E.g., let's compute a solution for a standard 8x8 board:

    > queens 8;
    [(1,1),(2,5),(3,8),(4,6),(5,3),(6,7),(7,2),(8,4)]



File: pure.info,  Node: C Interface,  Next: Standard Library,  Prev: Exception Handling,  Up: Top

9 C Interface
*************

Pure makes it very easy to call C functions (as well as functions in a
number of other languages supported by the GNU compiler collection). To
call an existing C function, you just need an *note extern: 95.
declaration of the function, as described below. By these means, all
functions in the standard C library and the Pure runtime are readily
available to Pure scripts. Functions can also be loaded from dynamic
libraries and LLVM bitcode files at runtime. In the latter case, you
don't even need to write any *note extern: 95. declarations, the
interpreter will do that for you. As of Pure 0.45, you can also add
inline C/C++ and Fortran code to your Pure scripts and have the Pure
interpreter compile them on the fly, provided that you have the
corresponding compilers from the LLVM project installed.

  In some cases you will still have to rely on big and complicated
third-party and system libraries which aren't readily available in
bitcode form. It goes without saying that writing all the *note extern:
95. declarations for such libraries can be a daunting task.
Fortunately, there is a utility to help with this, by extracting the
*note extern: 95. declarations automatically from C headers. Please see
*note External C Functions: b7. in the *note Caveats and Notes: c.
section for details.

* Menu:

* Extern Declarations::
* C Types::
* Importing Dynamic Libraries::
* Importing LLVM Bitcode::
* Inline Code::


File: pure.info,  Node: Extern Declarations,  Next: C Types,  Up: C Interface

9.1 Extern Declarations
=======================

To access an existing C function in Pure, you need an *note extern: 95.
declaration of the function, which is a simplified kind of C prototype.
The syntax of these declarations is described by the following grammar
rules:

    extern_decl ::= [scope] "extern" prototype ("," prototype) ";"
    prototype   ::= c_type identifier "(" [parameters] ")" ["=" identifier]
    parameters  ::= parameter ("," parameter)*
    parameter   ::= c_type [identifier]
    c_type      ::= identifier "*"*

Extern functions can be called in Pure just like any other. For
instance, the following commands, entered interactively in the
interpreter, let you use the `sin' function from the C library (of
course you could just as well put the *note extern: 95. declaration
into a script):

    > extern double sin(double);
    > sin 0.3;
    0.29552020666134

An *note extern: 95. declaration can also be prefixed with a *note
public: 98./*note private: 99. scope specifier:

    private extern double sin(double);

Multiple prototypes can be given in one *note extern: 95. declaration,
separating them with commas:

    extern double sin(double), double cos(double), double tan(double);

For clarity, the parameter types can also be annotated with parameter
names (these only serve informational purposes and are for the human
reader; they are effectively treated as comments by the compiler):

    extern double sin(double x);

Pointer types are indicated by following the name of the element type
with one or more asterisks, as in C. For instance:

    > extern char* strchr(char *s, int c);
    > strchr "foo bar" (ord "b");
    "bar"

As you can see in the previous example, some pointer types get special
treatment, allowing you to pass certain kinds of Pure data (such as Pure
strings as `char*' in this example). This is discussed in more detail
in *note C Types: b9. below.

  The interpreter makes sure that the parameters in a call match; if
not, then by default the call is treated as a normal form expression:

    > extern double sin(double);
    > sin 0.3;
    0.29552020666134
    > sin 0;
    sin 0

This gives you the opportunity to augment the external function with
your own Pure equations. To make this work, you have to make sure that
the *note extern: 95. declaration of the function comes first. For
instance, we might want to extend the `sin' function with a rule to
handle integers:

    > sin x::int = sin (double x);
    > sin 0;
    0.0

Sometimes it is preferable to replace a C function with a wrapper
function written in Pure. In such a case you can specify an *alias*
under which the original C function is known to the Pure program, so
that you can still call the C function from the wrapper. An alias is
introduced by terminating the extern declaration with a clause of the
form `= alias'. For instance:

    > extern double sin(double) = c_sin;
    > sin x::double = c_sin x;
    > sin x::int = c_sin (double x);
    > sin 0.3; sin 0;
    0.29552020666134
    0.0

As an alternative, you can also declare the C function in a special
namespace (cf. *note Namespaces: 74. in the *note Declarations: 32.
section):

    > namespace c;
    > extern double sin(double);
    > c::sin 0.3;
    0.29552020666134

Note that the namespace qualification only affects the Pure side; the
underlying C function is still called under the unqualified name as
usual. The way in which such qualified externs are accessed is the same
as for ordinary qualified symbols. In particular, the *note using
namespace: a9. declaration applies as usual, and you can declare such
symbols as *note private: 99. if needed. It is also possible to combine
a namespace qualifier with an alias:

    > namespace c;
    > extern double sin(double) = mysin;
    > c::mysin 0.3;
    0.29552020666134

Different aliases of the same external function (or, equivalently,
instances of the function declared in different namespaces) can be
declared in slightly different ways, which makes it possible to adjust
the interpretation of pointer values on the Pure side. This is
particularly useful for string arguments which, as described below, may
be passed both as `char*' (which implies copying and conversion to or
from the system encoding) and as `void*' (which simply passes through
the character pointers). For instance:

    > extern char *strchr(char *s, int c) = foo;
    > extern void *strchr(void *s, int c) = bar;
    > foo "foo bar" 98; bar "foo bar" 98;
    "bar"
    #<pointer 0x12c2f24>

Also note that, as far as Pure is concerned, different aliases of an
external function are really different functions. In particular, they
can each have their own set of augmenting Pure equations. For instance:

    > extern double sin(double);
    > extern double sin(double) = mysin;
    > sin === sin;
    1
    > sin === mysin;
    0
    > sin 1.0; mysin 1.0;
    0.841470984807897
    0.841470984807897
    > sin x::int = sin (double x);
    > sin 1; mysin 1;
    0.841470984807897
    mysin 1



File: pure.info,  Node: C Types,  Next: Importing Dynamic Libraries,  Prev: Extern Declarations,  Up: C Interface

9.2 C Types
===========

As indicated in the previous section, the data types in *note extern:
95.  declarations are either C type names or pointer types derived from
these. The special `expr*' pointer type is simply passed through; this
provides a means to deal with Pure data in C functions in a direct
fashion. For all other C types, Pure values are "marshalled"
(converted) from Pure to C when passed as arguments to C functions, and
the result returned by the C function is then converted back from C to
Pure. All of this is handled by the runtime system in a transparent
way, of course.

  Note that, to keep things simple, Pure does _not_ provide any
notations for C structs or function types, although it is possible to
represent pointers to such objects using `void*' or some other
appropriate pointer types. In practice, this simplified system should
cover most kinds of calls that need to be done when interfacing to C
libraries, but there are ways to work around these limitations if you
need to access C structs or call back from C to Pure, see *note
External C Functions: b7. in the *note Caveats and Notes: c. section
for details.

* Menu:

* Basic C Types::
* Pointer Types::
* Pointers and Matrices::
* Pointer Examples::


File: pure.info,  Node: Basic C Types,  Next: Pointer Types,  Up: C Types

9.2.1 Basic C Types
-------------------

Pure supports the usual range of basic C types: `void', `bool', `char',
`short', `int', `long', `float', `double', and converts between these
and the corresponding Pure data types (machine ints, bigints and double
values) in a straightforward way.

  The `void' type is only allowed in function results. It is converted
to the empty tuple `()'.

  Both `float' and `double' are supported as floating point types.
Single precision `float' arguments and return values are converted
from/to Pure's double precision floating point numbers.

  A variety of C integer types (`bool', `char', `short', `int', `long')
are provided which are converted from/to the available Pure integer
types in a straightforward way. In addition, the synonyms `int8',
`int16' and `int32' are provided for `char', `short' and `int',
respectively, and `int64' denotes 64 bit integers (a.k.a. ISO C99 `long
long'). Note that `long' is equivalent to `int32' on 32 bit systems,
whereas it is the same as `int64' on most 64 bit systems. To make it
easier to interface to various system routines, there's also a special
`size_t' integer type which usually is 4 bytes on 32 bit and 8 bytes on
64 bit systems.

  All integer parameters take both Pure ints and bigints as actual
arguments; truncation or sign extension is performed as needed, so that
the C interface behaves as if the argument was "cast" to the C target
type. Returned integers use the smallest Pure type capable of holding
the result, i.e., int for the C `char', `short' and `int' types, bigint
for `int64'.

  Pure considers all integers as signed quantities, but it is possible
to pass unsigned integers as well (if necessary, you can use a bigint
to pass positive values which are too big to fit into a machine int).
Also note that when an unsigned integer is returned by a C routine,
which is too big to fit into the corresponding signed integer type, it
will "wrap around" and become negative. In this case, depending on the
target type, you can use the `ubyte', `ushort', `uint', `ulong' and
`uint64' functions provided by the prelude to convert the result back
to an unsigned quantity.


File: pure.info,  Node: Pointer Types,  Next: Pointers and Matrices,  Prev: Basic C Types,  Up: C Types

9.2.2 Pointer Types
-------------------

The use of pointer types is also fairly straightforward, but Pure has
some special rules for the conversion of certain pointer types which
make it easy to pass aggregate Pure data to and from C routines, while
also following the most common idioms for pointer usage in C. The
following types of pointers are recognized both as arguments and return
values of C functions.

  Bidirectional pointer conversions:

   * `char*' is used for string arguments and return values which are
     converted from Pure's internal utf-8 based string representation
     to the system encoding and vice versa. (Thus a C routine can never
     modify the raw Pure string data in-place; if this is required then
     you'll have to pass the string argument as a `void*', see below.)

   * `void*' is for any generic pointer value, which is simply passed
     through unchanged. When used as an argument, you can also pass
     Pure strings, matrices and bigints. In this case the raw
     underlying data pointer (`char*' in the case of strings, `int*',
     `double*' or `expr*' in the case of numeric and symbolic matrices,
     and the GMP type `mpz_t' in the case of bigints) is passed, which
     allows the data to be modified in place (with care). In
     particular, passing bigints as `void*' makes it possible to call
     most GMP integer routines directly from Pure.

   * `dmatrix*', `cmatrix*' and `imatrix*' allow you to pass numeric
     Pure matrices of the appropriate types (double, complex, int).
     Here a pointer to the underlying GSL matrix structure is passed
     (not just the data itself).  This makes it possible to transfer
     GSL matrices between Pure and GSL routines in a direct fashion
     without any overhead. (For convenience, there are also some other
     pointer conversions for marshalling matrix arguments to numeric C
     vectors, which are described in *note Pointers and Matrices: bd.
     below.)

   * `expr*' is for any kind of Pure value. A pointer to the expression
     node is passed to or from the C function. This type is to be used
     for C routines which are prepared to deal with pristine Pure data,
     using the corresponding functions provided by the runtime. You can
     find many examples of this in the standard library.

  All other pointer types are simply taken at face value, allowing you
to pass Pure pointer values as is, without any conversions. This also
includes pointers to arbitrary named types which don't have a
predefined meaning in Pure, such as `FILE*'. As of Pure 0.45, the
interpreter keeps track of the actual names of all pointer types and
checks (at runtime) that the types match in an external call, so that
you can't accidentally get a core dump by passing, say, a `FILE*' for a
`char*'. (The call will then simply fail and yield a normal form, which
gives you the opportunity to hook into the function with your own Pure
definitions which may supply any desired data conversions.)  Typing
information about pointer values is also available to Pure scripts by
means of corresponding library functions, please see the _Tagged
Pointers_ section in the `purelib' for details.


File: pure.info,  Node: Pointers and Matrices,  Next: Pointer Examples,  Prev: Pointer Types,  Up: C Types

9.2.3 Pointers and Matrices
---------------------------

The following additional pointer conversions are provided to deal with
Pure matrix values in arguments of C functions, i.e., on the input
side. These enable you to pass Pure matrices for certain kinds of C
vectors. Note that in any case, you can also simply pass a suitable
plain pointer value instead.  Also, these types aren't special in
return values, where they will simply yield a pointer value (with the
exception of `char*' which gets special treatment as explained in the
previous subsection). Thus you will have to decode such results
manually if needed. The standard library provides various routines to
do this, please see the _String Functions_ and _Matrix Functions_
sections in the `purelib' for details.

  Numeric pointer conversions (input only):

   * `char*', `short*', `int*', `int64*', `float*', `double*' can be
     used to pass numeric matrices as C vectors. This kind of
     conversion passes just the matrix data (not the GSL matrix
     structure, as the `dmatrix*' et al conversions do) and does
     conversions between integer or floating point data of different
     sizes on the fly. You can either pass an int matrix as a `char*',
     `short*' `int*' or `int64*' argument, or a double or complex
     matrix as a `float*' or `double*' argument (complex values are
     then represented as two separate double numbers, first the real,
     then the imaginary part, for each matrix element).

   * `char**', `short**', `int**', `int64**', `float**', `double**'
     provide yet another way to pass numeric matrix arguments. This
     works analogously to the numeric vector conversions above, but
     here a temporary C vector of pointers is passed to the C function,
     whose elements point to the rows of the matrix.

  Argv-style conversions (input only):

   * `char**' and `void**' can be used to pass `argv'-style vectors as
     arguments to C functions. In this case, the Pure argument must be
     a symbolic vector of strings or generic pointer values. `char**'
     converts the string elements to the system encoding, whereas
     `void**' passes through character string data and other pointers
     unchanged (and allows in-place modification of the data). A
     temporary C vector of these elements is passed to the C function,
     which is always `NULL'-terminated and can thus be used for almost
     any purpose which requires such `argv'-style vectors.

  Note that in the numeric pointer conversions, the matrix data is
passed "per reference" to C routines, i.e., the C function may modify
the data "in place". This is true even for target data types such as
`short*' or `float**' which involve automatic conversions and hence
need temporary storage. In this case the data from the temporary
storage is written back to the original matrix when the function
returns, to maintain the illusion of in-place modification. Temporary
storage is also needed when the GSL matrix has the data in
non-contiguous storage. You may want to avoid this if performance is
critical, by always using "packed" matrices (see `pack' in _Matrix
Functions_) of the appropriate types.


File: pure.info,  Node: Pointer Examples,  Prev: Pointers and Matrices,  Up: C Types

9.2.4 Pointer Examples
----------------------

Let's finally have a look at some instructive examples to explain some
of the trickier pointer types.

  First, the matrix pointer types `dmatrix*', `cmatrix*' and `imatrix*'
can be used to pass double, complex double and int matrices to GSL
functions taking pointers to the corresponding GSL types (`gsl_matrix',
`gsl_matrix_complex' and `gsl_matrix_int') as arguments or returning
them as results. (Note that there is no special marshalling of Pure's
symbolic matrix type, as these aren't supported by GSL anyway.) Also
note that matrices are always passed by reference. Thus, if you need to
pass a matrix as an output parameter of a GSL matrix routine, you
should either create a zero matrix or a copy of an existing matrix to
hold the result. The prelude provides various operations for that
purpose (in particular, see the `dmatrix', `cmatrix', `imatrix' and
`pack' functions in matrices.pure). For instance, here is how you can
quickly wrap up GSL's double matrix addition function in a way that
preserves value semantics:

    > using "lib:gsl";
    > extern int gsl_matrix_add(dmatrix*, dmatrix*);
    > x::matrix + y::matrix = gsl_matrix_add x y $$ x when x = pack x end;
    > let x = dmatrix {1,2,3}; let y = dmatrix {2,3,2}; x; y; x+y;
    {1.0,2.0,3.0}
    {2.0,3.0,2.0}
    {3.0,5.0,5.0}

Most GSL matrix routines can be wrapped in this fashion quite easily. A
ready-made GSL interface providing access to all of GSL's numeric
functions is in the works; please check the Pure website for details.

  For convenience, it is also possible to pass any kind of numeric
matrix for a `char*', `short*', `int*', `int64*', `float*' or `double*'
parameter. This requires that the pointer and the matrix type match up;
conversions between `char', `short', `int64' and `int' data and,
likewise, between `float' and `double' are handled automatically,
however. For instance, here is how you can call the `puts' routine from
the C library with an int matrix encoding the string `"Hello, world!"'
as byte values (ASCII codes):

    > extern int puts(char*);
    > puts {72,101,108,108,111,44,32,119,111,114,108,100,33,0};
    Hello, world!
    14

Pure 0.45 and later also support `char**', `short**', `int**',
`int64**', `float**' and `double**' parameters which encode a matrix as
a vector of row pointers instead. This kind of matrix representation is
often found in audio and video processing software (where the rows of
the matrix might denote different audio channels, display lines or
video frames), but it's also fairly convenient to do any kind of matrix
processing in C. For instance, here's how to do matrix multiplication
(the naive algorithm):

    void matmult(int n, int l, int m, double **x, double **y, double **z)
    {
      int i, j, k;
      for (i = 0; i < n; i++)
        for (j = 0; j < m; j++) {
          z[i][j] = 0.0;
          for (k = 0; k < l; k++)
            z[i][j] += x[i][k]*y[k][j];
        }
    }

As you can see, this multiplies a `n' times `l' matrix `x' with a `l'
times `m' matrix `y' and puts the result into the `n' times `m' matrix
`z':

    > extern void matmult(int, int, int, double**, double**, double**);
    > let x = {0.11,0.12,0.13;0.21,0.22,0.23};
    > let y = {1011.0,1012.0;1021.0,1022.0;1031.0,1032.0};
    > let z = dmatrix (2,2);
    > matmult 2 3 2 x y z $$ z;
    {367.76,368.12;674.06,674.72}

Also new in Pure 0.45 is the support for passing `argv'-style vectors as
arguments. For instance, here is how you can use `fork' and `execvp' to
implement a poor man's version of the C `system' function. (This is
UNIX-specific and doesn't do much error-checking, but you get the idea.)

    extern int fork();
    extern int execvp(char *path, char **argv);
    extern int waitpid(int pid, int *status, int options);

    system cmd::string = case fork of
      // child: execute the program, bail out if error
      0 = execvp "/bin/sh" {"/bin/sh","-c",cmd} $$ exit 1;
      // parent: wait for the child and return its exit code
      pid = waitpid pid status 0 $$ status!0 >> 8
            when status = {0} end if pid>=0;
    end;

    system "echo Hello, world!";
    system "ls -l *.pure";
    system "exit 1";



File: pure.info,  Node: Importing Dynamic Libraries,  Next: Importing LLVM Bitcode,  Prev: C Types,  Up: C Interface

9.3 Importing Dynamic Libraries
===============================

By default, external C functions are resolved by the LLVM runtime,
which first looks for the symbol in the C library and Pure's runtime
library (or the interpreter executable, if the interpreter was linked
statically). Thus all C library and Pure runtime functions are readily
available in Pure programs.  Other functions can be provided by adding
them to the runtime, or by linking them into the runtime or the
interpreter executable. Better yet, you can just "dlopen" shared
libraries at runtime with a special form of the *note using: 31. clause:

    using "lib:libname[.ext]";

For instance, if you want to call the functions from library libxyz
directly from Pure:

    using "lib:libxyz";

After this declaration the functions from the given library will be
ready to be imported into your Pure program by means of corresponding
*note extern: 95.  declarations.

  Shared libraries opened with using clauses are searched for in the
same way as source scripts (see section *note Modules and Imports: a1.
above), using the *note -L: 1e. option and the *note PURE_LIBRARY: 51.
environment variable in place of *note -I: 1d. and *note PURE_INCLUDE:
50. If the library isn't found by these means, the interpreter will
also consider other platform-specific locations searched by the dynamic
linker, such as the system library directories and `LD_LIBRARY_PATH' on
Linux. The necessary filename suffix (e.g., .so on Linux or .dll on
Windows) will be supplied automatically when needed. Of course you can
also specify a full pathname for the library if you prefer that. If a
library file cannot be found, or if an *note extern: 95.  declaration
names a function symbol which cannot be resolved, an appropriate error
message is printed.


File: pure.info,  Node: Importing LLVM Bitcode,  Next: Inline Code,  Prev: Importing Dynamic Libraries,  Up: C Interface

9.4 Importing LLVM Bitcode
==========================

As of Pure 0.44, the interpreter also provides a direct way to import
LLVM bitcode modules in Pure scripts. The main advantage of this method
over the "plain" C interface explained above is that the bitcode loader
knows all the call interfaces and generates the necessary *note extern:
95. declarations automatically. This is more than just a convenience,
as it also eliminates at least some of the mistakes in *note extern:
95. declarations that may arise when importing functions manually from
dynamic libraries.

  LLVM bitcode is loaded in a Pure script using the following special
format of the *note using: 31. clause:

    using "bc:modname[.bc]";

(Here the `bc' tag indicates a bitcode file, and the default `.bc'
bitcode filename extension is supplied automatically. Also, the bitcode
file is searched for on the usual library search path.)

  That's it, no explicit *note extern: 95. declarations are required on
the Pure side. The Pure interpreter automatically creates *note extern:
95.  declarations (in the current namespace) for all the external
functions defined in the LLVM bitcode module, and generates the
corresponding wrappers to make the functions callable from Pure. (This
also works when batch-compiling a Pure script. In this case, the
bitcode file actually gets linked into the output code, so the loaded
bitcode module only needs to be present at compile time.)

  By default the imported symbols will be public. You can also specify
the desired scope of the symbols explicitly, by placing the *note
public: 98. or *note private: 99. keyword before the module name. For
instance:

    using private "bc:modname";

You can also import the same bitcode module several times, possibly in
different namespaces. This will not actually reload the module, but it
will create aliases for the external functions in different namespaces:

    namespace foo;
    using "bc:modname";
    namespace bar;
    using private "bc:modname";

You can load any number of bitcode modules along with shared libraries
in a Pure script, in any order. The JIT will try to satisfy external
references in modules and libraries from other loaded libraries and
bitcode modules. This is deferred until the code is actually
JIT-compiled, so that you can make sure beforehand that all required
libraries and bitcode modules have been loaded.  If the JIT fails to
resolve a function, the interpreter will print its name and also raise
an exception at runtime when the function is being called from other C
code. (You can then run your script in the debugger to locate the
external visible in Pure from which the unresolved function is called.)

  Let's take a look at a concrete example to see how this actually
works. Consider the following C code which defines a little function to
compute the greatest common divisor of two (machine) integers:

    int mygcd(int x, int y)
    {
      if (y == 0)
        return x;
      else
        return mygcd(y, x%y);
    }

Let's say that this code is in the file `mygcd.c', then you'd compile
it to a bitcode module using llvm-gcc as follows:

    llvm-gcc -emit-llvm -c mygcd.c -o mygcd.bc

Or, if you prefer to use *note clang: c4, the new LLVM-based C/C++
compiler:

    clang -emit-llvm -c mygcd.c -o mygcd.bc

Note that the `-emit-llvm -c' options instruct llvm-gcc or clang to
build an LLVM bitcode module. Of course, you can also add optimizations
and other options to the compile command as desired.

  You can now load the resulting bitcode module and run the `mygcd'
function in the Pure interpreter simply as follows:

    > using "bc:mygcd";
    > mygcd 75 105;
    15

To actually see the generated *note extern: 95. declaration of the
imported function, you can use the interactive `show' command:

    > show mygcd
    extern int mygcd(int, int);

Some more examples showing how to use the bitcode interface can be
found in the Pure sources. In particular, the interface also works with
Fortran (using llvm-gfortran), and there is special support for
interfacing to Grame's functional DSP programming language *note Faust:
c5. (the latter uses a special variant of the bitcode loader, which is
selected with the `dsp' tag in the *note using: 31. clause). Please
refer to the corresponding examples in the distribution for further
details.

  Please note that at this time the LLVM bitcode interface is still
somewhat experimental, and there are some known limitations:

   * LLVM doesn't distinguish between `char*' and `void*' in bitcode,
     so all `void*' parameters and return values in C code will be
     promoted to `char*' on the Pure side. Also, pointers to types
     which neither have a predefined meaning in Pure nor a proper type
     name in the bitcode file, will become a generic pointer type
     (`void*', `void**', etc.) in Pure. If this is a problem then you
     can just redeclare the corresponding functions under an alias
     _after_ loading the bitcode module, giving the proper argument and
     result types (see *note Extern Declarations: b8. above).

   * The bitcode interface is limited to the same range of C types as
     Pure's plain C interface. In practice, this should cover most C
     code, but it's certainly possible that you run into unsupported
     types for arguments and return values. The compiler will then
     print a warning; the affected functions will still be linked in,
     but they will not be callable from Pure.  Also note that calling
     conventions for passing C structs _by value_ depend on the host
     ABI, so you should have a look at the resulting *note extern: 95.
     declaration (using `show') to determine how the function is
     actually to be called from Pure.


File: pure.info,  Node: Inline Code,  Prev: Importing LLVM Bitcode,  Up: C Interface

9.5 Inline Code
===============

Instead of manually compiling source files to bitcode modules, you can
also just place the source code into a Pure script, enclosing it in `%<
... %>'.  (Optionally, the opening brace may also be preceded with a
*note public: 98.  or *note private: 99. scope specifier, which is used
in the same way as the scope specifier following the *note using: 31.
keyword when importing bitcode files.)

  For instance, here is a little script showing inline code for the
`mygcd' function from the previous subsection:

    %<
    int mygcd(int x, int y)
    {
      if (y == 0)
        return x;
      else
        return mygcd(y, x%y);
    }
    %>

    mygcd 75 105;

The interpreter automatically compiles the inlined code to LLVM bitcode
which is then loaded as usual. (Of course, this will only work if you
have the corresponding LLVM compilers installed.) This method has the
advantage that you don't have to write a Makefile and you can create
self-contained Pure scripts which include all required external
functions. The downside is that the inline code sections will have to
be recompiled every time you run the script with the interpreter which
may considerably increase startup times. If this is a problem then it's
usually better to import a separate bitcode module instead (see *note
Importing LLVM Bitcode: c3.), or at least batch-compile your script to
an executable (see *note Batch Compilation: b.).

  Currently, C, C++, Fortran and Faust are supported as foreign source
languages, with llvm-gcc, llvm-g++, llvm-gfortran and faust2 as the
corresponding compilers. Alternatively, the LLVM clang and clang++
compilers can be used for C/C++ compilation (this will actually be
default if the Pure interpreter itself was compiled with clang).
Examples for all of these can be found in the Pure sources.

  C is the default language. The desired source language can be
selected by placing an appropriate tag into the inline code section,
immediately after the opening brace. (The tag is removed before the
code is submitted to compilation.) For instance:

    %< -*- Fortran90 -*-
    function fact(n) result(p)
      integer n, p
      p = 1
      do i = 1, n
         p = p*i
      end do
    end function fact
    %>

    fact n::int = fact_ {n};
    map fact (1..10);

As indicated, the language tag takes the form `-*- lang -*-' where
`lang' can currently be any of `c', `c++', `fortran' and `dsp' (the
latter indicates the Faust language). Case is insignificant here, so
you can also write `C', `C++', `Fortran', `DSP' etc. For the `fortran'
tag, you may also have to specify the appropriate language standard,
such as `fortran90' which is used in the example above. The language
tag can also be followed by a module name, using the format `-*-
lang:name -*-'. This is optional for all languages except Faust (where
the module name specifies the namespace for the interface routines of
the Faust module). So, e.g., a Faust DSP named `test' would be
specified with a `dsp:test' tag. Case is _significant_ in the module
name.

  The Pure interpreter has some built-in knowledge on how to invoke the
LLVM compilers to produce a working bitcode file ready to be loaded by
the interpreter, so the examples above should work out of the box if
you have the required compilers installed on your `PATH'. However,
there are also some environment variables you can set for customization
purposes.  Specifically, `PURE_CC' is the command to invoke the C
compiler. This variable lets you specify the exact name of the
executable along with any debugging and optimization options that you
may want to add. Likewise, `PURE_CXX', `PURE_FC' and `PURE_FAUST' are
used for the C++, Fortran and Faust compilers, respectively.

  For instance, if you prefer to use clang as your C compiler, and
you'd like to invoke it with the `-O3' optimization option, you would
set `PURE_CC' to `"clang -O3"'. (To verify the settings you made, you
can have the interpreter echo the compilation commands which are
actually executed, by running Pure with the `-v0100' option, see *note
Verbosity and Debugging Options: 36.)


File: pure.info,  Node: Standard Library,  Next: Interactive Usage,  Prev: C Interface,  Up: Top

10 Standard Library
*******************

Pure comes with a collection of Pure library modules, which includes the
standard prelude (loaded automatically at startup time) and some other
modules which can be loaded explicitly with a *note using: 31. clause.
The prelude offers the necessary functions to work with the built-in
types (including arithmetic and logical operations) and to do most kind
of list processing you can find in ML- and Haskell-like languages. It
also provides a collection of basic string and matrix operations.
Please refer to the `purelib' for details on the provided operations.
Here is a very brief summary of some of the prelude operations which,
besides the usual arithmetic and logical operators, are probably used
most frequently:

 -- Description: x+y
     The arithmetic `+' operation is also used to denote list and string
     concatenation in Pure.

 -- Description: x:y
     This is the list-consing operation. `x' becomes the head of the
     list, `y' its tail. As '`:'' is a constructor symbol, you can use
     it in patterns on the left hand side of rewriting rules.

 -- Description: x..y
     Constructs arithmetic sequences. `x:y..z' can be used to denote
     sequences with arbitrary stepsize `y-x'. Infinite sequences can be
     constructed using an infinite bound (i.e., `inf' or `-inf'). E.g.,
     `1:3..inf' denotes the stream of all odd integers starting at 1.

 -- Description: x,y
     This is the pair constructor, used to create tuples of arbitrary
     sizes.  Tuples provide an alternative way to represent aggregate
     values in Pure.  In contrast to lists, tuples are always "flat",
     so that `(x,y),z' and `x,(y,z)' denote the same triple `x,y,z'.
     (This is explained in more detail in the *note Primary
     Expressions: 72. section.)

 -- Description: #x
     The size (number of elements) of the list, tuple, matrix or string
     `x'. In addition, `dim x' yields the dimensions (number of rows and
     columns) of a matrix.

 -- Description: x!y
     This is Pure's indexing operation, which applies to lists, tuples,
     matrices and strings. Note that all indices in Pure are
     zero-based, thus `x!0' and `x!(#x-1)' are the first and last
     element of `x'. In the case of matrices, the subscript may also be
     a pair of row and column indices, such as `x!(1,2)'.

 -- Description: x!!ys
     This is the "slicing" operation, which returns the list, tuple,
     matrix or string of all `x!y' while `y' runs through the (list or
     matrix) `ys'. Thus, e.g., `x!!(i..j)' returns all the elements
     between `i' and `j' (inclusive). Indices which fall outside the
     valid index range are quietly discarded. The index range `ys' may
     contain any number of indices (also duplicates), in any order.
     Thus `x!![0|i=1..n]' returns the first element of `x' `n' times,
     and, if `ys' is a permutation of the range `0..#x-1', then `x!!ys'
     yields the corresponding permutation of the elements of `x'. In
     the case of matrices the index range may also contain
     two-dimensional subscripts, or the index range itself may be
     specified as a pair of row/column index lists such as
     `x!!(i..j,k..l)'.

  The prelude also offers support operations for the implementation of
list and matrix comprehensions, as well as the customary list
operations like `head', `tail', `drop', `take', `filter', `map',
`foldl', `foldr', `scanl', `scanr', `zip', `unzip', etc., which make
list programming so much fun in modern FPLs. In Pure, these also work
on strings as well as matrices, although, for reasons of efficiency,
these data structures are internally represented as arrays.

  Besides the prelude, Pure's standard library also comprises a growing
number of additional library modules which we can only mention in
passing here. In particular, the `math' module provides additional
mathematical functions as well as Pure's complex and rational number
data types. Common container data structures like sets and dictionaries
are implemented in the `set' and `dict' modules, among others.
Moreover, the system interface can be found in the `system' module. In
particular, this module also provides operations to do basic C-style
I/O, including `printf' and `scanf'.


File: pure.info,  Node: Interactive Usage,  Next: Batch Compilation,  Prev: Standard Library,  Up: Top

11 Interactive Usage
********************

In interactive mode, the interpreter reads definitions and expressions
and processes them as usual. You can use the *note -i: 1d. option to
force interactive mode when invoking the interpreter with some script
files.  Additional scripts can be loaded interactively using either a
*note using: 31.  declaration or the interactive `run' command (see the
description of the `run' command below for the differences between
these). Or you can just start typing away, entering your own
definitions and expressions to be evaluated.

  The input language is just the same as for source scripts, and hence
individual definitions and expressions must be terminated with a
semicolon before they are processed. For instance, here is a simple
interaction which defines the factorial and then uses that definition
in some evaluations. Input lines begin with '> ', which is the
interpreter's default command prompt:

    > fact 1 = 1;
    > fact n = n*fact (n-1) if n>1;
    > let x = fact 10; x;
    3628800
    > map fact (1..10);
    [1,2,6,24,120,720,5040,40320,362880,3628800]

As indicated, in interactive mode the normal forms of toplevel
expressions are printed after each expression is entered. We also call
this the *read-eval-print loop*. Normal form expressions are usually
printed in the same form as you'd enter them. However, there are a few
special kinds of objects like anonymous closures, thunks ("lazy" values
to be evaluated when needed) and pointers which don't have a textual
representation in the Pure syntax and will be printed in the format
`#<'object description`>' by default. It is also possible to override
the print representation of any kind of expression by means of the
*note __show__: 8e. function, see *note The __show__ Function: 8f. for
details.

* Menu:

* Online Help::
* Interactive Commands::
* Last Result::
* Specifying Symbol Selections::
* The show Command::
* Definition Levels::
* Debugging::
* Interactive Startup::


File: pure.info,  Node: Online Help,  Next: Interactive Commands,  Up: Interactive Usage

11.1 Online Help
================

Online help is available in the interpreter with the interactive `help'
command, see *note Interactive Commands: cc. below. You need to have a
html browser installed for that. By default, the `help' command uses
*w3m*, but you can change this by setting either the *note PURE_HELP:
4e. or the *note BROWSER: 4c. environment variable accordingly.

  As of Pure 0.46, the interpreter provides a much improved `help'
command which gives you access to all the available documentation in
html format, which includes this manual, the `purelib', as well as all
manuals of the addon modules available from the Pure website.

  When invoked without arguments, the `help' command displays an
overview of the available documentation, from which you can follow the
links to the provided manuals:

    > help

(If the interpreter gives you an error message when you do this then you
haven't installed the documentation yet. The complete set of manuals is
provided as a separate package at the Pure website, please see the Pure
installation instructions for details.)

  The `help' command also accepts a parameter which lets you specify a
search term which is looked up in the global index, e.g.:

    > help foldl

Besides Pure functions, macros, variables and constants described in the
manual you can also look up program options and environment variables,
e.g.:

    > help -x
    > help pure-gen -x
    > help PURE_STACK

(Note that you can specify the program name to disambiguate between
options for different utilities, such as the `-x' option which is
accepted both by the Pure interpreter and the `pure-gen' program.)

  If the search term doesn't appear in the index, it is assumed to be a
topic (a link target, usually a section title) in the Pure manual. Note
that the docutils (http://docutils.sourceforge.net/) tools used to
generate the html source of the Pure documentation mangle the section
titles so that they are in lowercase and blanks are replaced with
hyphens. So to look up the present section in this manual you'd have to
type:

    > help online-help

The help files are in html format and located in the docs subdirectory
of the Pure library directory (i.e., /usr/local/lib/pure/docs by
default). You can look up topics in any of the help files with a
command like the following:

    > help pure-gsl#matrices

Here `pure-gsl' is the basename of the help file (library path and .html
suffix are supplied automatically), and `matrices' is a link target in
that document. To just read the pure-gsl.html file without specifying a
target, type the following:

    > help pure-gsl#

(Note that just `help pure-gsl' won't work, since it would look for a
search term in the index or a topic in the Pure manual.)

  Last but not least, you can also point the help browser to any html
document (either a local file or some website) denoted by a proper URL,
provided that your browser program can handle these. For instance:

    > help file:mydoc.html#foo
    > help http://pure-lang.googlecode.com



File: pure.info,  Node: Interactive Commands,  Next: Last Result,  Prev: Online Help,  Up: Interactive Usage

11.2 Interactive Commands
=========================

When running interactively, the interpreter accepts a number of special
commands useful for interactive purposes. Here is a quick rundown of the
currently supported operations:

 -- Description: ! command
     Shell escape.

 -- Description: break [symbol ...]
     Sets breakpoints on the given function or operator symbols. All
     symbols must be specified in fully qualified form, see the remarks
     below. If invoked without arguments, prints all currently defined
     breakpoints. This requires that the interpreter was invoked with
     the *note -g: 13. option to enable debugging support. See *note
     Debugging: 39. below for details.

 -- Description: bt
     Prints a full backtrace of the call sequence of the most recent
     evaluation, if that evaluation ended with an unhandled exception.
     This requires that the interpreter was invoked with the *note -g:
     13. option to enable debugging support. See *note Debugging: 39.
     below for details.

 -- Description: cd dir
     Change the current working dir.

 -- Description: clear [option ...] [symbol ...]
     Purge the definitions of the given symbols (functions, macros,
     constants or global variables). All symbols must be specified in
     fully qualified form, see the remarks below. If invoked as `clear
     ans', clears the `ans' value (see *note Last Result: d5. below).
     When invoked without any arguments, `clear' purges all definitions
     at the current interactive "level" (after confirmation) and
     returns you to the previous level, if any. (It might be a good
     idea to first check your current definitions with `show' or back
     them up with `dump' before you do that.)  The desired level can be
     specified with the `-t' option. See the description of the `save'
     command and *note Definition Levels: d6. below for further
     details. A description of the common options accepted by the
     `clear', `dump' and `show' commands can be found in *note
     Specifying Symbol Selections: d7. below.

 -- Description: del [-b|-t] [symbol ...]
     Deletes breakpoints and tracepoints on the given function or
     operator symbols. If the `-b' option is specified then only
     breakpoints are deleted; similarly, `del -t' only deletes
     tracepoints. Otherwise both breakpoints and tracepoints are
     deleted. All symbols must be specified in fully qualified form,
     see the remarks below. If invoked without non-option arguments,
     `del' clears _all_ currently defined breakpoints and/or
     tracepoints (after confirmation). See *note Debugging: 39. below
     for details.

 -- Description: dump [-n filename] [option ...] [symbol ...]
     Dump a snapshot of the current function, macro, constant and
     variable definitions in Pure syntax to a text file. All symbols
     must be specified in fully qualified form, see the remarks below.
     This works similar to the `show' command (see below), but writes
     the definitions to a file. The default output file is .pure in the
     current directory, which is then reloaded automatically the next
     time the interpreter starts up in interactive mode in the same
     directory. This provides a quick-and-dirty way to save an
     interactive session and have it restored later, but note that this
     isn't perfect. In particular, declarations of *note extern: 95.
     symbols won't be saved unless they're specified explicitly, and
     some objects like closures, thunks and pointers don't have a
     textual representation from which they could be reconstructed. To
     handle these, you'll probably have to prepare a corresponding
     .purerc file yourself, see *note Interactive Startup: da. below.

     A different filename can be specified with the `-n' option, which
     expects the name of the script to be written in the next argument,
     e.g: `dump -n myscript.pure'. You can then edit that file and use
     it as a starting point for an ordinary script or a .purerc file,
     or you can just run the file with the `run' command (see below) to
     restore the definitions in a subsequent interpreter session.

 -- Description: help [topic]
     Display online documentation. If a topic is given, it is looked up
     in the index. Alternatively, you can also specify a link target in
     any of the installed help files, or any other html document
     denoted by a proper URL.  Please see *note Online Help: 4. above
     for details.

 -- Description: ls [args]
     List files (shell *ls* command).

 -- Description: mem
     Print current memory usage. This reports the number of expression
     cells currently in use by the program, along with the size of the
     freelist (the number of allocated but currently unused expression
     cells). Note that the actual size of the expression storage may be
     somewhat larger than this, since the runtime always allocates
     expression memory in bigger chunks.  Also, this figure does not
     reflect other heap-allocated memory in use by the program, such as
     strings or malloc'ed pointers.

 -- Description: override
     Enter "override" mode. This allows you to add equations "above"
     existing definitions in the source script, possibly overriding
     existing equations. See *note Definition Levels: d6. below for
     details.

 -- Description: pwd
     Print the current working dir (shell *pwd* command).

 -- Description: quit
     Exits the interpreter.

 -- Description: run [-g|script]
     When invoked without arguments or with the `-g' option, `run' does
     a "cold" restart of the interpreter, with the scripts and options
     given on the interpreter's original command line. If just `-g' is
     specified as the argument, the interpreter is run with debugging
     enabled.  Otherwise the interpreter is invoked without debugging
     support. (This overrides the corresponding option from the
     interpreter's command line.)  This command provides a quick way to
     rerun the interpreter after changes in some of the loaded script
     files, or if you want to enable or disable debugging on the fly
     (which requires a restart of the interpreter). You'll also loose
     any definitions that you entered interactively in the interpreter,
     so you may want to back them up with `dump' beforehand.

     When invoked with a script name as argument, `run' loads the given
     script file and adds its definitions to the current environment.
     This works more or less like a *note using: 31. clause, but only
     searches for the script in the current directory and places the
     definitions in the script at the current temporary level, so that
     `clear' can be used to remove them again. Also note that namespace
     and pragma settings of scripts loaded with `run' stick around
     after loading the script. This allows you to quickly set up your
     environment by just running a script containing the necessary
     namespace declarations and compiler directives. (Alternatively,
     you can also use the interpreter's startup files for that purpose,
     see *note Interactive Startup: da. below.)

 -- Description: save
     Begin a new level of temporary definitions. A subsequent `clear'
     command (see above) will purge the definitions made since the most
     recent `save' command. See *note Definition Levels: d6. below for
     details.

 -- Description: show [option ...] [symbol ...]
     Show the definitions of symbols in various formats. See *note The
     show Command: e4.  below for details. All symbols must be
     specified in fully qualified form, see the remarks below. A
     description of the common options accepted by the `clear', `dump'
     and `show' commands can be found in *note Specifying Symbol
     Selections: d7. below.

 -- Description: stats [-m] [on|off]
     Enables (default) or disables "stats" mode, in which some
     statistics are printed after an expression has been evaluated.
     Invoking just `stats' or `stats on' only prints the cpu time in
     seconds for each evaluation. If the `-m' option is specified,
     memory usage is printed along with the cpu time, which indicates
     the maximum amount of expression memory (in terms of expression
     cells) used during the computation. Invoking `stats off' disables
     stats mode, while `stats -m off' just disables the printing of the
     memory usage statistics.

 -- Description: trace [symbol ...]
     Sets tracepoints on the given function or operator symbols. This
     works like the `break' command (see above) but only prints rule
     invocations and reductions without actually interrupting the
     evaluation. See *note Debugging: 39.  below for details.

 -- Description: underride
     Exits "override" mode. This returns you to the normal mode of
     operation, where new equations are added "below" previous rules of
     an existing function. See *note Definition Levels: d6. below for
     details.

  Note that these special commands are only recognized at the beginning
of the interactive command line (they are not reserved keywords of the
Pure language). Thus it's possible to "escape" identifiers looking like
commands by entering a space at the beginning of the line.

  Also note that symbols (identifiers, operators etc.) must always be
specified in fully qualified form. No form of namespace lookup is
performed by these commands, so they always work the same no matter
what `namespace' and `using namespace' declarations are currently in
effect.


File: pure.info,  Node: Last Result,  Next: Specifying Symbol Selections,  Prev: Interactive Commands,  Up: Interactive Usage

11.3 Last Result
================

Another convenience for interactive usage is the `ans' function, which
retrieves the most recent result printed in interactive mode. For
instance:

    > fact n = if n<=1 then 1 else n*fact (n-1);
    > map fact (1..10);
    [1,2,6,24,120,720,5040,40320,362880,3628800]
    > scanl (+) 0 ans;
    [0,1,3,9,33,153,873,5913,46233,409113,4037913]

Note that `ans' is just an ordinary function, defined in the prelude,
not a special command. However, there is a special `clear ans' command
which purges the `ans' value. This is useful, e.g., if you got a huge
result which you want to erase from memory before starting the next
computation.

    > clear ans
    > ans;
    ans



File: pure.info,  Node: Specifying Symbol Selections,  Next: The show Command,  Prev: Last Result,  Up: Interactive Usage

11.4 Specifying Symbol Selections
=================================

The `clear', `dump' and `show' commands all accept the following options
for specifying a subset of symbols and definitions on which to operate.
All symbols must be specified in fully qualified form. Options may be
combined, thus, e.g., `show -mft' is the same as `show -m -f -t'. Some
options specify optional numeric parameters; these must follow
immediately behind the option character if present, as in `-t0'.

`-c'
     Selects defined constants.

`-f'
     Selects defined functions.

`-g'
     Indicates that the following symbols are actually shell glob
     patterns and that all matching symbols should be selected.

`-m'
     Select defined macros.

`-pflag'
     Select only private symbols if _flag_ is nonzero (the default),
     otherwise (_flag_ is zero) select only public symbols. If this
     option is omitted then both private and public symbols are
     selected.

`-tlevel'
     Select symbols and definitions at the given "level" of definitions
     and above. This is described in more detail below. Briefly, the
     executing program and all imported modules (including the prelude)
     are at level 0, while "temporary" definitions made interactively
     in the interpreter are at level 1 and above. Thus a level of 1
     restricts the selection to all temporary definitions, whereas 0
     indicates all definitions (i.e., everything, including the
     prelude). If _level_ is omitted, it defaults to the current
     definitions level.

`-v'
     Select defined variables.

  In addition, the `-h' option prints a short help message describing
all available options of the command at hand.

  If none of the `-c', `-f', `-m' and `-v' options are specified, then
all kinds of symbols (constants, functions, macros and variables) are
selected, otherwise only the specified categories will be considered.

  A reasonable default is used if the `-t' option is omitted. By
default, if no symbols are specified, only temporary definitions are
considered, which corresponds to `-t1'. Otherwise the command applies
to all corresponding definitions, no matter whether they belong to the
executing program, the prelude, or some temporary level, which has the
same effect as `-t0'. This default choice can be overridden by
specifying the desired level explicitly.

  As a special case, just `clear' (without any other options or symbol
arguments) always backs out to the previous definitions level (instead
of level #1). This is inconsistent with the rules set out above, but is
implemented this way for convenience and backward compatibility. Thus,
if you really want to delete all your temporary definitions, use `clear
-t1' instead. When used in this way, the `clear' command will only
remove temporary definitions; if you need to remove definitions at
level #0, you must specify those symbols explicitly.

  Note that `clear -g *' will have pretty much the same disastrous
consequences as the Unix command `rm -rf *', so don't do that. Also note
that a macro or function symbol may well have defining equations at
different levels, in which case a command like `clear -tn foo' might
only affect some part of `foo''s definition. The `dump' and `show'
commands work analogously (albeit less destructively). See *note
Definition Levels: d6. below for some examples.


File: pure.info,  Node: The show Command,  Next: Definition Levels,  Prev: Specifying Symbol Selections,  Up: Interactive Usage

11.5 The show Command
=====================

The `show' command can be used to obtain information about defined
symbols in various formats. Besides the common selection options
discussed above, this command recognizes the following additional
options for specifying the content to be listed and the format to use.

`-a'
     Disassembles pattern matching automata. Works like the `-v4'
     option of the interpreter.

`-d'
     Disassembles LLVM IR, showing the generated LLVM assembler code of
     a function. Works like the `-v8' option of the interpreter.

`-e'
     Annotate printed definitions with lexical environment information
     (de Bruijn indices, subterm paths). Works like the `-v2' option of
     the interpreter.

`-l'
     Long format, prints definitions along with the summary symbol
     information. This implies `-s'.

`-s'
     Summary format, print just summary information about listed
     symbols.

  Symbols are always listed in lexicographic order. Note that some of
the options (in particular, `-a' and `-d') may produce excessive
amounts of information. By setting the *note PURE_MORE: 52. environment
variable, you can specify a shell command to be used for paging,
usually *more* or *less*.

  For instance, to list all temporary definitions made in an interactive
session, simply say:

    > show

You can also list a specific symbol, no matter whether it comes from the
interactive command line, the executing script or the prelude:

    > show foldl
    foldl f a x::matrix = foldl f a (list x);
    foldl f a s::string = foldl f a (chars s);
    foldl f a [] = a;
    foldl f a (x:xs) = foldl f (f a x) xs;

Wildcards can be used with the `-g' option, which is useful if you want
to print an entire family of related functions, e.g.:

    > show -g foldl*
    foldl f a x::matrix = foldl f a (list x);
    foldl f a s::string = foldl f a (chars s);
    foldl f a [] = a;
    foldl f a (x:xs) = foldl f (f a x) xs;
    foldl1 f x::matrix = foldl1 f (list x);
    foldl1 f s::string = foldl1 f (chars s);
    foldl1 f (x:xs) = foldl f x xs;

Or you can just specify multiple symbols as follows (this also works
with multiple glob patterns when you add the `-g' option):

    > show min max
    max x y = if x>=y then x else y;
    min x y = if x<=y then x else y;

You can also select symbols by category. E.g., the following command
shows summary information about all the variable symbols along with
their current values (using the "long" format):

    > show -lvg *
    argc       var  argc = 0;
    argv       var  argv = [];
    compiling  var  compiling = 0;
    sysinfo    var  sysinfo = "x86_64-unknown-linux-gnu";
    version    var  version = "0.47";
    5 variables

Or you can list just private symbols of the namespace `foo', as follows:

    > show -pg foo::*

The following command will list each and every symbol that's currently
defined (instead of `-g *' you can also use the `-t0' option):

    > show -g *

This usually produces a lot of output and is rarely needed, unless
you'd like to browse through an entire program including all library
imports. (In that case you might consider to use the `dump' command
instead, which writes the definitions to a file which can then be
loaded into a text editor for easier viewing. This may occasionally be
useful for debugging purposes.)

  Finally, there are two alternate forms of the `show' command: `show
namespace' which lists the current and search namespaces, and `show
namespaces' which lists all declared namespaces. These come in handy if
you have forgotten what namespaces are currently active and which other
namespaces are available in your program. For instance:

    > show namespace
    > show namespaces
    namespace C;
    namespace matrix;
    > using namespace C;
    > namespace my;
    > show namespace
    namespace my;
    using namespace C;



File: pure.info,  Node: Definition Levels,  Next: Debugging,  Prev: The show Command,  Up: Interactive Usage

11.6 Definition Levels
======================

To help with incremental development, the interpreter offers some
commands to manipulate the current set of definitions interactively. To
these ends, definitions are organized into different subsets called
*levels*. As already mentioned, the prelude, as well as other source
programs specified when invoking the interpreter, are always at level
0, while the interactive environment starts at level 1. Each `save'
command introduces a new temporary level, and each subsequent `clear'
command (without any arguments) "pops" the definitions on the current
level and returns you to the previous one (if any). This gives you a
"stack" of temporary environments which enables you to "plug and play"
in a (more or less) safe fashion, without affecting the rest of your
program.

  For all practical purposes, this stack is unlimited, so that you can
create as many levels as you like. However, this facility also has its
limitations. The interpreter doesn't really keep a full history of
everything you entered interactively, it only records the level a
variable, constant, and function or macro rule belongs to so that the
corresponding definitions can be removed again when the level is
popped. On the other hand, intermediate changes in variable values are
not recorded anywhere and cannot be undone. Moreover, global
declarations (which encompasses *note using: 31. clauses, *note extern:
95. declarations and special symbol declarations) always apply to all
levels, so they can't be undone either.

  That said, the temporary levels can still be pretty useful when
you're playing around with the interpreter. Here's a little example
which shows how to use `clear' to quickly get rid of a definition that
you entered interactively:

    > foo (x:xs) = x+foo xs;
    > foo [] = 0;
    > show
    foo (x:xs) = x+foo xs;
    foo [] = 0;
    > foo (1..10);
    55
    > clear
    This will clear all temporary definitions at level #1.
    Continue (y/n)? y
    > show
    > foo (1..10);
    foo [1,2,3,4,5,6,7,8,9,10]

We've seen already that normally, if you enter a sequence of equations,
they will be recorded in the order in which they were written. However,
it is also possible to override definitions in lower levels with the
`override' command:

    > foo (x:xs) = x+foo xs;
    > foo [] = 0;
    > show
    foo (x:xs) = x+foo xs;
    foo [] = 0;
    > foo (1..10);
    55
    > save
    save: now at temporary definitions level #2
    > override
    > foo (x:xs) = x*foo xs;
    > show
    foo (x:xs) = x*foo xs;
    foo (x:xs) = x+foo xs;
    foo [] = 0;
    > foo (1..10);
    warning: rule never reduced: foo (x:xs) = x+foo xs;
    0

Note that the equation `foo (x:xs) = x*foo xs' was inserted before the
previous rule `foo (x:xs) = x+foo xs', which is at level #1. (The latter
equation is now "shadowed" by the rule we just entered, hence the
compiler warns us that this rule can't be reduced any more.)

  Even in override mode, new definitions will be added after other
definitions at the _current_ level. This allows us to just continue
adding more high-priority definitions overriding lower-priority ones:

    > foo [] = 1;
    > show
    foo (x:xs) = x*foo xs;
    foo [] = 1;
    foo (x:xs) = x+foo xs;
    foo [] = 0;
    > foo (1..10);
    warning: rule never reduced: foo (x:xs) = x+foo xs;
    warning: rule never reduced: foo [] = 0;
    3628800

Again, the new equation was inserted above the existing lower-priority
rules, but below our previous equation `foo (x:xs) = x*foo xs' entered
at the same level. As you can see, we have now effectively replaced our
original definition of `foo' with a version that calculates list
products instead of sums, but of course we can easily go back one level
to restore the previous definition:

    > clear
    This will clear all temporary definitions at level #2.
    Continue (y/n)? y
    clear: now at temporary definitions level #1
    clear: override mode is on
    > show
    foo (x:xs) = x+foo xs;
    foo [] = 0;
    > foo (1..10);
    55

Note that `clear' reminded us that override mode is still enabled
(`save' will do the same if override mode is on while pushing a new
definitions level). To turn it off again, use the `underride' command.
This will revert to the normal behaviour of adding new equations below
existing ones:

    > underride

It's also possible to use `clear' to back out multiple levels at once,
if you specify the target level to be cleared with the -t option. For
instance:

    > save
    save: now at temporary definitions level #2
    > let bar = 99;
    > show
    let bar = 99;
    foo (x:xs) = x+foo xs;
    foo [] = 0;
    > // this scraps all our scribblings!
    > clear -t1
    This will clear all temporary definitions at level #1 and above.
    Continue (y/n)? y
    clear: now at temporary definitions level #1
    > show
    >

Finally, it is worth noting here that the facilities described above
are also available to Pure programs, as the `save' and `clear' commands
can also be executed under program control using the `evalcmd'
primitive; see *note Reflection: e9. in the *note Caveats and Notes: c.
section for details.


File: pure.info,  Node: Debugging,  Next: Interactive Startup,  Prev: Definition Levels,  Up: Interactive Usage

11.7 Debugging
==============

The interpreter provides a simple but reasonably convenient symbolic
debugging facility when running interactively. To make this work, you
have to specify the *note -g: 13. option when invoking the interpreter
(`pure -g'). If you're already at the interpreter's command line, you
can also use the `run -g' command to enable the debugger. The `-g'
option disables tail call optimization (see *note Stack Size and Tail
Recursion: 3f.) to make it easier to debug programs. It also causes
special debugging code to be generated which will make your program run
_much_ slower. Therefore the *note -g: 13. option should only be used
if you actually need the debugger.

  One common use of the debugger is "post mortem" debugging after an
evaluation ended with an unhandled exception. In such a case, the `bt'
command of the interpreter prints a backtrace of the call sequence
which caused the exception. Note that this only works if debugging mode
was enabled. For instance:

    > [1,2]!3;
    <stdin>, line 2: unhandled exception 'out_of_bounds' while evaluating '[1,2]!3'
    > bt
       [1] (!): (x:xs)!n::int = xs!(n-1) if n>0;
         n = 3; x = 1; xs = [2]
       [2] (!): (x:xs)!n::int = xs!(n-1) if n>0;
         n = 2; x = 2; xs = []
       [3] (!): []!n::int = throw out_of_bounds;
         n = 1
    >> [4] throw: extern void pure_throw(expr*) = throw;
         x1 = out_of_bounds

The last call, which is also marked with the `>>' symbol, is the call
that raised the exception. The format is similar to the `p' command of
the debugger, see below, but `bt' always prints a full backtrace. (As
with the `show' command of the interpreter, you can set the *note
PURE_MORE: 52.  environment variable to pipe the output through the
corresponding command, or use `evalcmd' to capture the output of `bt'
in a string.)

  The debugger can also be used interactively. To these ends, you can
set breakpoints on functions with the `break' command. The debugger
then gets invoked as soon as a rule for one of the given functions is
executed. Example:

    > fact n::int = if n>0 then n*fact (n-1) else 1;
    > break fact
    > fact 1;
    ** [1] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 1
    (Type 'h' for help.)
    :
    ** [2] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 0
    :
    ++ [2] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 0
         --> 1
    ** [2] (*): x::int*y::int = x*y;
         x = 1; y = 1
    :
    ++ [2] (*): x::int*y::int = x*y;
         x = 1; y = 1
         --> 1
    ++ [1] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 1
         --> 1
    1

Lines beginning with `**' indicate that the evaluation was interrupted
to show the rule (or external) which is currently being considered,
along with the current depth of the call stack, the invoked function
and the values of parameters and other local variables in the current
lexical environment. In contrast, the prefix `++' denotes reductions
which were actually performed during the evaluation and the results
that were returned by the function call (printed as `--> return value').

  Sometimes you might also see funny symbols like `#<closure>',
`#<case>' or `#<when>' instead of the function name. These indicate
lambdas and the special variable-binding environments, which are all
implemented as anonymous closures in Pure. Also note that the debugger
doesn't know about the argument names of external functions (which are
optional in Pure and not recorded anywhere), so it will display the
generic names `x1', `x2' etc. instead.

  At the debugger prompt '`:'' you can enter various special debugger
commands, or just keep on hitting the carriage return key to walk
through an evaluation step by step, as we did in the example above.
(Command line editing works as usual at the debugger prompt, if it is
enabled.) The usual commands are provided to walk through an
evaluation, print and navigate the call stack, step over the current
call, or continue the evaluation unattended until you hit another
breakpoint. If you know other source level debuggers like *gdb* then
you should feel right at home. You can type `h' at the debugger prompt
to print the following list:

    : h
    Debugger commands:
    a       auto: step through the entire program, run unattended
    c [f]   continue until next breakpoint, or given function f
    h       help: print this list
    n       next step: step over reduction
    p [n]   print rule stack (n = number of frames)
    r       run: finish evaluation without debugger
    s       single step: step into reduction
    t, b    move to the top or bottom of the rule stack
    u, d    move up or down one level in the rule stack
    x       exit the interpreter (after confirmation)
    .       reprint current rule
    ! cmd   shell escape
    ? expr  evaluate expression
    <cr>    single step (same as 's')
    <eof>   step through program, run unattended (same as 'a')

The command syntax is very simple. Besides the commands listed above
you can also enter comment lines (`// comment text') which will just be
ignored. Extra arguments on commands which don't expect any will
generally be ignored as well. The single letter commands all have to be
separated from any additional parameters with whitespace, whereas the
'`!'', '`?'' and '`.'' commands count as word delimiters and can thus
be followed immediately by an argument. For convenience, the '`?''
command can also be omitted if the expression to be evaluated doesn't
start with a single letter or one of the special punctuation commands.

  The debugger can be exited or suspended in the following ways:

   * You can type `c' to continue the evaluation until the next
     breakpoint, or `c foo' in order to proceed until the debugger hits
     an invokation of the function `foo'.

   * You can type `r' to run the rest of the evaluation without the
     debugger.

   * The `a' ("auto") command single-steps through the rest of the
     evaluation, running unattended. This command can also be entered
     by just hitting the end-of-file key (`Ctrl-d' on Unix systems) at
     the debugger prompt.

   * You can also type `x' to exit from the debugger _and_ the
     interpreter immediately (after confirmation).

  At the debugger prompt, you can use the `u' ("up"), `d' ("down"), `t'
("top") and `b' ("bottom") commands to move around on the current call
stack. The `p' command prints a range of the call stack centered around
the currently selected stack frame, which is indicated with the `>>'
tag, whereas `**' denotes the current bottom of the stack (which is the
rule to be executed with the single step command). The `p' command can
also be followed by a numeric argument which indicates the number of
stack frames to be printed (this will then become the default for
subsequent invocations of `p'). The `n' command steps over the call
selected with the stack navigation commands. For instance:

    > fact 3;
    ** [1] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 3
    : c *
    ** [4] (*): x::int*y::int = x*y;
         x = 1; y = 1
    : p
       [1] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 3
       [2] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 2
       [3] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 1
    ** [4] (*): x::int*y::int = x*y;
         x = 1; y = 1
    : u
    >> [3] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 1
    : u
    >> [2] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 2
    : p
       [1] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 3
    >> [2] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 2
       [3] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 1
    ** [4] (*): x::int*y::int = x*y;
         x = 1; y = 1
    : n
    ++ [2] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 2
         --> 2
    ** [2] (*): x::int*y::int = x*y;
         x = 3; y = 2
    :

If you ever get lost, you can reprint the current rule with the '`.''
command:

    : .
    ** [2] (*): x::int*y::int = x*y;
         x = 3; y = 2

Another useful feature is the `?' command which lets you evaluate any
Pure expression, with the local variables of the current rule bound to
their corresponding values. Like the `n' command, `?' applies to the
current stack frame as selected with the stack navigation commands. The
expression must be entered on a single line, and the trailing semicolon
is optional. For instance:

    > fact 3;
    ** [1] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 3
    : c *
    ** [4] (*): x::int*y::int = x*y;
         x = 1; y = 1
    : ?x+y
    2
    : u
    >> [3] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 1
    : n>0, fact n
    1,1

A third use of the debugger is to trace function calls. For that the
interpreter provides the `trace' command which works similarly to
`break', but sets so-called "tracepoints" which only print rule
invocations and reductions instead of actually interrupting the
evaluation. For instance, assuming the same example as above, let's
first remove the breakpoint on `fact' (using the `del' command) and
then set it as a tracepoint instead:

    > del fact
    > trace fact
    > fact 1;
    ** [1] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 1
    ** [2] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 0
    ++ [2] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 0
         --> 1
    ++ [1] fact: fact n::int = if n>0 then n*fact (n-1) else 1;
         n = 1
         --> 1
    1

The `break' and `trace' commands can also be used in concert if you want
to debug some functions while only tracing others.

  The current sets of breakpoints and tracepoints can be changed with
the `break', `trace' and `del' commands, as shown above, and just
`break' or `trace' without any arguments lists the currently defined
breakpoints or tracepoints, respectively. Please see *note Interactive
Commands: cc. above for details. Also note that these are really
interpreter commands, so to enter them you first have to exit the
debugger (using the `a' or `r' command) if an evaluation is currently
in progress. (However, it's possible to set a temporary breakpoint in
the debugger with the `c' command, see above.)


File: pure.info,  Node: Interactive Startup,  Prev: Debugging,  Up: Interactive Usage

11.8 Interactive Startup
========================

In interactive mode, the interpreter also runs some additional scripts
at startup, after loading the prelude and the scripts specified on the
command line. This lets you tailor the interactive environment to your
liking.

  The interpreter first looks for a .purerc file in the user's home
directory (as given by the `HOME' environment variable) and then for a
.purerc file in the current working directory. These are just ordinary
Pure scripts which may contain any additional definitions that you
need. The .purerc file in the home directory is for global definitions
which should always be available when running interactively, while the
.purerc file in the current directory can be used for project-specific
definitions.

  Finally, you can also have a .pure initialization file in the current
directory, which is usually created with the `dump' command (see
above). This file is loaded after the .purerc files if it is present.

  The interpreter processes all these files in the same way as with the
`run' command (see above). When invoking the interpreter, you can
specify the *note --norc: 22. option on the command line if you wish to
skip these initializations.


File: pure.info,  Node: Batch Compilation,  Next: Caveats and Notes,  Prev: Interactive Usage,  Up: Top

12 Batch Compilation
********************

The interpreter's *note -c: 16. option provides a means to turn Pure
scripts into standalone executables. This feature is still a bit
experimental. In particular, note that the compiled executable is
essentially a _static snapshot_ of your program which is executed on
the "bare metal", without a hosting interpreter. Only a minimal runtime
system is provided. This considerably reduces startup times, but also
implies the following quirks and limitations:

   * All toplevel expressions and *note let: 80. bindings are evaluated
     _after_ all functions have been defined. This might cause
     inconsistent behaviour with an interpreted run of the same
     program, which executes expressions and variable definitions
     immediately, as the program is being processed. To avoid these
     semantic differences, you'll have to make sure that expressions
     are evaluated _after_ all functions used in the evaluation have
     been defined completely.

   * Toplevel expressions won't be of much use in a batch-compiled
     program, unless, of course, they are evaluated for their
     side-effects. Usually your program will have to include at least
     one of these to play the role of the "main program" in your
     script. In most cases these expressions are best placed after all
     the function and variable definitions, at the end of your program.

   * The `eval' function can only be used to evaluate plain toplevel
     expressions.  You can define local functions and variables in
     *note with: 67. and *note when: 6e. clauses inside an expression,
     but you can't use `eval' to define new global variables and
     functions. In other words, anything which changes the executing
     program is "verboten".  Moreover, the introspective capabilities
     provided by `evalcmd' (discussed under *note Reflection: e9. in
     the *note Caveats and Notes: c. section) won't work either because
     the interactive commands are all disabled. If you need any of
     these capabilities, you have to run your program with the
     interpreter.

   * Constant and macro definitions, being compile time features, aren't
     available in the compiled program. If you need to use these with
     `eval' at run time, you have to provide them through variable and
     function definitions instead. Also, the compiler usually strips
     unused functions from the output code, so that only functions
     which are actually called somewhere in the static program text are
     available to `eval'.  (The *note -u: 25. option and the *note
     --required: 49. pragma can be used to avoid this, see *note Code
     Size and Unstripped Executables: ec. below.)

   * Code which gets executed to compute constant values at compile
     time will generally _not_ be executed in the compiled executable,
     so your program shouldn't rely on side-effects of such
     computations (this would be bad practice anyway). There is an
     exception to this rule, however, namely if a constant value
     contains run time data such as pointers and local functions which
     requires an initialization at run time, then the batch compiler
     will generate code for that. (The same happens if the *note
     --noconst: 42. option is used to force computation of constant
     values at run time, see *note Code Size and Unstripped
     Executables: ec.)

  What all this boils down to is that anything which requires the
compile time or interactive facilities of the interpreter, is
unavailable. These restrictions only apply at run time, of course. At
compile time the program _is_ being executed by the interpreter so you
can use `eval' and `evalcmd' in any desired way. See the description of
the `compiling' variable below for how to distinguish these cases in
your script.

  For most kinds of scripts, the above restrictions aren't really that
much of an obstacle, or can easily be worked around. For the few
scripts which actually need the full dynamic capabilities of Pure
you'll just have to run the script with the interpreter. This isn't a
big deal either, only the startup will be somewhat slower because the
script is compiled on the fly. Once the JIT has done its thing the
"interpreted" script will run every bit as fast as the "compiled" one,
since in fact _both_ are compiled (only at different times) to exactly
the same code!

  Also note that during a batch compilation, the compiled program is
actually executed as usual, i.e., the script is also run _at compile
time_. This might first seem to be a big annoyance, but it actually
opens the door for some powerful programming techniques like partial
evaluation (http://en.wikipedia.org/wiki/Partial_evaluation). It is
also a necessity because of Pure's highly dynamic nature. For instance,
Pure allows you to define constants by evaluating an arbitrary
expression (see *note Constant Definitions: 87. below), and using
`eval' a program can easily modify itself in even more unforeseeable
ways. Therefore pretty much anything in your program can actually
depend on previous computations performed while the program is being
executed.

* Menu:

* Example::
* Code Size and Unstripped Executables::
* Other Output Code Formats::
* Calling Pure Functions From C::


File: pure.info,  Node: Example,  Next: Code Size and Unstripped Executables,  Up: Batch Compilation

12.1 Example
============

For the sake of a concrete example, consider the following little
script:

    using system;

    fact n = if n>0 then n*fact (n-1) else 1;

    main n = do puts ["Hello, world!", str (map fact (1..n))];

    if argc<=1 then () else main (sscanf (argv!1) "%d");

When invoked from the command line, with the number `n' as the first
parameter, this program will print the string `"Hello, world!"' and the
list of the first `n' factorials:

    $ pure -x hello.pure 10
    Hello, world!
    [1,2,6,24,120,720,5040,40320,362880,3628800]

Note the condition on `argc' in the last line of the script. This
prevents the program from producing an exception if no command line
parameters are specified, so that the program can also be run
interactively:

    $ pure -i -q hello.pure
    > main 10;
    Hello, world!
    [1,2,6,24,120,720,5040,40320,362880,3628800]
    ()
    > quit

To turn the script into an executable, we just invoke the Pure
interpreter with the *note -c: 16. option, using the *note -o: 23.
option to specify the desired output file name:

    $ pure -c hello.pure -o hello
    $ ./hello 10
    Hello, world!
    [1,2,6,24,120,720,5040,40320,362880,3628800]

Next suppose that we'd like to supply the value `n' at _compile_ rather
than run time. To these ends we want to turn the value passed to the
`main' function into a compile time constant, which can be done as
follows:

    const n = if argc>1 then sscanf (argv!1) "%d" else 10;

(Note that we provide `10' as a default if `n' isn't specified on the
command line.)

  Moreover, in such a case we usually want to skip the execution of the
main function at compile time. The Pure runtime provides a special
system variable `compiling' which holds a truth value indicating
whether the program is actually running under the auspices of the batch
compiler, so that it can adjust accordingly. In our example, the
evaluation of `main' becomes:

    if compiling then () else main n;

Our program now looks as follows:

    using system;

    fact n = if n>0 then n*fact (n-1) else 1;

    main n = do puts ["Hello, world!", str (map fact (1..n))];

    const n = if argc>1 then sscanf (argv!1) "%d" else 10;
    if compiling then () else main n;

This script "specializes" `n' to the first (compile time) parameter when
being batch-compiled, and it still works as before when we run it
through the interpreter in both batch and interactive mode, too:

    $ pure -i -q hello.pure
    Hello, world!
    [1,2,6,24,120,720,5040,40320,362880,3628800]
    > main 5;
    Hello, world!
    [1,2,6,24,120]
    ()
    > quit

    $ pure -x hello.pure 7
    Hello, world!
    [1,2,6,24,120,720,5040]

    $ pure -o hello -c -x hello.pure 7
    $ ./hello
    Hello, world!
    [1,2,6,24,120,720,5040]

You'll rarely need an elaborate setup like this, most of the time
something like our simple first example will do the trick. But, as
you've seen, Pure can easily do it.


File: pure.info,  Node: Code Size and Unstripped Executables,  Next: Other Output Code Formats,  Prev: Example,  Up: Batch Compilation

12.2 Code Size and Unstripped Executables
=========================================

By default, the batch compiler strips unused functions from the output
code, to keep the code size small. You can disable this with the *note
-u: 25.  option, in which case the output code includes _all_ functions
defined in the compiled program or imported through a *note using: 31.
clause, even if they don't seem to be used anywhere. This considerably
increases compilation times and makes the compiled executable much
larger. For instance, on a 64 bit Linux systems with ELF binaries the
executable of our hello.pure example is about thrice as large:

    $ pure -o hello -c -x hello.pure 7 && ls -l hello
    -rwxr-xr-x 1 ag users 178484 2010-01-12 06:21 hello
    $ pure -o hello -c -u -x hello.pure 7 && ls -l hello
    -rwxr-xr-x 1 ag users 541941 2010-01-12 06:21 hello

(Note that even the stripped executable is fairly large when compared to
compiled C code, as it still contains the symbol table of the entire
program, which is needed by the runtime environment.)

  Stripped executables should be fine for most purposes, but you have
to be careful when using `eval' in your compiled program. The compiler
only does a _static_ analysis of which functions might be reached from
the initialization code (i.e., toplevel expressions and *note let: 80.
bindings). It does _not_ take into account code run via the `eval'
routine. Thus, functions used only in `eval'ed code will be stripped
from the executable, as if they were never defined at all. If such a
function is then being called using `eval' at runtime, it will evaluate
to a plain constructor symbol.

  If this is a problem then you can either use the *note -u: 25. option
to produce an unstripped executable, or you can force functions to be
included in the stripped executable with the *note --required: 49.
pragma (cf. *note Code Generation Options: 2b.). For instance:

    #! --required foo
    foo x = bar (x-1);
    eval "foo 99";

There is another code generation option which may have a substantial
effect on code size, namely the *note --noconst: 42. option. Normally,
constant values defined in a `const' definition are precomputed at
compile time and then stored in the generated executable; this reduces
startup times but may increase the code size considerably if your
program contains big constant values such as lists. If you prefer
smaller executables then you can use the *note --noconst: 42. option to
force the value of the constant to be recomputed at run time (which
effectively turns the constant into a kind of read-only variable). For
instance:

    #! --noconst
    const xs = 1L..100000L;
    sum = foldl (+) 0;

    using system;
    puts $ str $ sum xs;

On my 64 bit Linux system this produces a 187115 bytes executable.
Without *note --noconst: 42. the code becomes almost an order of
magnitude larger in this case (1788699 bytes). On the other hand, the
smaller executable also takes a little longer to run since it must
first recompute the value of the list constant at startup. So you have
to consider the tradeoffs in a given situation. Usually big executables
aren't much of a problem on modern operating systems, but if your
program contains a lot of big constants then this may become an
important consideration. However, if a constant value takes a long time
to compute then you'll be better off with the default behaviour of
precomputing the value at compile time.


File: pure.info,  Node: Other Output Code Formats,  Next: Calling Pure Functions From C,  Prev: Code Size and Unstripped Executables,  Up: Batch Compilation

12.3 Other Output Code Formats
==============================

Note that while the batch compiler generates native executables by
default, it can just as well create object files which can be linked
into other C/C++ programs and libraries:

    $ pure -o hello.o -c -x hello.pure 7

The .o extension tells the compiler that you want an object file. When
linking the object module, you also need to supply an initialization
routine which calls the `__pure_main__' function in hello.o to
initialize the compiled module. This routine is declared in C/C++ code
as follows:

    extern "C" void __pure_main__(int argc, char** argv);

As indicated, `__pure_main__' is to be invoked with two parameters, the
argument count and `NULL'-terminated argument vector which become the
`argc' and the `argv' of the Pure program, respectively. (You can also
just pass 0 for both arguments if you don't need to supply command line
parameters.) The purpose of `__pure_main__' is to initialize a shell
instance of the Pure interpreter which provides the minimal runtime
support necessary to execute the Pure program, and to invoke all
"initialization code" (variable definitions and toplevel expressions)
of the program itself.

  A minimal C `main' function which does the job of initializing the
Pure module looks as follows:

    extern void __pure_main__(int argc, char** argv);

    int main(int argc, char** argv)
    {
      __pure_main__(argc, argv);
      return 0;
    }

If you link the `main' routine with the Pure module, don't forget to
also pull in the Pure runtime library. Assuming that the above C code
is in pure_main.c:

    $ gcc -c pure_main.c -o pure_main.o
    $ g++ -o hello hello.o pure_main.o -lpure
    $ ./hello
    Hello, world!
    [1,2,6,24,120,720,5040]

(The C++ compiler is used as the linker here so that the standard C++
library gets linked in, too. This is necessary because Pure's runtime
library is actually written in C++.)

  In fact, this is pretty much what `pure -c' actually does for you when
creating an executable.

  If your script loads dynamic libraries (`using "lib:...";') then
you'll also have to link with those; _all_ external references have to
be resolved at compile time. This is taken care of automatically when
creating executables. Otherwise it is a good idea to run `pure -c' with
the `-v0100' verbosity option so that it prints the libraries to be
linked (in addition to the commands which are invoked in the
compilation process):

    $ pure -v0100 -c hello.pure -o hello.o
    opt -f -std-compile-opts hello.o.bc | llc -f -o hello.o.s
    gcc -c hello.o.s -o hello.o
    Link with: g++ hello.o -lpure

Well, we already knew that, so let's consider a slightly more
interesting example from Pure's ODBC module:

    $ pure -v0100 -c pure-odbc/examples/menagerie.pure -o menagerie.o
    opt -f -std-compile-opts menagerie.o.bc | llc -f -o menagerie.o.s
    gcc -c menagerie.o.s -o menagerie.o
    Link with: g++ menagerie.o /usr/local/lib/pure/odbc.so -lpure
    $ g++ -shared -o menagerie.so menagerie.o /usr/local/lib/pure/odbc.so -lpure

Note that the listed link options are necessary but might not be
sufficient; `pure -c' just makes a best guess based on the Pure source.
On most systems this will be good enough, but if it isn't, you can just
add options to the linker command as needed to pull in additional
required libraries.

  As this last example shows, you can also create shared libraries from
Pure modules. However, on some systems (most notably x86_64), this
requires that you pass the *note -fPIC: 1a. option when batch-compiling
the module, so that position-independent code is generated:

    $ pure -c -fPIC pure-odbc/examples/menagerie.pure -o menagerie.o

Also note that even when building a shared module, you'll have to
supply an initialization routine which calls `__pure_main__' somewhere.

  Last but not least, `pure -c' can also generate just plain LLVM
assembler code:

    pure -c hello.pure -o hello.ll

Note the .ll extension; this tells the compiler that you want an LLVM
assembler file. An LLVM bitcode file can be created just as easily:

    pure -c hello.pure -o hello.bc

In these cases you'll have to have to handle the rest of the compilation
yourself. This gives you the opportunity, e.g., to play with special
optimization and code generation options provided by the LLVM
toolchain. Please refer to the LLVM documentation
(http://llvm.org/docs/) (in particular, the description of the opt and
llc programs) for details.


File: pure.info,  Node: Calling Pure Functions From C,  Prev: Other Output Code Formats,  Up: Batch Compilation

12.4 Calling Pure Functions From C
==================================

Another point worth mentioning here is that you can't just call Pure
functions in a batch-compiled module directly. That's because in order
to call a Pure function, at least in the current implementation, you
have to set up a Pure stack frame for the function. However, there's a
convenience function called `pure_funcall' in the runtime API to handle
this. This function takes a pointer to the Pure function, the argument
count and the arguments themselves (as `pure_expr*' objects) as
parameters. For instance, here is a pure_main.c module which can be
linked against the hello.pure program from above, which calls the
`fact' function from the Pure program:

    #include <stdio.h>
    #include <pure/runtime.h>

    extern void __pure_main__(int argc, char** argv);
    extern pure_expr *fact(pure_expr *x);

    int main()
    {
      int n = 10, m;
      __pure_main__(0, NULL);
      if (pure_is_int(pure_funcall(fact, 1, pure_int(n)), &m))
        printf("fact %d = %d\n", n, m);
      return 0;
    }

And here's how you can compile, link and run this program:

    $ pure -o hello.o -c -x hello.pure 7
    $ gcc -o pure_main.o -c pure_main.c
    $ g++ -o myhello hello.o pure_main.o -lpure
    $ ./myhello
    Hello, world!
    [1,2,6,24,120,720,5040]
    fact 10 = 3628800

Note that the first two lines are output from the Pure program; the
last line is what gets printed by the `main' routine in pure_main.c.


File: pure.info,  Node: Caveats and Notes,  Next: Author,  Prev: Batch Compilation,  Up: Top

13 Caveats and Notes
********************

This section is a grab bag of casual remarks, useful tips and tricks,
and information on common pitfalls, quirks and limitations of the
current implementation and how to deal with them.

* Menu:

* Etymology::
* Backward Compatibility::
* Error Recovery::
* The __show__ Function::
* Non-Linear Patterns::
* "As" Patterns::
* Head = Function::
* With and when::
* Numeric Calculations::
* Constant Definitions::
* External C Functions::
* Calling Special Forms::
* Laziness::
* Reflection::
* Hygienic Macros::
* Stack Size and Tail Recursion::
* Handling of Asynchronous Signals::


File: pure.info,  Node: Etymology,  Next: Backward Compatibility,  Up: Caveats and Notes

13.1 Etymology
==============

People keep asking me what's so "pure" about Pure. The long and
apologetic answer is that Pure tries to stay as close as possible to
the spirit of term rewriting without sacrificing practicality. It's
possible and in fact quite easy to write purely functional programs in
Pure, and you're encouraged to do so whenever this is possible or at
least reasonable. On the other hand, Pure doesn't get in your way if
you want to call external operations with side effects; it does allow
you to call any C function after all.

  The short (and true) answer is that I simply liked the name, and
there wasn't any programming language named "Pure" yet (quite a feat
nowadays), so there's one now. If you insist on a (recursive)
backronym, just take "PURE" to stand for the "Pure Universal Rewriting
Engine".


File: pure.info,  Node: Backward Compatibility,  Next: Error Recovery,  Prev: Etymology,  Up: Caveats and Notes

13.2 Backward Compatibility
===========================

Pure is based on the author's earlier *note Q: f4. language, but it
offers many new and powerful features and programs run much faster than
their Q equivalents. The language also went through a thorough facelift
in order to modernize the syntax and make it more similar to other
modern-style functional languages, in particular *note Miranda: f5. and
*note Haskell: 5e. Thus porting Q scripts to Pure often involves a
substantial amount of manual work, but it can (and has) been done.

  Since its modest beginnings in April 2008, Pure has gone through a
lot of major and minor revisions which raise various backward
compatibility issues.  We document these in the following, in order to
facilitate the porting of older Pure scripts.

  Pure 0.7 introduced built-in matrix structures, which called for some
minor changes in the syntax of comprehensions and arithmetic sequences.
Specifically, the template expression and generator/filter clauses of a
comprehension are now separated with `|' instead of `;'. Moreover,
arithmetic sequences with arbitrary stepsize are now written `x:y..z'
instead of `x,y..z', and the '`..'' operator now has a higher precedence
than the '`,'' operator. This makes writing matrix slices like
`x!!(i..j,k..l)' much more convenient.

  In Pure 0.13 the naming of the logical and bitwise operations was
changed, so that these are now called `~', `&&', `||' and
`not'/`and'/`or', respectively. (Previously, `~' was used for bitwise,
`not' for logical negation, which was rather inconsistent, albeit
compatible with the naming of the `not' operation in Haskell and ML.)
Also, to stay in line with this naming scheme, inequality was renamed
to `~=' (previously `!=').

  Pure 0.14 introduced the namespaces feature. Consequently, the scope
of private symbols is now confined to a namespace rather than a source
module; scripts making use of private symbols need to be adapted
accordingly. Also note that syntax like `foo::int' may now also denote
a qualified symbol rather than a tagged variable, if `foo' has been
declared as a namespace. You can work around such ambiguities by
renaming the variable, or by placing spaces around the '`::'' delimiter
(these aren't permitted in a qualified symbol, so the construct `foo ::
int' is always interpreted as a tagged variable, no matter whether
`foo' is also a valid namespace).

  Pure 0.26 extended the namespaces feature to add support for
hierarchical namespaces. This means that name lookup works in a
slightly different fashion now (see *note Hierarchical Namespaces: ab.
for details), but old code which doesn't use the new feature should
continue to work unchanged.

  Pure 0.26 also changed the `nullary' keyword to *note nonfix: 69,
which is more consistent with the other kinds of fixity declarations.
Moreover, the parser was enhanced so that it can cope with a
theoretically unbounded number of precedence levels, and the system of
standard operators in the prelude was modified so that it becomes
possible to sneak in new operator symbols with ease; details can be
found in the *note Symbol Declarations: 5b. section.

  Pure 0.41 added support for optimization of indirect tail calls, so
that any previous restrictions on the use of tail recursion in indirect
function calls and mutually recursive globals have been removed.
Moreover, the logical operators `&&' and `||' are now tail-recursive in
their second operand and can also be extended with user-defined
equations, just like the other builtins. Note that this implies that
the values returned by `&&' and `||' aren't normalized to the values 0
and 1 any more (this isn't possible with tail call semantics). If you
need this then you'll have to make sure that either the operands are
already normalized, or you'll have to normalize the result yourself.

  Also, as of Pure 0.41 the batch compiler produces stripped
executables by default. To create unstripped executables you now have
to use the *note -u: 25.  option, see *note Code Size and Unstripped
Executables: ec. for details. The `-s' option to produce stripped
executables is still provided for backward compatibility, but it won't
have any effect unless you use it to override a previous *note -u: 25.
option.

  Pure 0.43 changed the rules for looking up symbols in user-defined
namespaces.  Unqualified symbols are now created in the current (rather
than the global) namespace by default, see *note Symbol Lookup and
Creation: 3a. for details. The *note -w: 28. option can be used to get
warnings about unqualified symbols which are resolved to a different
namespace than previously. It also provides a means to check your
scripts for implicit declarations which might indicate missing or
mistyped function symbols.

  Pure 0.45 added support for checking arbitrary pointer types in the C
interface, so that you don't have to worry about passing the wrong
kinds of pointers to system and library routines any more. Moreover,
the interpretation of numeric pointer arguments (`int*' etc.) was
changed to bring them in line with the other new numeric matrix
conversions (`int**' etc.). In particular, the matrix data can now be
modified in-place and type checking is more strict (`int*' requires an
int matrix, etc.). Also, there's now support for `argv'-style vector
arguments (`char**' and `void**'). Please see the *note C Types: b9.
section for details.

  Pure 0.47 changed the syntax used to denote *note inline code: 55.
sections from `%{...%}' to `%<...%>'. This resolves an ambiguity in the
syntax (note that `%{' is legal Pure syntax; it could denote a `%'
operator followed by a matrix value), and also makes it easier to
properly support this construct in Emacs Pure mode.


File: pure.info,  Node: Error Recovery,  Next: The __show__ Function,  Prev: Backward Compatibility,  Up: Caveats and Notes

13.3 Error Recovery
===================

The parser uses a fairly simplistic panic mode error recovery which
tries to catch syntax errors at the toplevel only. This seems to work
reasonably well, but might catch some errors much too late.
Unfortunately, Pure's terseness makes it rather difficult to design a
better scheme. As a remedy, the parser accepts an empty definition
(just `;' by itself) at the toplevel only. Thus, in interactive usage,
if the parser seems to eat away your input without doing anything,
entering an extra semicolon or two should break the spell, putting you
back at the toplevel where you can start typing the definition again.


File: pure.info,  Node: The __show__ Function,  Next: Non-Linear Patterns,  Prev: Error Recovery,  Up: Caveats and Notes

13.4 The __show__ Function
==========================

 -- Function: __show__ x
     This function provides a "hook" to override the print
     representations of expressions at runtime, which works in a
     fashion similar to Haskell's `show' function.

  *note __show__: 8e. is just an ordinary Pure function expected to
return a string with the desired custom representation of a normal form
value given as the function's single argument. The interpreter prints
the strings returned by *note __show__: 8e. just as they are. It will
not check whether they conform to Pure syntax and/or semantics, or
modify them in any way. Also, the library doesn't define this function
anywhere, so you are free to add any rules that you want.

  Custom print representations are most useful for interactive
purposes, if you're not happy with the default print syntax of some
kinds of objects. One particularly useful application of *note
__show__: 8e. is to change the format of numeric values. Here are some
examples:

    > using system;
    > __show__ x::double = sprintf "%0.6f" x;
    > 1/7;
    0.142857
    > __show__ x::int = sprintf "0x%0x" x;
    > 1786;
    0x6fa
    > using math;
    > __show__ (x::double +: y::double) = sprintf "%0.6f+%0.6fi" (x,y);
    > cis (-pi/2);
    0.000000+-1.000000i

The prelude function `str', which returns the print representation of
any Pure expression, calls *note __show__: 8e. as well:

    > str (1/7);
    "0.142857"

Conversely, you can call the `str' function from *note __show__: 8e, but
in this case it always returns the default representation of an
expression. This prevents the expression printer from going recursive,
and allows you to define your custom representation in terms of the
default one. E.g., the following rule removes the `L' suffixes from
bigint values:

    > __show__ x::bigint = init (str x);
    > fact n = foldl (*) 1L (1..n);
    > fact 30;
    265252859812191058636308480000000

Of course, your definition of *note __show__: 8e. can also call *note
__show__: 8e.  itself recursively to determine the custom
representation of an object.

  One case which needs special consideration are thunks (futures). The
printer will _never_ use *note __show__: 8e. for those, to prevent them
from being forced inadvertently. In fact, you _can_ use *note __show__:
8e. to define custom representations for thunks, but only in the
context of a rule for other kinds of objects, such as lists. For
instance:

    > nonfix ...;
    > __show__ (x:xs) = str (x:...) if thunkp xs;
    > 1:2:(3..inf);
    1:2:3:...

Another case which needs special consideration are numeric matrices. For
efficiency, the expression printer will always use the default
representation for these, unless you override the representation of the
matrix as a whole. E.g., the following rule for double matrices mimics
Octave's default output format (for the sake of simplicity, this isn't
perfect, but you get the idea):

    > __show__ x::matrix =
    >   strcat [printd j (x!(i,j))|i=0..n-1; j=0..m-1] + "\n"
    > with printd 0 = sprintf "\n%10.5f"; printd _ = sprintf "%10.5f" end
    > when n,m = dim x end if dmatrixp x;
    > {1.0,1/2;1/3,4.0};
       1.00000   0.50000
       0.33333   4.00000

Finally, by just purging the definition of the *note __show__: 8e.
function you can easily go back to the standard print syntax:

    > clear __show__
    > 1/7; 1786; cis (-pi/2);
    0.142857142857143
    1786
    6.12303176911189e-17+:-1.0

Note that if you have a set of definitions for the *note __show__: 8e.
function which should always be loaded at startup, you can put them
into the interpreter's interactive startup files, see *note Interactive
Usage: a.


File: pure.info,  Node: Non-Linear Patterns,  Next: "As" Patterns,  Prev: The __show__ Function,  Up: Caveats and Notes

13.5 Non-Linear Patterns
========================

As explained in section *note Patterns: 78, Pure allows multiple
occurrences of the same variable in a pattern (so-called
non-linearities):

    foo x x = x;

This rule will only be matched if both occurrences of `x' are bound to
the same value. More precisely, the two instances of `x' will checked
for syntactic equality during pattern matching, using the `same'
primitive provided by the prelude. This may need time proportional to
the sizes of both argument terms, and thus become quite costly for big
terms. In fact, `same' might not even terminate at all if the compared
terms are both infinite lazy data structures, such as in `foo (1..inf)
(1..inf)'. So you have to be careful to avoid such uses.

  When using non-linearities in conjunction with "as" patterns, you
also have to make sure that the "as" variable does not occur inside the
corresponding subpattern. Thus a definition like the following is
illegal:

    > foo xs@(x:xs) = x;
    <stdin>, line 1: error in pattern (recursive variable 'xs')

The explanation is that such a pattern couldn't possibly be matched by a
finite list anyway. Indeed, the only match for `xs@(x:xs)' would be an
infinite list of `x''s, and there's no way that this condition could be
verified in a finite amount of time. Therefore the interpreter reports a
"recursive variable" error in such situations.


File: pure.info,  Node: "As" Patterns,  Next: Head = Function,  Prev: Non-Linear Patterns,  Up: Caveats and Notes

13.6 "As" Patterns
==================

In the current implementation, "as" patterns cannot be placed on the
"spine" of a function definition. Thus rules like the following, which
have the pattern somewhere in the head of the left-hand side, will all
provoke an error message from the compiler:

    a@foo x y   = a,x,y;
    a@(foo x) y = a,x,y;
    a@(foo x y) = a,x,y;

This is because the spine of a function application is not available
when the function is called at runtime. "As" patterns in pattern
bindings (*note let: 80, *note const: 43, *note case: 6d, *note when:
6e.) are not affected by this restriction since the entire value to be
matched is available at runtime. For instance:

    > case bar 99 of y@(bar x) = y,x+1; end;
    bar 99,100



File: pure.info,  Node: Head = Function,  Next: With and when,  Prev: "As" Patterns,  Up: Caveats and Notes

13.7 Head = Function
====================

"As" patterns are also a useful device if you need to manipulate
function applications in a generic way. Note that the "head = function"
rule means that the head symbol `f' of an application `f x1 ... xn'
occurring on (or inside) the left-hand side of an equation, variable
binding, or pattern-matching lambda expression, is always interpreted
as a literal function symbol (not a variable). This implies that you
cannot match the "function" component of an application against a
variable, at least not directly. An anonymous "as" pattern like `f@_'
does the trick, however, since the anonymous variable is always
recognized, even if it occurs as the head symbol of a function
application. Here's a little example which demonstrates how you can
convert a function application to a list containing the function and
all arguments:

    > foo x = a [] x with a xs (x@_ y) = a (y:xs) x; a xs x = x:xs end;
    > foo (a b c d);
    [a,b,c,d]

This may seem a little awkward, but as a matter of fact the "head =
function" rule is quite useful since it covers the common cases without
forcing the programmer to declare "constructor" symbols (except nonfix
symbols). On the other hand, generic rules operating on arbitrary
function applications are not all that common, so having to "escape" a
variable using the anonymous "as" pattern trick is a small price to pay
for that convenience.

  Sometimes you may also run into the complementary problem, i.e., to
match a function argument against a given function. Consider this code
fragment:

    foo x = x+1;
    foop f = case f of foo = 1; _ = 0 end;

You might expect `foop' to return true for `foo', and false on all other
values. Better think again, because in reality `foop' will always return
true! In fact, the Pure compiler will warn you about the second rule of
the *note case: 6d. expression not being used at all:

    > foop 99;
    warning: rule never reduced: _ = 0;
    1

This happens because an identifier on the left-hand side of a rule,
which is neither the head symbol of a function application nor a *note
nonfix: 69.  symbol, is always considered to be a variable (cf. *note
Variables in Equations: 68.), even if that symbol is defined as a
global function elsewhere.  So `foo' isn't a literal name in the above
*note case: 6d. expression, it's a variable!  (As a matter of fact,
this is rather useful, since otherwise a rule like `f g = g+1' would
suddenly change meaning if you happen to add a definition like `g x =
x-1' somewhere else in your program, which certainly isn't desirable.)

  A possible workaround is to "escape" the function symbol using an
empty namespace qualifier:

    foop f = case f of ::foo = 1; _ = 0 end;

This trick works in *note case: 6d. expressions and function
definitions, but fails in circumstances in which qualified variable
symbols are permitted (i.e., in variable and constant definitions). A
better solution is to employ the syntactic equality operator `==='
defined in the prelude to match the target value against the function
symbol. This allows you to define the `foop' predicate as follows:

    > foop f = f===foo;
    > foop foo, foop 99;
    1,0

Another way to deal with the situation would be to just declare `foo'
as a nonfix symbol. However, this makes the `foo' symbol "precious",
i.e., after such a declaration it cannot be used as a local variable
anymore. It's usually a good idea to avoid that kind of thing, at least
for generic symbols, so the above solution is preferred in this case.


File: pure.info,  Node: With and when,  Next: Numeric Calculations,  Prev: Head = Function,  Up: Caveats and Notes

13.8 With and when
==================

A common source of confusion is that Pure provides two different
constructs to bind local function and variable symbols, respectively.
This distinction is necessary because Pure does not segregate defined
functions and constructors, and thus there is no magic to figure out
whether an equation like `foo x = y' by itself is meant as a definition
of a function `foo' with formal parameter `x' and return value `y', or
a pattern binding defining the local variable `x' by matching the
pattern `foo x' against the value of `y'. The *note with: 67. construct
does the former, *note when: 6e. the latter. (As a mnemonic, you may
consider that *note when: 6e. conveys a sense of time, as the
individual variable definitions in a *note when: 6e. clause are
executed in order, while the function definitions in a *note with: 67.
clause are all done simultaneously.)

  Another speciality is that *note with: 67. and *note when: 6e.
clauses are tacked on to the end of the expression they belong to. This
mimics mathematical language and makes it easy to read and understand a
definition in a "top-down" fashion.  This style differs considerably
from other block-structured programming languages, however, which often
place local definitions in front of the code they apply to. To grasp
the operational meaning of such nested definitions, it can be helpful
to read the nested scopes "in reverse" (from bottom to top). Some
people also prefer to write their programs that way. In contrast to
Haskell and ML which have *note let: 80. expressions to support that
kind of notation, Pure doesn't provide any special syntax for this. But
note that you can always write *note when: 6e. clauses in the following
style which places the "body" at the bottom of the clause:

    result when
      y = foo (x+1);
      z = bar y;
      result = baz z;
    end;

This doesn't incur any overhead, since the compiler will always
eliminate the trivial "tail binding" for the result value. E.g., the
above will compile to exactly the same code as:

    baz z when
      y = foo (x+1);
      z = bar y;
    end;



File: pure.info,  Node: Numeric Calculations,  Next: Constant Definitions,  Prev: With and when,  Up: Caveats and Notes

13.9 Numeric Calculations
=========================

If possible, you should decorate numeric variables on the left-hand
sides of function definitions with the appropriate type tags, like
`int' or `double'. This often helps the compiler to generate better
code and makes your programs run faster. The `|' syntax makes it easy
to add the necessary specializations of existing rules to your program.
E.g., taking the polymorphic implementation of the factorial as an
example, you only have to add a left-hand side with the appropriate
type tag to make that definition go as fast as possible for the special
case of machine integers:

    fact n::int    |
    fact n         = n*fact(n-1) if n>0;
                   = 1 otherwise;

(This obviously becomes unwieldy if you have to deal with several
numeric arguments of different types, however, so in this case it is
usually better to just use a polymorphic rule.)

  Also note that int (the machine integers), bigint (the GMP "big"
integers) and double (floating point numbers) are all different kinds
of objects. While they can be used in mixed operations (such as
multiplying an int with a bigint which produces a bigint, or a bigint
with a double which produces a double), the `int' tag will only ever
match a machine int, _not_ a bigint or a double. Likewise, `bigint'
only matches bigints (never int or double values), and `double' only
doubles. Thus, if you want to define a function operating on different
kinds of numbers, you'll also have to provide equations for all the
types that you need (or a polymorphic rule which catches them all).
This also applies to equations matching against constant values of these
types. In particular, a small integer constant like `0' only matches
machine integers, not bigints; for the latter you'll have to use the
"big L" notation `0L'. Similarly, the constant `0.0' only matches
doubles, but not ints or bigints.


File: pure.info,  Node: Constant Definitions,  Next: External C Functions,  Prev: Numeric Calculations,  Up: Caveats and Notes

13.10 Constant Definitions
==========================

Constants differ from variables in that they cannot be redefined
(that's their main purpose after all) so that their values, once
defined, can be substituted into other definitions which use them. For
instance:

    > const c = 2;
    > foo x = c*x;
    > show foo
    foo x = 2*x;
    > foo 99;
    198

While a variable can be rebound to a new value at any time, you will
get an error message if you try to do this with a constant:

    > const c = 3;
    <stdin>, line 5: symbol 'c' is already defined as a constant

Note that in interactive mode you can work around this by purging the
old definition with the `clear' command. However, this won't affect any
earlier uses of the symbol:

    > clear c
    > const c = 3;
    > bar x = c*x;
    > show foo bar
    bar x = 3*x;
    foo x = 2*x;

(You'll also have to purge any existing definition of a variable if you
want to redefine it as a constant, or vice versa, since Pure won't let
you redefine an existing constant or variable as a different kind of
symbol. The same also holds if a symbol is currently defined as a
function or a macro.)

  Constants can also be used in patterns (i.e., on the left-hand side
of a rule in a definition or a *note case: 6d. expression), but only if
you also declare the corresponding symbol as *note nonfix: 69. This is
useful, e.g., if you'd like to use constants such as `true' and `false'
on the left-hand side of a definition, just like other *note nonfix:
69. symbols:

    > show false true
    const false = 0;
    const true = 1;
    > nonfix false true;
    > check false = "no"; check true = "yes";
    > show check
    check 0 = "no";
    check 1 = "yes";
    > check (5>0);
    "yes"

Note that without the *note nonfix: 69. declaration, the above
definition of `check' wouldn't work as intended, because the `true' and
`false' symbols on the left-hand side of the two equations would be
interpreted as local variables. Also note that the standard library
never declares any constant symbols as *note nonfix: 69, since once a
symbol is *note nonfix: 69. there's no going back. Thus the library
leaves this to the programmer to decide.

  As the value of a constant is known at compile time, the compiler can
apply various optimizations to uses of such values. In particular, the
Pure compiler inlines constant scalars (numbers, strings and pointers)
by literally substituting their values into the output code, and it
also precomputes simple constant expressions involving only (machine)
integer and double values. Example:

    > extern double atan(double);
    > const pi = 4*atan 1.0;
    > show pi
    const pi = 3.14159265358979;
    > foo x = 2*pi*x;
    > show foo
    foo x = 6.28318530717959*x;

In addition, the LLVM backend eliminates dead code automatically, so
you can employ a constant to configure your code for different
environments, without any runtime penalties:

    const win = index sysinfo "mingw32" >= 0;
    check boy = bad boy if win;
              = good boy otherwise;

In this case the code for one of the branches of `check' will be
completely eliminated, depending on the outcome of the configuration
check.

  For efficiency, constant aggregates (lists, tuples, matrices and
other kinds of non-scalar terms) receive special treatment. Here, the
constant is computed once and stored in a read-only variable which then
gets looked up at runtime, just like an ordinary global variable.
However, there's an important difference: If a script is batch-compiled
(cf. *note Batch Compilation: b.), the constant value is normally
computed _at compile time only_; when running the compiled executable,
the constant value is simply reconstructed, which is often much more
efficient than recomputing its value. For instance, you might use this
to precompute a large table whose computation may be costly or involve
functions with side effects:

    const table = [foo x | x = 1..1000000];
    process table;

Note that this only works with *note const: 43. values which are
completely determined at compile time. If a constant contains run time
objects such as pointers and (local) functions, this is impossible, and
the batch compiler will instead create code to recompute the value of
the constant at run time.  For instance, consider:

    using system;
    const p = malloc 100;
    foo p;

Here, the value of the pointer `p' of course critically depends on its
computation (involving a side effect which sets aside a corresponding
chunk of memory). It would become unusable without actually executing
the initialization, so the compiler generates the appropriate run time
initialization code in this case. For all practical purposes, this
turns the constant into a read-only variable. (There's also a code
generation option to force this behaviour even for "normal" constants
for which it's not strictly necessary, in order to create smaller
executables; see *note Code Size and Unstripped Executables: ec. for
details.)


File: pure.info,  Node: External C Functions,  Next: Calling Special Forms,  Prev: Constant Definitions,  Up: Caveats and Notes

13.11 External C Functions
==========================

The interpreter always takes your *note extern: 95. declarations of C
routines at face value. It will not go and read any C header files to
determine whether you actually declared the function correctly! So you
have to be careful to give the proper declarations, otherwise your
program might well give a segfault when calling the function. (This
problem can to some extent be alleviated by using the bitcode
interface. See *note Importing LLVM Bitcode: c3. and *note Inline Code:
55. in the *note C Interface: 33. section.)

  Another limitation of the C interface is that it does not offer any
special support for C structs and C function parameters. However, an
optional addon module is available which interfaces to the libffi
(http://sourceware.org/libffi/) library to provide that kind of
functionality, please see `pure-ffi' for details.

  Last but not least, to make it easier to create Pure interfaces to
large C libraries, there's a separate pure-gen program available at the
Pure website.  This program takes a C header (.h) file and creates a
corresponding Pure module with definitions and *note extern: 95.
declarations for the constants and functions declared in the header.
Please refer to `pure-gen' for details.


File: pure.info,  Node: Calling Special Forms,  Next: Laziness,  Prev: External C Functions,  Up: Caveats and Notes

13.12 Calling Special Forms
===========================

Special forms are recognized at compile time only. Thus the *note
catch: b5.  function, as well as *note quote: 66. and the operators
*note &&: 62, *note ||: 63, *note $$: 64. and *note &: 65, are only
treated as special forms in direct (saturated) calls. They can still be
used if you pass them around as function values or in partial
applications, but in this case they lose all their special call-by-name
argument processing.


File: pure.info,  Node: Laziness,  Next: Reflection,  Prev: Calling Special Forms,  Up: Caveats and Notes

13.13 Laziness
==============

Pure does lazy evaluation in the same way as *note Alice ML: fe,
providing an explicit operation (*note &: 65.) to defer evaluation and
create a "future" which is called by need. However, note that like any
language with a basically eager evaluation strategy, Pure cannot really
support lazy evaluation in a fully automatic way. That is, coding an
operation so that it works with infinite data structures usually
requires additional thought, and sometimes special code will be needed
to recognize futures in the input and handle them accordingly. This can
be hard, but of course in the case of the prelude operations this work
has already been done for you, so as long as you stick to these, you'll
never have to think about these issues. (It should be noted here that
lazy evaluation has its pitfalls even in fully lazy FPLs, such as hidden
memory leaks and other kinds of subtle inefficiencies or non-termination
issues resulting from definitions being too lazy or not lazy enough.
You can read about that in any good textbook on Haskell.)

  The prelude goes to great lengths to implement all standard list
operations in a way that properly deals with streams (a.k.a. lazy
lists). What this all boils down to is that all list operations which
can reasonably be expected to operate in a lazy way on streams, will do
so. (Exceptions are inherently eager operations such as `#', `reverse'
and `foldl'.) Only those portions of an input stream will be traversed
which are strictly required to produce the result. For most purposes,
this works just like in fully lazy FPLs such as Haskell. However, there
are some notable differences:

   * Since Pure uses dynamic typing, some of the list functions may
     have to peek ahead one element in input streams to check their
     arguments for validity, meaning that these functions will be
     slightly more eager than their Haskell counterparts.

   * Pure's list functions never produce truly cyclic list structures
     such as the ones you get, e.g., with Haskell's `cycle' operation.
     (This is actually a good thing, because the current implementation
     of the interpreter cannot garbage-collect cyclic expression data.)
     Cyclic streams such as `cycle [1]' or `fix (1:)' will of course
     work as expected, but, depending on the algorithm, memory usage
     may increase linearly as they are traversed.

   * Pattern matching is always refutable (and therefore eager) in
     Pure. If you need something like Haskell's irrefutable matches,
     you'll have to code them explicitly using futures. See the
     definition of the `unzip' function in the prelude for an example
     showing how to do this.

  Here are some common pitfalls with lazy data structures in Pure that
you should be aware of:

   * Laziness and side effects don't go well together, as most of the
     time you can't be sure when a given thunk will be executed. So as
     a general guideline you should avoid side effects in thunked data
     structures. If you can't avoid them, then at least make sure that
     all accesses to the affected resources are done through a single
     instance of the thunked data structure. E.g., the following
     definition lets you create a stream of random numbers:

         > using math;
         > let xs = [random | _ = 1..inf];

     This works as expected if only a single stream created with
     `random' exists in your program. However, as the `random' function
     in the `math' module modifies an internal data structure to
     produce a sequence of pseudorandom numbers, using two or more such
     streams in your program will in fact modify the same underlying
     data structure and thus produce two disjoint subsequences of the
     same underlying pseudorandom sequence which might not be
     distributed uniformly any more.

   * You should avoid keeping references to potentially big (or even
     infinite) thunked data structures when traversing them (unless you
     specifically need to memoize the entire data structure). In
     particular, if you assign such a data structure to a local
     variable, the traversal of the data structure should then be
     invoked as a tail call. If you fail to do this, it forces the
     entire memoized part of the data structure to stay in main memory
     while it is being traversed, leading to rather nasty memory leaks.
     Please see the `all_primes' function in *note Lazy Evaluation and
     Streams: 89. for an example.


File: pure.info,  Node: Reflection,  Next: Hygienic Macros,  Prev: Laziness,  Up: Caveats and Notes

13.14 Reflection
================

Pure versions since 0.12 offer some basic reflection capabilities via
the `evalcmd' primitive. This function provides access to interactive
commands like `clear', `save' and `show', which enable you to inspect
and modify the running program. The only "canonical" way to represent an
entire Pure program in Pure itself is the program text, hence `evalcmd'
only provides a textual interface at this time. But of course custom
higher-level representations can be built on top of that, similar to
those discussed in section *note The Quote: 7e.

  Here's an example showing what can be done using the `show' command
and a little bit of trivial text processing. The following `sym_info'
function retrieves information about a given collection of global
symbols in a way which can be processed in a Pure program. The `cat'
argument can be any combination of the letters "c", "v", "f" and "m"
denoting the categories of constants, variables, functions and macros,
respectively. (You can also just leave this empty if you don't care
about the type of symbol.) The `pat' argument is a shell-like glob
pattern for the name of symbols which should be listed (just "*"
matches all symbols). The result is a list of tuples `(name, value,
cat, descr)' with the name of the symbol and its value, as well as the
category and description of the symbol, as provided by `show -s'.

    using system;
    sym_info cat::string pat::string
    = [name,eval ("("+name+")"),descr | name,descr = info]
    when
      // Get the info about matching symbols from the 'show' command.
      info = evalcmd $ sprintf "show -sg%s %s" (cat,pat);
      // Split into lines.
      info = if null info then [""] else split "\n" $ init info;
      // Get rid of the last line with the summary information.
      info = init info;
      // Retrieve the information that we need.
      info = [x | x@(s,_) = map fields info;
      // Get rid of extra lines containing extern and fixity declarations.
              s ~= "extern" && s ~= "nonfix" && s ~= "outfix" &&
              s ~= "prefix" && s ~= "postfix" && ~fnmatch "infix*" s 0];
    end with
      // Regex call to split the summary information about one symbol, as
      // returned by 'show -s', into the name and description parts.
      fields s::string = tuple $
              [info!2 | info = tail $ regs $ reg_info $
               regex "([^ ]+)[ ]+([a-z]*)[ ]*(.*)" REG_EXTENDED s 0];
    end;

E.g., this call retrieves information about all defined macros:

    > sym_info "m" "*";
    [("$",($),"mac","2 args, 1 rules"),(".",(.),"mac","3 args, 1 rules"),
    ("void",void,"mac","1 args, 6 rules")]



File: pure.info,  Node: Hygienic Macros,  Next: Stack Size and Tail Recursion,  Prev: Reflection,  Up: Caveats and Notes

13.15 Hygienic Macros
=====================

As mentioned in the *note Macro Hygiene: b2. section, Pure macros are
lexically scoped and thus "hygienic". So Pure macros are not suceptible
to name capture, but there is also one Pure-related caveat here. The
expression printer currently doesn't check for different bindings of
the same variable identifier when it prints a (compile time)
expression. For instance, consider:

    > def F x = x+y when y = x+1 end;
    > foo y = F y;
    > show foo
    foo y = y+y when y = y+1 end;

This _looks_ as if `y' got captured, but in fact it's not, it's just the
`show' command which displays the definition in an incorrect way. You
can add the `-e' option to `show' which prints the deBruijn indices of
locally bound symbols, then you see that the actual bindings are all
right anyway (note that the number before the colon is the actual
deBruijn index, the sequence of bits behind it is the subterm path):

    > show -e foo
    foo y/*0:1*/ = y/*1:1*/+y/*0:*/ when y/*0:*/ = y/*0:1*/+1 end;

Alas, this means that if you use `dump' to write such a definition to a
text file and read it back with `run' later, then you'll get the wrong
definition. This is an outright bug in the expression printer which will
hopefully be fixed some time. But for the time being you will have to
correct such glitches manually.


File: pure.info,  Node: Stack Size and Tail Recursion,  Next: Handling of Asynchronous Signals,  Prev: Hygienic Macros,  Up: Caveats and Notes

13.16 Stack Size and Tail Recursion
===================================

Pure programs may need a considerable amount of stack space to handle
recursive function and macro calls, and the interpreter itself also
takes its toll. So you should configure your system accordingly (8 MB
of stack space is recommended for 32 bit systems, systems with 64 bit
pointers probably need more). If the *note PURE_STACK: 3e. environment
variable is defined, the interpreter performs advisory stack checks on
function entry and raises a Pure exception if the current stack size
exceeds the given limit. The value of *note PURE_STACK: 3e. should be
the maximum stack size in kilobytes. Please note that this is only an
advisory limit which does not change the program's physical stack size.
Your operating system should supply you with a command such as
ulimit(1) to set the real process stack size. (The *note PURE_STACK:
3e. limit should be a little less than that, to account for temporary
stack usage by the interpreter itself.)

  Like Scheme, Pure does proper tail calls (if LLVM provides that
feature on the platform at hand), so tail-recursive definitions should
work fine in limited stack space. For instance, the following little
program will loop forever if your platform supports the required
optimizations:

    loop = loop;
    loop;

This also works if your definition involves function parameters, guards
and multiple equations, of course. Moreover, conditional expressions
(*note if: 6f.-*note then: 70.-*note else: 71.) are tail-recursive in
both branches, and the logical operators *note &&: 62. and *note ||: 63,
as well as the sequence operator *note $$: 64, are tail-recursive in
their second operand.

  Also note that tail call optimization is _always_ disabled if the
debugger is enabled (-g). This makes it much easier to debug programs,
but means that you may run into stack overflows when debugging a
program that does deep tail recursion.


File: pure.info,  Node: Handling of Asynchronous Signals,  Prev: Stack Size and Tail Recursion,  Up: Caveats and Notes

13.17 Handling of Asynchronous Signals
======================================

As described in section *note Exception Handling: b3, signals delivered
to the process can be caught and handled with Pure's exception handling
facilities.  This has its limitations, however. Since Pure code cannot
be executed directly from a C signal handler, checks for pending
signals are only done on function entry. This means that in certain
situations (such as the execution of an external C routine), delivery
of a signal may be delayed by an arbitrary amount of time. Moreover, if
more than one signal arrives between two successive signal checks, only
the last one will be reported in the current implementation.

  When delivering a signal which has been remapped to a Pure exception,
the corresponding exception handler (if any) will be invoked as usual.
Further signals are blocked while the exception handler is being
executed.

  A fairly typical case is that you have to handle signals in a
tail-recursive function. This can be done with code like the following:

    using system;

    // Remap some common POSIX signals.
    do (trap SIG_TRAP) [SIGHUP, SIGINT, SIGTERM];

    loop = catch handler process $$ loop
    with handler (signal k) = printf "Hey, I got signal %d.\n" k end;
    process = sleep 1; // do something

Running the above `loop' function enters an endless loop reporting all
signals delivered to the process. Note that to make this work, the
tail-recursive invocation of `loop' must immediately follow the
signal-handling code, so that signals don't escape the exception
handler.

  Of course, in a real application you'd probably want the `loop'
function to carry around some data to be processed by the `process'
routine, which then returns an updated value for the next iteration.
This can be implemented as follows:

    loop x = loop (catch handler (process x))
    with handler (signal k) = printf "Hey, I got signal %d.\n" k $$ 0 end;
    process x = printf "counting: %d\n" x $$ sleep 1 $$ x+1;



File: pure.info,  Node: Author,  Next: Copying,  Prev: Caveats and Notes,  Up: Top

14 Author
*********

Albert Gräf <<Dr.Graef@t-online.de>>, Dept. of Computer Music, Johannes
Gutenberg University of Mainz, Germany.

  The author gratefully acknowledges the contributions by Scott E.
Dillard, Rooslan S. Khayrov, Eddie Rucker, Libor Spacek, Jiri Spitz and
Sergei Winitzki, as well as Toni Graffy, Michel Salim and Ryan Schmidt
who maintain the SUSE Linux, Fedora Core and OSX packages,
respectively. Thanks are also due to Vili Aapro, Alvaro Castro
Castilla, John Cowan, Chris Double, Tim Haynes, Roman Neuhauser, Wm
Leler, John Lunney and Max Wolf.


File: pure.info,  Node: Copying,  Next: References and Links,  Prev: Author,  Up: Top

15 Copying
**********

Pure comes with a fairly liberal license which lets you distribute your
own Pure programs and extensions under a license of your choice and
permits linking of commercial applications against the Pure runtime and
the Pure standard library without requiring special permission.
Moreover, the Pure interpreter (the `pure' main program), the Pure
runtime library (`libpure') and the Pure standard library (the Pure
scripts in the `lib' folder distributed with the software) are
distributed as free software, and you are welcome to modify and
redistribute them under the appropriate license terms, as detailed
below.

  (The above explanations are not legal advice. Please read the full
text of the licenses and consult qualified professional counsel for an
interpretation of the license terms as they apply to you.)

  The _Pure interpreter_ is free software: you can redistribute it
and/or modify it under the terms of the GNU General Public License as
published by the Free Software Foundation, either version 3 of the
License, or (at your option) any later version.

  The _Pure runtime library_ and the _Pure standard library_ are also
free software: you can redistribute them and/or modify them under the
terms of the GNU _Lesser_ General Public License as published by the
Free Software Foundation, either version 3 of the License, or (at your
option) any later version.

  Pure is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE.

  Please see the GNU General Public License
(http://www.gnu.org/copyleft/gpl.html) and the GNU Lesser General
Public License (http://www.gnu.org/copyleft/lgpl.html) for the precise
license terms. You can also find the license conditions in the COPYING
and COPYING.LESSER files accompanying the software. Also, please see
the source code for the copyright and license notes pertaining to
individual source files which are part of this software.

  Pure uses *note LLVM: 7. as its compiler backend. LLVM is under
Copyright (c) 2003-2010 by the University of Illinois at
Urbana-Champaign, and is licensed under a 3-clause BSD-style license,
please read COPYING.LLVM included in the distribution for the exact
licensing terms. You can also find the LLVM license at the *note LLVM:
7. website.


File: pure.info,  Node: References and Links,  Next: Index,  Prev: Copying,  Up: Top

16 References and Links
***********************

Aardappel
     Wouter van Oortmerssen's functional programming language based on
     term rewriting, <http://wouter.fov120.com/aardappel>.

Alice ML
     A version of ML (see below) from which Pure borrows its model of
     lazy evaluation, <http://www.ps.uni-sb.de/alice>.

Franz Baader and Tobias Nipkow
     _Term Rewriting and All That._ Cambridge University Press,
     Cambridge, 1998.

Bertrand
     Wm Leler's constraint programming language based on term rewriting,
     <http://groups.google.com/group/bertrand-constraint>. See Wm Leler:
     _Constraint Programming Languages: Their Specification and
     Generation._ Addison-Wesley, 1988.

Clang
     The new C/C++/Objective C compiler designed specifically to work
     on top of LLVM, <http://clang.llvm.org>. Clang provides a
     comparatively light-weight alternative to llvm-gcc which is faster
     and has better and more friendly diagnostics than gcc.

Faust
     Grame's functional DSP programming language,
     <http://faust.grame.fr>.

GNU Multiprecision Library
     Free library for arbitrary precision arithmetic,
     <http://gmplib.org>.

GNU Octave
     A popular high-level language for numeric applications and free
     MATLAB replacement, <http://www.gnu.org/software/octave>.

GNU Scientific Library
     A free software library for numeric applications, can be used with
     Pure's numeric matrices, <http://www.gnu.org/software/gsl>.

Haskell
     A popular non-strict FPL, <http://www.haskell.org>.

LLVM
     The LLVM code generator framework, <http://llvm.org>.

Miranda
     David Turner's non-strict FPL, <http://miranda.org.uk>. Miranda
     was fairly successful in its time and one of the forerunners of
     *note Haskell: 5e.

ML
     A popular strict FPL. See Robin Milner, Mads Tofte, Robert Harper,
     D. MacQueen: _The Definition of Standard ML (Revised)._ MIT Press,
     1997.

Michael O'Donnell
     _Equational Logic as a Programming Language._ Series in the
     Foundations of Computing. MIT Press, Cambridge, Mass., 1985.

Q
     Another term rewriting language by yours truly,
     <http://q-lang.sf.net>.


File: pure.info,  Node: Index,  Prev: References and Links,  Up: Top

Index
*****

 [index ]
* Menu:

* $$ infix function:                     Special Forms.       (line  48)
* & postfix function:                    Special Forms.       (line  73)
* && infix function:                     Special Forms.       (line  15)
* ' prefix function:                     Special Forms.       (line  91)
* __show__ function:                     The __show__ Function.
                                                              (line   6)
* break command:                         Interactive Commands.
                                                              (line  13)
* BROWSER:                               Online Help.         (line  10)
* bt command:                            Interactive Commands.
                                                              (line  21)
* catch function:                        Exception Handling.  (line  11)
* cd command:                            Interactive Commands.
                                                              (line  28)
* clear command:                         Interactive Commands.
                                                              (line  31)
* command:                               Interactive Commands.
                                                              (line  10)
* del command:                           Interactive Commands.
                                                              (line  47)
* dump command:                          Interactive Commands.
                                                              (line  58)
* environment variable BROWSER <1>:      Online Help.         (line  10)
* environment variable BROWSER:          Environment.         (line   9)
* environment variable HOME:             Interactive Startup. (line  12)
* environment variable LD_LIBRARY_PATH:  Importing Dynamic Libraries.
                                                              (line  32)
* environment variable PATH <1>:         Inline Code.         (line  81)
* environment variable PATH <2>:         Hierarchical Namespaces.
                                                              (line  34)
* environment variable PATH:             Modules and Imports. (line 134)
* environment variable PURE_CC:          Inline Code.         (line  83)
* environment variable PURE_CXX:         Inline Code.         (line  86)
* environment variable PURE_EAGER_JIT <1>: Environment.       (line  19)
* environment variable PURE_EAGER_JIT:   Compiling Scripts.   (line  18)
* environment variable PURE_FAUST:       Inline Code.         (line  86)
* environment variable PURE_FC:          Inline Code.         (line  86)
* environment variable PURE_HELP <1>:    Online Help.         (line   9)
* environment variable PURE_HELP:        Environment.         (line  10)
* environment variable PURE_INCLUDE <1>: Importing Dynamic Libraries.
                                                              (line  29)
* environment variable PURE_INCLUDE <2>: Modules and Imports. (line  83)
* environment variable PURE_INCLUDE:     Environment.         (line  27)
* environment variable PURE_LESS:        Environment.         (line  37)
* environment variable PURE_LIBRARY <1>: Importing Dynamic Libraries.
                                                              (line  28)
* environment variable PURE_LIBRARY:     Environment.         (line  31)
* environment variable PURE_MORE <1>:    Debugging.           (line  38)
* environment variable PURE_MORE <2>:    The show Command.    (line  34)
* environment variable PURE_MORE:        Environment.         (line  35)
* environment variable PURE_PS:          Environment.         (line  40)
* environment variable PURE_STACK <1>:   Stack Size and Tail Recursion.
                                                              (line  10)
* environment variable PURE_STACK <2>:   Exception Handling.  (line  38)
* environment variable PURE_STACK <3>:   Recursive Macros.    (line  59)
* environment variable PURE_STACK <4>:   Environment.         (line  44)
* environment variable PURE_STACK:       Code Generation Options.
                                                              (line  36)
* environment variable PURELIB <1>:      Modules and Imports. (line  86)
* environment variable PURELIB <2>:      Environment.         (line  14)
* environment variable PURELIB:          Overview of Operation.
                                                              (line  41)
* help command:                          Interactive Commands.
                                                              (line  82)
* HOME:                                  Interactive Startup. (line  12)
* LD_LIBRARY_PATH:                       Importing Dynamic Libraries.
                                                              (line  32)
* ls command:                            Interactive Commands.
                                                              (line  89)
* mem command:                           Interactive Commands.
                                                              (line  92)
* override command:                      Interactive Commands.
                                                              (line 102)
* PATH <1>:                              Inline Code.         (line  81)
* PATH <2>:                              Hierarchical Namespaces.
                                                              (line  34)
* PATH:                                  Modules and Imports. (line 134)
* pure command line option --:           Options.             (line  77)
* pure command line option --checks:     Code Generation Options.
                                                              (line  33)
* pure command line option --const:      Code Generation Options.
                                                              (line  47)
* pure command line option --ctags:      Options.             (line  12)
* pure command line option --eager fun:  Code Generation Options.
                                                              (line  98)
* pure command line option --eager-jit:  Options.             (line  16)
* pure command line option --etags:      Options.             (line  12)
* pure command line option --fold:       Code Generation Options.
                                                              (line  59)
* pure command line option --help:       Options.             (line  27)
* pure command line option --nochecks:   Code Generation Options.
                                                              (line  33)
* pure command line option --noconst:    Code Generation Options.
                                                              (line  47)
* pure command line option --noediting:  Options.             (line  43)
* pure command line option --nofold:     Code Generation Options.
                                                              (line  59)
* pure command line option --noprelude:  Options.             (line  46)
* pure command line option --norc:       Options.             (line  50)
* pure command line option --notc:       Code Generation Options.
                                                              (line  82)
* pure command line option --required fun: Code Generation Options.
                                                              (line 111)
* pure command line option --tc:         Code Generation Options.
                                                              (line  82)
* pure command line option --version:    Options.             (line  68)
* pure command line option -c:           Options.             (line   9)
* pure command line option -fpic:        Options.             (line  20)
* pure command line option -fPIC:        Options.             (line  20)
* pure command line option -g:           Options.             (line  24)
* pure command line option -h:           Options.             (line  27)
* pure command line option -i:           Options.             (line  31)
* pure command line option -I directory: Options.             (line  34)
* pure command line option -L directory: Options.             (line  37)
* pure command line option -l libname:   Options.             (line  40)
* pure command line option -n:           Options.             (line  46)
* pure command line option -o filename:  Options.             (line  53)
* pure command line option -q:           Options.             (line  56)
* pure command line option -T filename:  Options.             (line  59)
* pure command line option -u:           Options.             (line  62)
* pure command line option -v[level]:    Options.             (line  65)
* pure command line option -w:           Options.             (line  71)
* pure command line option -x:           Options.             (line  74)
* PURE_CC:                               Inline Code.         (line  83)
* PURE_CXX:                              Inline Code.         (line  86)
* PURE_EAGER_JIT:                        Compiling Scripts.   (line  18)
* PURE_FAUST:                            Inline Code.         (line  86)
* PURE_FC:                               Inline Code.         (line  86)
* PURE_HELP <1>:                         Online Help.         (line   9)
* PURE_HELP:                             Environment.         (line  10)
* PURE_INCLUDE <1>:                      Importing Dynamic Libraries.
                                                              (line  29)
* PURE_INCLUDE:                          Modules and Imports. (line  83)
* PURE_LESS:                             Environment.         (line  37)
* PURE_LIBRARY:                          Importing Dynamic Libraries.
                                                              (line  28)
* PURE_MORE <1>:                         Debugging.           (line  38)
* PURE_MORE:                             The show Command.    (line  34)
* PURE_STACK <1>:                        Stack Size and Tail Recursion.
                                                              (line  10)
* PURE_STACK <2>:                        Exception Handling.  (line  38)
* PURE_STACK <3>:                        Recursive Macros.    (line  59)
* PURE_STACK:                            Code Generation Options.
                                                              (line  36)
* PURELIB <1>:                           Modules and Imports. (line  86)
* PURELIB <2>:                           Environment.         (line  15)
* PURELIB:                               Overview of Operation.
                                                              (line  41)
* pwd command:                           Interactive Commands.
                                                              (line 108)
* quit command:                          Interactive Commands.
                                                              (line 111)
* quote function:                        Special Forms.       (line  91)
* run command:                           Interactive Commands.
                                                              (line 114)
* save command:                          Interactive Commands.
                                                              (line 141)
* show command:                          Interactive Commands.
                                                              (line 147)
* stats command:                         Interactive Commands.
                                                              (line 155)
* trace command:                         Interactive Commands.
                                                              (line 166)
* underride command:                     Interactive Commands.
                                                              (line 172)
* || infix function:                     Special Forms.       (line  15)

                                                             
                              



Tag Table:
Node: Top229
Ref: 0326
Ref: fdl326
Ref: 1326
Ref: gpl326
Ref: 2326
Ref: lgpl326
Node: Introduction3262
Ref: introduction3346
Ref: 53346
Ref: the-pure-manual3346
Ref: 63346
Node: Further Reading7150
Ref: further reading7246
Ref: d7246
Ref: further-reading7246
Ref: e7841
Ref: functional-programming7841
Node: Typographical Conventions8438
Ref: typographical conventions8534
Ref: 118534
Ref: typographical-conventions8534
Node: Invoking Pure10407
Ref: invoking pure10501
Ref: 810501
Ref: invoking-pure10501
Node: Options12519
Ref: options12604
Ref: 1512604
Ref: 1612712
Ref: cmdoption-pure-c12712
Ref: 1712752
Ref: cmdoption-pure--ctags12752
Ref: 1812772
Ref: cmdoption-pure--etags12772
Ref: 1912856
Ref: cmdoption-pure--eager-jit12856
Ref: 1a12991
Ref: cmdoption-pure-fpic12991
Ref: 1313087
Ref: cmdoption-pure-g13087
Ref: 1b13135
Ref: cmdoption-pure-h13135
Ref: 1c13150
Ref: cmdoption-pure--help13150
Ref: 1d13204
Ref: cmdoption-pure-i13204
Ref: 1e13367
Ref: cmdoption-pure-l13367
Ref: 1f13524
Ref: cmdoption-pure--noediting13524
Ref: 2013584
Ref: cmdoption-pure-n13584
Ref: 2113599
Ref: cmdoption-pure--noprelude13599
Ref: 2213654
Ref: cmdoption-pure--norc13654
Ref: 2313721
Ref: cmdoption-pure-o13721
Ref: 1413790
Ref: cmdoption-pure-q13790
Ref: 2413875
Ref: cmdoption-pure-t13875
Ref: 2513973
Ref: cmdoption-pure-u13973
Ref: 2614046
Ref: cmdoption-pure-v14046
Ref: 2714118
Ref: cmdoption-pure--version14118
Ref: 2814182
Ref: cmdoption-pure-w14182
Ref: 2914229
Ref: cmdoption-pure-x14229
Ref: 2a14300
Ref: cmdoption-pure--14300
Node: Overview of Operation14622
Ref: overview of operation14733
Ref: 2c14733
Ref: overview-of-operation14733
Ref: 2e16833
Ref: index-016833
Ref: 3016881
Ref: index-116881
Node: Compiling Scripts17424
Ref: compiling scripts17543
Ref: 2d17543
Ref: compiling-scripts17543
Ref: 3418369
Ref: index-218369
Node: Tagging Scripts21707
Ref: tagging scripts21826
Ref: 3721826
Ref: tagging-scripts21826
Ref: 3822222
Ref: tags22222
Node: Running Interactively23515
Ref: running interactively23648
Ref: 1223648
Ref: running-interactively23648
Node: Verbosity and Debugging Options25533
Ref: verbosity and debugging options25674
Ref: 3625674
Ref: verbosity-and-debugging-options25674
Node: Code Generation Options28550
Ref: code generation options28683
Ref: 2b28683
Ref: code-generation-options28683
Ref: 3b30159
Ref: cmdoption-pure--checks30159
Ref: 3c30180
Ref: cmdoption-pure--nochecks30180
Ref: 3d30333
Ref: index-330333
Ref: 4130970
Ref: cmdoption-pure--const30970
Ref: 4230990
Ref: cmdoption-pure--noconst30990
Ref: 4431605
Ref: cmdoption-pure--fold31605
Ref: 4531624
Ref: cmdoption-pure--nofold31624
Ref: 4632315
Ref: cmdoption-pure--tc32315
Ref: 4732332
Ref: cmdoption-pure--notc32332
Ref: 4833116
Ref: cmdoption-pure--eager33116
Ref: 4933838
Ref: cmdoption-pure--required33838
Node: Startup Files34321
Ref: startup files34434
Ref: 4a34434
Ref: startup-files34434
Node: Environment34920
Ref: environment35001
Ref: 4b35001
Ref: 4c35139
Ref: envvar-browser35139
Ref: 4d35171
Ref: index-435171
Ref: 2f35368
Ref: envvar-purelib35368
Ref: 4f35460
Ref: index-535460
Ref: 3535563
Ref: envvar-pure_eager_jit35563
Ref: 4e35710
Ref: envvar-pure_help35710
Ref: 5035858
Ref: envvar-pure_include35858
Ref: 5135982
Ref: envvar-pure_library35982
Ref: 5236107
Ref: envvar-pure_more36107
Ref: 5336261
Ref: index-636261
Ref: 5436344
Ref: envvar-pure_ps36344
Ref: 3e36446
Ref: envvar-pure_stack36446
Node: Pure Overview36808
Ref: pure overview36901
Ref: 936901
Ref: pure-overview36901
Node: Lexical Matters38719
Ref: lexical matters38826
Ref: 5738826
Ref: lexical-matters38826
Ref: 5942613
Ref: code-charts42613
Ref: 5a42613
Ref: unicode-consortium42613
Ref: 5c45166
Ref: id145166
Node: Definitions and Symbolic Evaluation45166
Ref: definitions and symbolic evaluation45304
Ref: 5d45304
Ref: definitions-and-symbolic-evaluation45304
Node: Variables in Equations50259
Ref: variables in equations50399
Ref: 6850399
Ref: variables-in-equations50399
Node: Expression Syntax52590
Ref: expression syntax52708
Ref: 6a52708
Ref: expression-syntax52708
Node: Primary Expressions56412
Ref: primary expressions56510
Ref: 7256510
Ref: primary-expressions56510
Node: Simple Expressions68649
Ref: simple expressions68775
Ref: 7968775
Ref: simple-expressions68775
Node: Special Expressions71889
Ref: special expressions71987
Ref: 6f71987
Ref: if71987
Ref: 7071987
Ref: then71987
Ref: 7171987
Ref: else71987
Ref: 6d71987
Ref: case71987
Ref: 6e71987
Ref: when71987
Ref: 6771987
Ref: with71987
Ref: 5871987
Ref: end71987
Ref: 7a71987
Ref: special-expressions71987
Node: Special Forms74499
Ref: special forms74603
Ref: 7574603
Ref: special-forms74603
Ref: 7c74603
Ref: id274603
Ref: 6275003
Ref: &&/special75003
Ref: 6375024
Ref: ||/special75024
Ref: 6476742
Ref: $$76742
Ref: 6577546
Ref: &77546
Ref: 6678559
Ref: quote78559
Ref: 7d78581
Ref: '78581
Node: Toplevel79255
Ref: toplevel79355
Ref: 7f79355
Ref: def79355
Ref: 8079355
Ref: let79355
Ref: 4379355
Ref: const79355
Ref: 8179355
Node: Scoping Rules82704
Ref: scoping rules82782
Ref: 8382782
Ref: scoping-rules82782
Node: Rule Syntax85809
Ref: rule syntax85897
Ref: 6185897
Ref: rule-syntax85897
Node: Patterns87083
Ref: patterns87155
Ref: 7887155
Node: Type Tags92516
Ref: type tags92610
Ref: 6092610
Ref: type-tags92610
Ref: 8492610
Ref: id492610
Node: General Rules95298
Ref: general rules95396
Ref: 8595396
Ref: general-rules95396
Node: Simple Rules98628
Ref: simple rules98708
Ref: 8698708
Ref: simple-rules98708
Node: Examples100238
Ref: examples100325
Ref: 77100325
Node: List Comprehensions102054
Ref: list comprehensions102152
Ref: 88102152
Ref: list-comprehensions102152
Node: Lazy Evaluation and Streams103790
Ref: lazy evaluation and streams103916
Ref: 89103916
Ref: lazy-evaluation-and-streams103916
Ref: 8a103916
Ref: id5103916
Node: Matrix Computations108461
Ref: matrix computations108585
Ref: 8b108585
Ref: matrix-computations108585
Ref: 8c108585
Ref: id6108585
Node: Symbolic Matrices115999
Ref: symbolic matrices116107
Ref: 90116107
Ref: symbolic-matrices116107
Node: Record Data118328
Ref: record data118426
Ref: 92118426
Ref: record-data118426
Ref: 93118426
Ref: id7118426
Node: The Quote123837
Ref: the quote123909
Ref: 7e123909
Ref: the-quote123909
Node: Declarations127107
Ref: declarations127189
Ref: 32127189
Node: Symbol Declarations128291
Ref: symbol declarations128385
Ref: 98128385
Ref: public128385
Ref: 99128385
Ref: private128385
Ref: 9a128385
Ref: infix128385
Ref: 9b128385
Ref: infixl128385
Ref: 9c128385
Ref: infixr128385
Ref: 9d128385
Ref: prefix128385
Ref: 9e128385
Ref: postfix128385
Ref: 91128385
Ref: outfix128385
Ref: 69128385
Ref: nonfix128385
Ref: 5b128385
Ref: symbol-declarations128385
Node: Modules and Imports135526
Ref: modules and imports135639
Ref: 31135639
Ref: using135639
Ref: a1135639
Ref: modules-and-imports135639
Ref: a2138997
Ref: index-7138997
Ref: a3139111
Ref: index-8139111
Ref: a4139409
Ref: index-9139409
Ref: a5139696
Ref: index-10139696
Ref: a6139724
Ref: index-11139724
Ref: a7141311
Ref: index-12141311
Node: Namespaces141615
Ref: namespaces141700
Ref: 96141700
Ref: namespace141700
Ref: 74141700
Node: Using Namespaces145288
Ref: using namespaces145384
Ref: a9145384
Ref: using-namespace145384
Ref: aa145384
Ref: using-namespaces145384
Node: Symbol Lookup and Creation147823
Ref: symbol lookup and creation147943
Ref: 3a147943
Ref: symbol-lookup-and-creation147943
Node: Private Symbols154435
Ref: private symbols154562
Ref: 9f154562
Ref: private-symbols154562
Node: Hierarchical Namespaces155736
Ref: hierarchical namespaces155854
Ref: ab155854
Ref: hierarchical-namespaces155854
Ref: ac156996
Ref: index-13156996
Node: Scoped Namespaces158024
Ref: scoped namespaces158118
Ref: a8158118
Ref: scoped-namespaces158118
Node: Macros161367
Ref: macros161459
Ref: 82161459
Node: Optimization Rules163272
Ref: optimization rules163356
Ref: ad163356
Ref: optimization-rules163356
Ref: ae163356
Ref: id9163356
Node: Recursive Macros167722
Ref: recursive macros167841
Ref: 94167841
Ref: recursive-macros167841
Ref: b0170383
Ref: index-14170383
Node: User-Defined Special Forms170428
Ref: user-defined special forms170542
Ref: b1170542
Ref: user-defined-special-forms170542
Node: Macro Hygiene174409
Ref: macro hygiene174498
Ref: b2174498
Ref: macro-hygiene174498
Node: Exception Handling176897
Ref: exception handling176988
Ref: b3176988
Ref: exception-handling176988
Ref: b4176988
Ref: id10176988
Ref: b5177300
Ref: catch177300
Ref: b6178468
Ref: index-15178468
Node: C Interface180914
Ref: c interface181015
Ref: 33181015
Ref: c-interface181015
Node: Extern Declarations182468
Ref: extern declarations182549
Ref: 95182549
Ref: extern182549
Ref: b8182549
Ref: extern-declarations182549
Node: C Types187572
Ref: c types187689
Ref: b9187689
Ref: c-types187689
Ref: ba187689
Ref: id11187689
Node: Basic C Types188929
Ref: basic c types189006
Ref: bb189006
Ref: basic-c-types189006
Node: Pointer Types191174
Ref: pointer types191281
Ref: bc191281
Ref: pointer-types191281
Node: Pointers and Matrices194473
Ref: pointers and matrices194583
Ref: bd194583
Ref: pointers-and-matrices194583
Node: Pointer Examples197759
Ref: pointer examples197847
Ref: be197847
Ref: pointer-examples197847
Node: Importing Dynamic Libraries202071
Ref: importing dynamic libraries202191
Ref: bf202191
Ref: importing-dynamic-libraries202191
Ref: c0203312
Ref: index-16203312
Ref: c1203387
Ref: index-17203387
Ref: c2203598
Ref: index-18203598
Node: Importing LLVM Bitcode203990
Ref: importing llvm bitcode204114
Ref: c3204114
Ref: importing-llvm-bitcode204114
Node: Inline Code209864
Ref: inline code209952
Ref: 55209952
Ref: inline-code209952
Ref: c6213282
Ref: index-19213282
Ref: c7213396
Ref: index-20213396
Ref: c8213597
Ref: index-21213597
Ref: c9213609
Ref: index-22213609
Ref: ca213623
Ref: index-23213623
Ref: cb213843
Ref: index-24213843
Node: Standard Library214084
Ref: standard library214184
Ref: 56214184
Ref: standard-library214184
Node: Interactive Usage218447
Ref: interactive usage218553
Ref: a218553
Ref: interactive-usage218553
Node: Online Help220546
Ref: online help220638
Ref: 4220638
Ref: online-help220638
Ref: cd220938
Ref: index-25220938
Ref: ce220966
Ref: index-26220966
Ref: cf222739
Ref: docutils222739
Node: Interactive Commands223680
Ref: interactive commands223792
Ref: cc223792
Ref: interactive-commands223792
Ref: d0224023
Ref: index-27224023
Ref: d1224070
Ref: index-28224070
Ref: d2224497
Ref: index-29224497
Ref: d3224823
Ref: index-30224823
Ref: d4224885
Ref: index-31224885
Ref: d8225871
Ref: index-32225871
Ref: d9226460
Ref: index-33226460
Ref: db227979
Ref: index-34227979
Ref: dc228307
Ref: index-35228307
Ref: dd228373
Ref: index-36228373
Ref: de228924
Ref: index-37228924
Ref: df229170
Ref: index-38229170
Ref: e0229250
Ref: index-39229250
Ref: e1229301
Ref: index-40229301
Ref: e2230985
Ref: index-41230985
Ref: e3231235
Ref: index-42231235
Ref: e5231647
Ref: index-43231647
Ref: e6232264
Ref: index-44232264
Ref: e7232568
Ref: index-45232568
Node: Last Result233376
Ref: last result233505
Ref: d5233505
Ref: last-result233505
Node: Specifying Symbol Selections234215
Ref: specifying symbol selections234340
Ref: d7234340
Ref: specifying-symbol-selections234340
Node: The show Command237700
Ref: the show command237831
Ref: e4237831
Ref: the-show-command237831
Ref: e8238933
Ref: index-46238933
Node: Definition Levels241699
Ref: definition levels241811
Ref: d6241811
Ref: definition-levels241811
Node: Debugging246991
Ref: debugging247106
Ref: 39247106
Ref: ea248797
Ref: index-47248797
Node: Interactive Startup257609
Ref: interactive startup257698
Ref: da257698
Ref: interactive-startup257698
Ref: eb258066
Ref: index-48258066
Node: Batch Compilation258922
Ref: batch compilation259029
Ref: b259029
Ref: batch-compilation259029
Ref: ed264189
Ref: partial-evaluation264189
Node: Example264316
Ref: example264420
Ref: ee264420
Node: Code Size and Unstripped Executables267373
Ref: code size and unstripped executables267511
Ref: ec267511
Ref: code-size-and-unstripped-executables267511
Node: Other Output Code Formats270971
Ref: other output code formats271131
Ref: ef271131
Ref: other-output-code-formats271131
Ref: f0275640
Ref: llvm-documentation275640
Node: Calling Pure Functions From C275640
Ref: calling pure functions from c275755
Ref: f1275755
Ref: calling-pure-functions-from-c275755
Node: Caveats and Notes277248
Ref: caveats and notes277344
Ref: c277344
Ref: caveats-and-notes277344
Node: Etymology277970
Ref: etymology278062
Ref: f2278062
Node: Backward Compatibility278891
Ref: backward compatibility279006
Ref: f3279006
Ref: backward-compatibility279006
Node: Error Recovery284751
Ref: error recovery284878
Ref: f6284878
Ref: error-recovery284878
Node: The __show__ Function285534
Ref: the __show__ function285658
Ref: 8f285658
Ref: the-show-function285658
Ref: 8e285713
Ref: __show__285713
Node: Non-Linear Patterns289354
Ref: non-linear patterns289477
Ref: f7289477
Ref: non-linear-patterns289477
Node: "As" Patterns290877
Ref: "as" patterns290994
Ref: f8290994
Ref: as-patterns290994
Node: Head = Function291750
Ref: head = function291861
Ref: af291861
Ref: head-function291861
Node: With and when295412
Ref: with and when295530
Ref: f9295530
Ref: with-and-when295530
Node: Numeric Calculations297656
Ref: numeric calculations297779
Ref: fa297779
Ref: numeric-calculations297779
Node: Constant Definitions299688
Ref: constant definitions299818
Ref: 87299818
Ref: constant-definitions299818
Node: External C Functions304823
Ref: external c functions304954
Ref: b7304954
Ref: external-c-functions304954
Ref: fb305854
Ref: libffi305854
Node: Calling Special Forms306239
Ref: calling special forms306358
Ref: fc306358
Ref: calling-special-forms306358
Node: Laziness306845
Ref: laziness306954
Ref: fd306954
Node: Reflection311460
Ref: reflection311563
Ref: e9311563
Node: Hygienic Macros314231
Ref: hygienic macros314355
Ref: ff314355
Ref: hygienic-macros314355
Node: Stack Size and Tail Recursion315709
Ref: stack size and tail recursion315855
Ref: 3f315855
Ref: stack-size-and-tail-recursion315855
Ref: 100316243
Ref: index-49316243
Ref: 101316455
Ref: index-50316455
Ref: 102316743
Ref: index-51316743
Node: Handling of Asynchronous Signals317807
Ref: handling of asynchronous signals317929
Ref: 40317929
Ref: handling-of-asynchronous-signals317929
Node: Author319950
Ref: author320036
Ref: 103320036
Node: Copying320605
Ref: copying320694
Ref: 3320694
Node: References and Links323056
Ref: references and links323144
Ref: 104323144
Ref: references-and-links323144
Ref: 7b323193
Ref: aardappel323193
Ref: fe323334
Ref: alice-ml323334
Ref: 10323469
Ref: baader-and-nipkow323469
Ref: a0323587
Ref: bertrand323587
Ref: c4323847
Ref: clang323847
Ref: c5324119
Ref: faust324119
Ref: 105324206
Ref: gnu-multiprecision-library324206
Ref: 73324206
Ref: gmp324206
Ref: 106324314
Ref: gnu-octave324314
Ref: 8d324314
Ref: octave324314
Ref: 76324458
Ref: gnu-scientific-library324458
Ref: 107324458
Ref: gsl324458
Ref: 5e324619
Ref: haskell324619
Ref: 7324685
Ref: llvm324685
Ref: f5324750
Ref: miranda324750
Ref: 5f324921
Ref: ml324921
Ref: f325080
Ref: michael-o-donnell325080
Ref: f4325230
Ref: q325230
Node: Index325315
Ref: index325387
Ref: 6b337207
Ref: 6c337238
Ref: 97337269

End Tag Table


Local Variables:
coding: utf-8
End:
